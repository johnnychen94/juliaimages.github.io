var documenterSearchIndex = {"docs":
[{"location":"tutorials/conversions_views/#page_conversions_views","page":"Conversions vs. views","title":"Conversions vs. views","text":"","category":"section"},{"location":"tutorials/conversions_views/#Sharing-memory:-an-introduction-to-views","page":"Conversions vs. views","title":"Sharing memory: an introduction to views","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"In Arrays, Numbers, and Colors we discussed how one can convert the element type of an array a = [1,2,3,4] using a syntax like Float64.(a). You might be curious what affect, if any, Int.(a) has:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> a = [1,2,3,4]\n4-element Array{Int64,1}:\n 1\n 2\n 3\n 4\n\njulia> b = Int.(a)\n4-element Array{Int64,1}:\n 1\n 2\n 3\n 4","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"There's certainly no obvious change, and as you'd expect b == a returns true.  Beyond having equal size and elements, there's a more extensive notion of \"sameness\": do a and b refer to the same storage area in memory?  We can test that in the following ways:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> a === b   # note: 3 equal signs!\nfalse","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"or more generally by setting a value and seeing whether the change is reflected in the other:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> b[1] = 5\n5\n\njulia> b\n4-element Array{Int64,1}:\n 5\n 2\n 3\n 4\n\njulia> a\n4-element Array{Int64,1}:\n 1\n 2\n 3\n 4","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Since the types of a and b are identical, both tests tell us that a and b are independent objects, even if they (initially) had the same values.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"This occurs because f.(a) (which calls the function broadcast(f, a)) always allocates a new array to return its values. However, not all functions operate this way. One good example is view:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> v = view(a, :)\n4-element view(::Array{Int64,1}, :) with eltype Int64:\n 1\n 2\n 3\n 4","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"v and a have the same values, but again they are distinct objects:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> v == a\ntrue\n\njulia> v === a\nfalse","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"However, they share the same memory:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> v[1] = 10\n10\n\njulia> v\n4-element view(::Array{Int64,1}, :) with eltype Int64:\n 10\n  2\n  3\n  4\n\njulia> a\n4-element Array{Int64,1}:\n 10\n  2\n  3\n  4","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Consequently, v is a \"view\" of the values stored in a.  While this usage of view is trivial, more generally it can be used to select a rectangular region of interest, which is a common operation in image processing; this region is selected without copying any data, and any manipulations of the values within this region are reflected in the original (parent) array. See the documentation on view, by typing ?view, for more information.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"view is not the only function with this property: another good example is reshape, which can be used to change the dimensions of an array:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> r = reshape(a, 2, 2)\n2×2 Array{Int64,2}:\n 10  3\n  2  4\n\njulia> r[1,2] = 7\n7\n\njulia> r\n2×2 Array{Int64,2}:\n 10  7\n  2  4\n\njulia> a\n4-element Array{Int64,1}:\n 10\n  2\n  7\n  4","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Notice that the return type of reshape is just an Array, one which happens to be serving as a view of a. However, some inputs cannot be represented as a view with an Array. For example:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> r = reshape(1:15, 3, 5)\n3×5 reshape(::UnitRange{Int64}, 3, 5) with eltype Int64:\n 1  4  7  10  13\n 2  5  8  11  14\n 3  6  9  12  15","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"A UnitRange is represented compactly–-storing only the starting and stopping values–-so there is no memory location that can be referenced to access all values. In such cases, reshape returns a ReshapedArray, which is a generic \"view type\" that handles reshaping of any kind of AbstractArray.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"The output of both view and reshape are always views: make a change in either the parent or the view, and the change is reflected in the other.","category":"page"},{"location":"tutorials/conversions_views/#Views-for-\"converting\"-between-fixed-point-and-raw-representations","page":"Conversions vs. views","title":"Views for \"converting\" between fixed-point and raw representations","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Arrays, Numbers, and Colors also introduced the fixed-point numbers used in some representations of color (or grayscale) information. If you want to switch representation, you can use the reinterpret function:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> using FixedPointNumbers\n\njulia> x = 0.5N0f8\n0.502N0f8\n\njulia> y = reinterpret(x)  # alternatively, use: reinterpret(UInt8, x)\n0x80\n\njulia> reinterpret(N0f8, y)\n0.502N0f8","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"You can apply this to arrays:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> a = [0.2N0f8, 0.8N0f8]\n2-element Array{N0f8,1} with eltype Normed{UInt8,8}:\n 0.2N0f8\n 0.8N0f8\n\njulia> b = reinterpret.(a)\n2-element Array{UInt8,1}:\n 0x33\n 0xcc","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Because of the f.(a) call, b does not share memory with a:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> b[2] = 0xff\n0xff\n\njulia> a\n2-element Array{N0f8,1} with eltype Normed{UInt8,8}:\n 0.2N0f8\n 0.8N0f8","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Often this might not be a problem, but sometimes you might wish that these referenced the same underlying object.  For such situations, JuliaImages, through the ImageCore package (which is bundled with Images), implements views that can perform this reinterpretation:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> using Images\n\njulia> v = rawview(a)\n2-element reinterpret(UInt8, ::Array{N0f8,1}):\n 0x33\n 0xcc\n\njulia> v[2] = 0xff\n0xff\n\njulia> a\n2-element Array{N0f8,1} with eltype Normed{UInt8,8}:\n 0.2N0f8\n 1.0N0f8","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"The opposite transformation is normedview:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> c = [0x11, 0x22]\n2-element Array{UInt8,1}:\n 0x11\n 0x22\n\njulia> normedview(c)\n2-element reinterpret(N0f8, ::Array{UInt8,1}):\n 0.067N0f8\n 0.133N0f8","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"normedview allows you to pass the interpreted type as the first argument, i.e., normedview(N0f8, A), and indeed it's required to do so unless A has element type UInt8, in which case normedview assumes you want N0f8.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Like reshape, both rawview and normedview might return an Array or a more complicated type (a ReinterpretArray, or a MappedArray from the MappedArrays package), depending on the types of the inputs.","category":"page"},{"location":"tutorials/conversions_views/#Color-separations:-views-for-converting-between-numbers-and-colors","page":"Conversions vs. views","title":"Color separations: views for converting between numbers and colors","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"In Arrays, Numbers, and Colors, we pointed out that one can convert a numeric array to a grayscale array with Gray.(a); the opposite transformation can be performed with real.(b). Handling RGB colors is a little more complicated, because the dimensionality of the array changes. One approach is to use Julia's comprehensions:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> a = reshape(collect(0.1:0.1:0.6), 3, 2)\n3×2 Array{Float64,2}:\n 0.1  0.4\n 0.2  0.5\n 0.3  0.6\n\njulia> c = [RGB(a[1,j], a[2,j], a[3,j]) for j = 1:2]\n2-element Array{RGB{Float64},1} with eltype RGB{Float64}:\n RGB{Float64}(0.1,0.2,0.3)\n RGB{Float64}(0.4,0.5,0.6)\n\njulia> x = [getfield(c[j], i) for i = 1:3, j = 1:2]\n3×2 Array{Float64,2}:\n 0.1  0.4\n 0.2  0.5\n 0.3  0.6","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"While this approach works, it's not without flaws:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"this implementation relies on the two-dimensionality of a; a 3d array (producing a 2d color image) would need a different implementation\nthe use of getfield assumes that elements of c have fields and that they are in the order r, g, b. Given the large number of different representations of RGB supported by ColorTypes, neither of these assumptions is entirely safe.\nit always makes a copy of the data","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"To address these weaknesses, JuliaImages provides two complementary view function, colorview and channelview:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> colv = colorview(RGB, a)\n2-element reshape(reinterpret(RGB{Float64}, ::Array{Float64,2}), 2) with eltype RGB{Float64}:\n RGB{Float64}(0.1,0.2,0.3)\n RGB{Float64}(0.4,0.5,0.6)\n\njulia> chanv = channelview(c)\n3×2 reinterpret(Float64, ::Array{RGB{Float64},2}):\n 0.1  0.4\n 0.2  0.5\n 0.3  0.6","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"colorview and channelview always return a view of the original array.","category":"page"},{"location":"tutorials/conversions_views/#Using-colorview-to-make-color-overlays","page":"Conversions vs. views","title":"Using colorview to make color overlays","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Another use for colorview is to combine multiple grayscale images into a single color image. For example:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"using Colors, Images\nr = range(0,stop=1,length=11)\nb = range(0,stop=1,length=11)\nimg1d = colorview(RGB, r, zeroarray, b)\n\n# output\n\n11-element mappedarray(RGB{Float64}, ImageCore.extractchannels, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::ImageCore.ZeroArray{Float64,1,Base.OneTo{Int64}}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) with eltype RGB{Float64}:\n RGB{Float64}(0.0,0.0,0.0)\n RGB{Float64}(0.1,0.0,0.1)\n RGB{Float64}(0.2,0.0,0.2)\n RGB{Float64}(0.3,0.0,0.3)\n RGB{Float64}(0.4,0.0,0.4)\n RGB{Float64}(0.5,0.0,0.5)\n RGB{Float64}(0.6,0.0,0.6)\n RGB{Float64}(0.7,0.0,0.7)\n RGB{Float64}(0.8,0.0,0.8)\n RGB{Float64}(0.9,0.0,0.9)\n RGB{Float64}(1.0,0.0,1.0)","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"results (in IJulia) in","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"(Image: linspace)","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"zeroarray is a special constant that \"expands\" to return the equivalent of an all-zeros array with axes matching the other inputs to colorview.","category":"page"},{"location":"tutorials/conversions_views/#Changing-the-order-of-dimensions","page":"Conversions vs. views","title":"Changing the order of dimensions","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"When you've separated colors into a separate color dimension, some code might assume that color is the last (slowest) dimension. You can convert directly using Julia's permutedims function:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> pc = permutedims(a, (2,1))\n2×3 Array{Float64,2}:\n 0.1  0.2  0.3\n 0.4  0.5  0.6","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"permutedims explicitly creates a new array with the data rearranged in memory. It's also possible to perform something similar as a view:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> pv = PermutedDimsArray(a, (2,1))\n2×3 PermutedDimsArray(::Array{Float64,2}, (2, 1)) with eltype Float64:\n 0.1  0.2  0.3\n 0.4  0.5  0.6","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"While this looks the same, pv (unlike pc) shares memory with a; this is an apparent permutation, achieved by having the indexing of a PermutedDimsArray swap the input indexes whenever individual elements are accessed.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"One thing to be aware of is that the performance of these two might differ, for reasons that have to do with how CPUs and memory work rather than any limitation of Julia. If a is large and you want to access all three elements corresponding to the color channels of a single pixel, pv will likely be more efficient because values are adjacent in memory and thus likely share a cache line. Conversely, if you want to access different pixels from a single color channel sequentially, pc may be more efficient (for the same reason).","category":"page"},{"location":"tutorials/conversions_views/#Adding-padding","page":"Conversions vs. views","title":"Adding padding","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Sometimes when you want to compare two images, one might be of a different size than another. You can create array views that have common indices with paddedviews:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> a1 = reshape([1,2], 2, 1)\n2×1 Array{Int64,2}:\n 1\n 2\n\njulia> a2 = [1.0,2.0]'\n1×2 LinearAlgebra.Adjoint{Float64,Array{Float64,1}}:\n 1.0  2.0\n\njulia> a1p, a2p = paddedviews(0, a1, a2);   # 0 is the fill value\n\njulia> a1p\n2×2 PaddedView(0, ::Array{Int64,2}, (Base.OneTo(2), Base.OneTo(2))) with eltype Int64:\n 1  0\n 2  0\n\njulia> a2p\n2×2 PaddedView(0.0, ::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, (Base.OneTo(2), Base.OneTo(2))) with eltype Float64:\n 1.0  2.0\n 0.0  0.0","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"This can be especially useful in conjunction with colorview to compare two (or more) grayscale images. See Keeping track of location with unconventional indices for more information.","category":"page"},{"location":"tutorials/conversions_views/#StackedViews","page":"Conversions vs. views","title":"StackedViews","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"Sometimes it's helpful to combine several images into a single view for further array-like manipulation.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"julia> img1 = reshape(1:8, (2,4))\n2×4 reshape(::UnitRange{Int64}, 2, 4) with eltype Int64:\n 1  3  5  7\n 2  4  6  8\n\njulia> img2 = reshape(11:18, (2,4))\n2×4 reshape(::UnitRange{Int64}, 2, 4) with eltype Int64:\n 11  13  15  17\n 12  14  16  18\n\njulia> sv = StackedView(img1, img2)\n2×2×4 StackedView{Int64,3,Tuple{Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}},Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}}:\n[:, :, 1] =\n  1   2\n 11  12\n\n[:, :, 2] =\n  3   4\n 13  14\n\n[:, :, 3] =\n  5   6\n 15  16\n\n[:, :, 4] =\n  7   8\n 17  18\n\njulia> imgMatrix = reshape(sv, (2, 8))\n2×8 reshape(::StackedView{Int64,3,Tuple{Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}},Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}}, 2, 8) with eltype Int64:\n  1   2   3   4   5   6   7   8\n 11  12  13  14  15  16  17  18","category":"page"},{"location":"tutorials/conversions_views/#Decoupling-views-from-the-parent-memory","page":"Conversions vs. views","title":"Decoupling views from the parent memory","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"If you want to use some of these views but have an application where the sharing of memory is actually problematic, keep in mind that you can always call Julia's copy function to create a copy of the array. The type of the resulting copy might not be identical to the original, but the values will be the same.","category":"page"},{"location":"tutorials/conversions_views/#Composing-views-(and-compact-summaries)","page":"Conversions vs. views","title":"Composing views (and compact summaries)","text":"","category":"section"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"When Julia displays an array as text, there is usually a 1-line summary at the top showing the array type. You may have already noticed that JuliaImages uses an unconventional syntax for summarizing information about certain kinds of arrays. For example, the type of pv above is","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"PermutedDimsArray{Float64,2,(2,1),(2,1),Array{Float64,2}}","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"but when you display such an object, in the summary line it prints as","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"2×3 PermutedDimsArray(::Array{Float64,2}, (2, 1)) with eltype Float64","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"This is intended to result in more easily-readable information about types.","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"The main motivation for this is that different view types can be combined freely, and when you do so sometimes the type gets quite long. For example, suppose you have a disk file storing a m×n×3×t UInt8 array representing an RGB movie (t being the time axis). To have it display as an RGB movie, you might create the following view of the array A:","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"using Images\nusing Random\nRandom.seed!(1234)","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"A = rand(UInt8, 5, 6, 3, 10);\nmov = colorview(RGB, normedview(PermutedDimsArray(A, (3,1,2,4))));\n\nsummary(mov)\ntypeof(mov)","category":"page"},{"location":"tutorials/conversions_views/","page":"Conversions vs. views","title":"Conversions vs. views","text":"While there is little or no performance cost to making use of JuliaImage's convenient views, sometimes the types can get complicated!","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/#Histogram-equalisation","page":"Histogram equalisation","title":"Histogram equalisation","text":"","category":"section"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"This demo issustrates the use of Histogram equalization, gamma correction matching and Contrast Limited Adaptive Histogram Equalization","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"Histogram equalisation is used to imporve the contrast in an single-channel grayscale image. It distributes the intensity of the image in a uniform manner. The natural justification for uniformity is that the image has better contrast if the intensity levels of an image span a wide range on the intensity scale. The transformation is based on mapping of cumulative histogram","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"using ImageContrastAdjustment, TestImages, ImageCore\n\nimg = testimage(\"moonsurface\")","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"Now we will apply Histogram equalisation, gamma correction and Adaptive histogram equalisation method to enhance contrast of the image","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"hist_equal = adjust_histogram(img, Equalization(nbins = 256))\ngamma_correction = adjust_histogram(img, GammaCorrection(gamma = 2))\nhist_adapt = adjust_histogram(img, AdaptiveEqualization(nbins = 256, rblocks = 4, cblocks = 4, clip = 0.2))\n\nmosaicview(img, hist_equal, gamma_correction, hist_adapt; nrow = 1)","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"","category":"page"},{"location":"democards/examples/spatial_transformation/histogram_equalization/","page":"Histogram equalisation","title":"Histogram equalisation","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"install/#page_get_started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"install/","page":"Getting started","title":"Getting started","text":"Most users probably want to start with the Images.jl package, which bundles much (but not all) of the functionality in JuliaImages.","category":"page"},{"location":"install/#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"install/","page":"Getting started","title":"Getting started","text":"Images (and possibly some additional packages) may be all you need to manipulate images programmatically. You can install Images.jl via the package manager,","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"(v1.0) pkg> add Images","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"note: Note\nPeople in some regions such as China might fail to install/precompile Images due to poor network status. Using proxy/VPN that has stable connection to Amazon S3 and Github can solve this issue.","category":"page"},{"location":"install/#sec_imageio","page":"Getting started","title":"Loading your first image","text":"","category":"section"},{"location":"install/","page":"Getting started","title":"Getting started","text":"If this is your first time working with images in julia, it's likely that you'll need to install some image IO backends to load the images. The current available backends for image files are:","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"ImageMagick.jl covers most image formats and has extra functionality. This can be your first choice if you don't have a preference.\nQuartzImageIO.jl exposes macOS's native image IO functionality to Julia. In some cases it's faster than ImageMagick, but it might not cover all your needs.\nImageIO.jl is a new image IO backend (requires julia >=v\"1.3\") that provides an optimized performance for PNG files. Check benchmark here\nOMETIFF.jl supports OME-TIFF files. If you don't know what it is, then it is likely that you don't need this package.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"These backends aren't exclusive to each other, so if you're a macOS user, you can install all these backends. And in most cases, you don't need to directly interact with these backends, instead, we use the save and load provided by the FileIO.jl frontend. If you've installed multiple backends then FileIO will choose the most appropriate backend acoording to your file format. For example, if available ImageIO is used to load PNG files.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"Adding these gives you a basic image IO setup:","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"(v1.0) pkg> add FileIO ImageMagick ImageIO","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"and to load an image, you can use","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"using Images, FileIO\nusing ImageShow # hide\n# specify the path to your local image file\nimg_path = \"/path/to/image.png\"\nimg_path = joinpath(\"assets\", \"installation\", \"mandrill.tiff\") # hide\nimg = load(img_path)","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"When testing ideas or just following along with the documentation, it can be useful to have some images to work with. The TestImages.jl package bundles several \"standard\" images for you.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"(v1.0) pkg> add TestImages","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"To load one of the images from this package, say","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"using TestImages\n# backends such as ImageMagick are required\nimg = testimage(\"mandrill\")","category":"page"},{"location":"install/#sec_visualization","page":"Getting started","title":"Displaying images","text":"","category":"section"},{"location":"install/","page":"Getting started","title":"Getting started","text":"When working with images, it's obviously helpful to be able to look at them.  If you use Julia through Juno or IJulia, images should display automatically:","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"(Image: IJulia)","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"Currently there're five julia packages can be used to display an image:","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"ImageShow is used to support image display in Juno and IJulia. This happens automatically if you are using Images.\nImageInTerminal is used to support image display in terminal.\nImageView is an image display GUI. (For OSX and Windows platforms, Julia at least v1.3 is required)\nPlots maintained by JuliaPlots is a general plotting package that support image display.\nMakie is also maintained by JuliaPlots but provides rich interactive functionality. ","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"To visualize multiple images in one frame, you can create a bigger image from multiple image sources with mosaicview, which is an enhanced version of cats.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"using Images, TestImages","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"img1 = testimage(\"mandrill\") # 512*512 RGB image\nimg2 = testimage(\"blobs\") # 254*256 Gray image\nmosaicview(img1, img2; nrow=1)","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"img = testimage(\"mri-stack\") # 226×186×27 Gray image\nmosaicview(img; fillvalue=0.5, npad=2, ncol=7, rowmajor=true)","category":"page"},{"location":"install/#Troubleshooting","page":"Getting started","title":"Troubleshooting","text":"","category":"section"},{"location":"install/","page":"Getting started","title":"Getting started","text":"Reading and writing images, as well as graphical display, involve interactions with external software libraries; occasionally, the installation of these libraries goes badly. Fortunately, the artifact system shipped since Julia 1.3 has made this process much more reliable, so if you're experiencing any installation trouble, please try with Julia 1.3 or higher.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"This documentation is generated with the following environment setup. While reading the documentation, if you encounter any errors or if the outputs in your local machine differ from the documentation, you could first check the Julia and package versions you're using. If the error or inconsistency still exists, please file an issue for that; it helps us improve the documentation.","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"using InteractiveUtils","category":"page"},{"location":"install/","page":"Getting started","title":"Getting started","text":"using Pkg, Dates\ntoday()\nversioninfo()\nPkg.status()","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/#Structural-Similarity-Index","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"","category":"section"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"When comparing images, the Mean Squared Error (or MSE), though straightforward to calculate, may not be a very good indicator of their perceived similarity.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"The Structural Similarity Index (or SSIM) aims to address this shortcoming by taking texture into account, and assigning a higher score to images that may appear similar.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"using Images, TestImages\nusing Random\n\nimg_orig = float64.(testimage(\"cameraman\"))","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"We use a grayscale image out of the TestImages package, which provides a standard suite of test images. float/float32/float64 preserve colorant information: thus the image is now composed of pixels of type Gray{Float64}.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"assess_ssim(img_orig, img_orig)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"The assess_ssim function, which takes two images as inputs and returns their structural similarity index, is the simplest way to calculate the SSIM of two images.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"An SSIM score of 1.00 indicates perfect structural similarity, as is expected out of identical images.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"Now, we create two variations of the original image: image_const on the left has the intensity of all its pixels increased by 0.2 times the intensity range, while image_noise on the right has the intensity of some of its pixels increased, and that of the others decreased by the same amount. The two images look quite different visually.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"noise = ones(size(img_orig)) .* 0.2 .* (maximum(img_orig) - minimum(img_orig))\nimg_const = img_orig + noise\n\nmask = rand(Float64, size(img_orig)) .< 0.5\nnoise[mask] = noise[mask] .* -1\nimg_noise = img_orig + noise\n\nmosaicview(img_const, img_noise; nrow=1)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"We use the mse funtion defined in ImageDistances to calculate the mean squared error between the original and the two modified images.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"mse(img_orig, img_const), mse(img_orig, img_noise)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"Despite their visual differences, both the images have the exact same mean squared error of 0.400, when compared with the original. This demonstrates how in certain cases, MSE can fail to capture the perceived similarity of images.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"assess_ssim(img_orig, img_const), assess_ssim(img_orig, img_noise)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"Their SSIM scores vary significantly, with image_const being rated much closer to the original image in terms of perceived similarity, which is in line with what visually seems to be the case.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/#Custom-Parameters","page":"Structural Similarity Index","title":"Custom Parameters","text":"","category":"section"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"While assess_ssim is a convenient way to calculate the SSIM of two images, it does not allow for custom parameters to be passed to the SSIM algorithm, for which we have the following syntax.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"iqi = SSIM(KernelFactors.gaussian(2.0, 11), (0.5, 0.5, 0.5))\nassess(iqi, img_orig, img_const)","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"Here, the first parameter is the kernel used to weight the neighbourhood of each pixel while calculating the SSIM locally, and defaults to KernelFactors.gaussian(1.5, 11). The second parameter is the set of weights (α, β, γ) given to the lunimance (L), contrast (C) and structure (S) terms while calculating the SSIM, and defaults to (1.0, 1.0, 1.0). Recall that SSIM is defined as Lᵅ × Cᵝ × Sᵞ.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/#References","page":"Structural Similarity Index","title":"References","text":"","category":"section"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"Zhou Wang; Bovik, A.C.; ,”Mean squared error: Love it or leave it? A new look at Signal Fidelity Measures,” Signal Processing Magazine, IEEE, vol. 26, no. 1, pp. 98-117, Jan. 2009.\nZ. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"","category":"page"},{"location":"democards/examples/image_quality_and_benchmarks/structural_similarity_index/","page":"Structural Similarity Index","title":"Structural Similarity Index","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"tutorials/quickstart/#page_quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"If you're comfortable with Julia or have used another image-processing package before, this page may help you get started quickly. If some of the terms or concepts here seem strange, don't worry–-there are much more detailed explanations in the following sections.","category":"page"},{"location":"tutorials/quickstart/#Images-are-just-arrays","page":"Quickstart","title":"Images are just arrays","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"using Images, ImageDraw\n\nmake_roi(tl::Point, br::Point) = Polygon([tl, Point(br.x, tl.y), br, Point(tl.x, br.y)])\nmake_roi(xs::UnitRange, ys::UnitRange) = make_roi(Point(ys[1], xs[1]), Point(ys[end], xs[end]))","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"For most purposes, any AbstractArray can be treated as an image. For example, numeric array can be interpreted as a grayscale image.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img = rand(4, 3)","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Gray.(img) #hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"We could also select a region-of-interest from a larger image","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"# generate an image that starts black in the upper left\n# and gets bright in the lower right\nimg = Array(reshape(range(0,stop=1,length=10^4), 100, 100))\n# make a copy\nimg_c = img[51:70, 21:70] # red\n# make a view\nimg_v = @view img[16:35, 41:90] # blue\n\nout = vcat(img, hcat(img_c, img_v)) # hide\nout = RGB.(Gray.(out)) # hide\nimg_boundary = make_roi(1:100, 1:100) # hide\nroi_c = make_roi(51:70, 21:70) # hide\nroi_v = make_roi(16:35, 41:90) # hide\nroi_c_boundary = make_roi(101:120, 1:50) # hide\nroi_v_boundary = make_roi(101:120, 51:100) # hide\ndraw!(out, img_boundary, RGB{Float64}(0, 0, 0)) # hide\ndraw!(out, roi_c_boundary, RGB{Float64}(1, 0, 0)) # hide\ndraw!(out, roi_c, RGB{Float64}(1, 0, 0)) # hide\ndraw!(out, roi_v_boundary, RGB{Float64}(0, 0, 1)) # hide\ndraw!(out, roi_v, RGB{Float64}(0, 0, 1)) # hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"As you might know, changing the value of a view would affect the original image, while changing that of a copy doesn't:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"fill!(img_c, 1) # red region in original doesn't change\nfill!(img_v, 0) # blue\n\nout = vcat(img, hcat(img_c, img_v)) # hide\nout = RGB.(Gray.(out)) # hide\nimg_boundary = make_roi(1:100, 1:100) # hide\nroi_c = make_roi(51:70, 21:70) # hide\nroi_v = make_roi(16:35, 41:90) # hide\nroi_c_boundary = make_roi(101:120, 1:50) # hide\nroi_v_boundary = make_roi(101:120, 51:100) # hide\ndraw!(out, img_boundary, RGB{Float64}(0, 0, 0)) # hide\ndraw!(out, roi_c_boundary, RGB{Float64}(1, 0, 0)) # hide\ndraw!(out, roi_c, RGB{Float64}(1, 0, 0)) # hide\ndraw!(out, roi_v_boundary, RGB{Float64}(0, 0, 1)) # hide\ndraw!(out, roi_v, RGB{Float64}(0, 0, 1)) # hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Don't worry if you don't get the \"image\" result, that's expected and you'll learn how to automatically display an image later in JuliaImages.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Some add-on packages enable additional behavior. For example,","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"using Unitful, AxisArrays\nusing Unitful: mm, s\n\nimg = AxisArray(rand(256, 256, 6, 50), (:x, :y, :z, :time), (0.4mm, 0.4mm, 1mm, 2s))\nnothing # hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"defines a 4d image (3 space dimensions plus one time dimension) with the specified name and physical pixel spacing for each coordinate. The AxisArrays package supports rich and efficient operations on such arrays, and can be useful to keep track of not just pixel spacing but the orientation convention used for multidimensional images.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"JuliaImages interoperates smoothly with AxisArrays and many other packages.  As further examples,","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"the ImageMetadata package (incorporated into Images itself) allows you to \"tag\" images with custom metadata\nthe IndirectArrays package supports indexed (colormap) images\nthe MappedArrays package allows you to represent lazy value-transformations, facilitating work with images that may be too large to store in memory at once\nImageTransformations allows you to encode rotations, shears, deformations, etc., either eagerly or lazily","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"It is very easy to define new array types in Julia–and consequently specialized images or operations–and have them interoperate smoothly with the vast majority of functions in JuliaImages.","category":"page"},{"location":"tutorials/quickstart/#Array-elements-are-pixels-(and-vice-versa)","page":"Quickstart","title":"Array elements are pixels (and vice versa)","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"using Images, MosaicViews","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Elements of image are called pixels; in JuliaImages we provide an abstraction on this concept. For example, we have Gray for grayscale image, RGB for RGB image, Lab for Lab image, and etc.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Creating a pixel is initializing a struct of that type:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Gray(0.0) # black\nGray(1.0) # white\nRGB(1.0, 0.0, 0.0) # red\nRGB(0.0, 1.0, 0.0) # green\nRGB(0.0, 0.0, 1.0) # blue\n[RGB.(Gray(0.0)) RGB.(Gray(1.0)) RGB(1.0, 0.0, 0.0) RGB(0.0, 1.0, 0.0) RGB(0.0, 0.0, 1.0)] # hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"and image is just an array of pixel objects:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_gray = rand(Gray, 2, 2)\nimg_rgb = rand(RGB, 2, 2)\nimg_lab = rand(Lab, 2, 2)","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"mosaicview(RGB.(img_gray), RGB.(img_rgb), RGB.(img_lab), # hide\n           fillvalue=RGB(1, 1, 1), # hide\n           nrow=1, npad=2) # hide","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"As you can see, both img_rgb and img_lab images are of size 2 times 2 (instead of 2 times 2 times 3 or 3 times 2 times 2); a RGB image is an array of RGB pixels whereas a Lab image is an array of Lab pixel.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nIt's recommended to use Gray instead of the Number type in JuliaImages since it indicates that the array of numbers is best interpreted as a grayscale image. For example, it triggers Atom/Juno and Jupyter to display the array as an image instead of a matrix of numbers. There's no performance overhead for using Gray over Number.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"This design choice facilitates generic code that can handle both grayscale and color images without needing to introduce extra loops or checks for a color dimension. It also provides more rational support for 3d grayscale images–which might happen to have size 3 along the third dimension–and consequently helps unify the \"computer vision\" and \"biomedical image processing\" communities.","category":"page"},{"location":"tutorials/quickstart/#Color-conversions-are-construction/view","page":"Quickstart","title":"Color conversions are construction/view","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Conversions between different Colorants are straightforward:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"RGB.(img_gray) # Gray => RGB\nGray.(img_rgb) # RGB => Gray","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nYou'll see broadcasting semantics used in JuliaImages here and there, check the documentation if you're not familiar with it.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Sometimes, to work with other packages, you'll need to convert a m times n RGB image to m times n times 3 numeric array and vice versa. The functions channelview and colorview are designed for this purpose. For example:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_CHW = channelview(img_rgb) # 3 * 2 * 2\nimg_HWC = permutedims(img_CHW, (2, 3, 1)) # 2 * 2 * 3","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_CHW = permutedims(img_HWC, (3, 1, 2)) # 3 * 2 * 2\nimg_rgb = colorview(RGB, img_CHW) # 2 * 2","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"warning: Warning\nDon't overuse channelview because it loses the colorant information by converting an image to a raw numeric array.It's very likely that users from other languages will have the tendency to channelview every image they're going to process. Unfamiliarity of the pixel concept provided by JuliaImages doesn't necessarily mean it's bad.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nThe reason we use CHW (i.e., channel-height-width) order instead of HWC is that this provides a memory friendly indexing mechanisim for Array. By default, in Julia the first index is also the fastest (i.e., has adjacent storage in memory). For more details, please refer to the performance tip: Access arrays in memory order, along columnsYou can use PermutedDimsArray to \"reinterpret\" the orientation of a chunk of memory without making a copy, or permutedims if you want a copy.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"For Gray images, the following codes are almost equivalent except that the construction version copies the data while the view version doesn't.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_num = rand(4, 4)\n\nimg_gray_copy = Gray.(img_num) # construction\nimg_num_copy = Float64.(img_gray_copy) # construction\n\nimg_gray_view = colorview(Gray, img_num) # view\nimg_num_view = channelview(img_gray_view) # view\nnothing # hide","category":"page"},{"location":"tutorials/quickstart/#The-0-to-1-intensity-scale","page":"Quickstart","title":"The 0-to-1 intensity scale","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"using ImageCore, ImageShow, FixedPointNumbers","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"In JuliaImages, by default all images are displayed assuming that 0 means \"black\" and 1 means \"white\" or \"saturated\" (the latter applying to channels of an RGB image).","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Perhaps surprisingly, this 0-to-1 convention applies even when the intensities are encoded using only 8-bits per color channel. JuliaImages uses a special type, N0f8, that interprets an 8-bit \"integer\" as if it had been scaled by 1/255, thus encoding values from 0 to 1 in 256 steps.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"N0f8 numbers (standing for Normalized, with 0 integer bits and 8 fractional bits) obey standard mathematical rules, and can be added, multiplied, etc. There are types like N0f16 for working with 16-bit images (and even N2f14 for images acquired with a 14-bit camera, etc.).","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_n0f8 = rand(N0f8, 2, 2)\nfloat.(img_n0f8)","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nThis infrastructure allows us to unify \"integer\" and floating-point images, and avoids the need for special conversion functions (e.g., im2double in MATLAB) that change the value of pixels when your main goal is simply to change the type (numeric precision and properties) used to represent the pixel.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Although it's not recommended, but you can use rawview to get the underlying storage data and convert it to UInt8 (or other types) if you insist.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_n0f8_raw = rawview(img_n0f8)\nfloat.(img_n0f8_raw)","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Conversions between the storage type, i.e., the actual numeric type, without changing the color type are supported by the following functions:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"float32, float64\nn0f8, n6f10, n4f12, n2f14, n0f16","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img = rand(Gray{N0f8}, 2, 2)\nimg_float32 = float32.(img) # Gray{N0f8} => Gray{Float32}\nimg_n0f16 = n0f16.(img_float32) # Gray{Float32} => Gray{N0f16}","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"If you don't want to specify the destination type, float is designed for this:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"img_n0f8 = rand(Gray{N0f8}, 2, 2)\nimg_float = float.(img_n0f8) # Gray{N0f8} => Gray{Float32}","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"For a view-like conversion without new memory allocation, of_eltype in MappedArrays is designed for this:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"using MappedArrays\nimg_float_view = of_eltype(Gray{Float32}, img_n0f8)\neltype(img_float_view)","category":"page"},{"location":"tutorials/quickstart/#Arrays-with-arbitrary-indices","page":"Quickstart","title":"Arrays with arbitrary indices","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"If you have an input image and perform some kind of spatial transformation on it, how do pixels/voxels in the transformed image match up to pixels in the input? Through Julia's support for arrays with indices that start at values other than 1, it is possible to allow array indices to represent absolute position in space, making it straightforward to keep track of the correspondence between location across multiple images. More information can be found in Keeping track of location with unconventional indices.","category":"page"},{"location":"tutorials/quickstart/#Function-categories","page":"Quickstart","title":"Function categories","text":"","category":"section"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"See Summary and function reference for more information about each of the topics below. The list below is accessible via ?Images from the Julia REPL. If you've used other frameworks previously, you may also be interested in the Comparison with other image processing frameworks. Also described are the ImageFeatures.jl and ImageSegmentation.jl packages, which support a number of algorithms important for computer vision.","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Constructors, conversions, and traits:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Construction: use constructors of specialized packages, e.g., AxisArray, ImageMeta, etc.\n\"Conversion\": colorview, channelview, rawview, normedview, permuteddimsview, paddedviews\nTraits: pixelspacing, sdims, timeaxis, timedim, spacedirections","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Contrast/coloration:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"clamp01, clamp01nan, scaleminmax, colorsigned, scalesigned","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Algorithms:","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Reductions: maxfinite, maxabsfinite, minfinite, meanfinite, sad, ssd, integral_image, boxdiff, gaussian_pyramid\nResizing and spatial transformations: restrict, imresize, warp\nFiltering: imfilter, imfilter!, imfilter_LoG, mapwindow, imROF, padarray\nFiltering kernels: Kernel. or KernelFactors., followed by ando[345], guassian2d, imaverage, imdog, imlaplacian, prewitt, sobel\nExposure : build_histogram, adjust_histogram, imadjustintensity, imstretch, imcomplement, AdaptiveEqualization, GammaCorrection, cliphist\nGradients: backdiffx, backdiffy, forwarddiffx, forwarddiffy, imgradients\nEdge detection: imedge, imgradients, thin_edges, magnitude, phase, magnitudephase, orientation, canny\nCorner detection: imcorner, harris, shi_tomasi, kitchen_rosenfeld, meancovs, gammacovs, fastcorners\nBlob detection: blob_LoG, findlocalmaxima, findlocalminima\nMorphological operations: dilate, erode, closing, opening, tophat, bothat, morphogradient, morpholaplace, feature_transform, distance_transform\nConnected components: label_components, component_boxes, component_lengths, component_indices, component_subscripts, component_centroids\nInterpolation: bilinear_interpolation","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"Test images and phantoms (see also TestImages.jl):","category":"page"},{"location":"tutorials/quickstart/","page":"Quickstart","title":"Quickstart","text":"shepp_logan","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/#RGB-to-HSV-and-thresholding","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"","category":"section"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"This example illustrates how RGB to HSV (Hue, Saturation, Value) conversion can be used to facilitate segmentation processes.","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"using Images, TestImages, LinearAlgebra\n\nrgb_img = testimage(\"lighthouse\")\nhsv_img = HSV.(rgb_img)\nchannels = channelview(float.(hsv_img))\nhue_img = channels[1,:,:]\nvalue_img = channels[3,:,:]\nsaturation_img = channels[2,:,:]\nnothing #hide","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"A simple segmentation of the image can then be effectively performed by a mere thresholding of the HSV channels.","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"mask = zeros(size(hue_img))\nh, s, v = 80, 150, 150\nfor ind in eachindex(hue_img)\n    if hue_img[ind] <= h && saturation_img[ind] <= s/255 && value_img[ind] <= v/255\n        mask[ind] = 1\n    end\nend\nbinary_img = colorview(Gray, mask)","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"The obtained binary image can be used as a mask on the original RGB image.","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"segmented_img = mask .* rgb_img\n\nhcat(rgb_img, binary_img, segmented_img)","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"","category":"page"},{"location":"democards/examples/color_channels/rgb_hsv_thresholding/","page":"RGB to HSV and thresholding","title":"RGB to HSV and thresholding","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/#Image-Compression-using-SVD","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"","category":"section"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"This demonstration shows how to work with color channels to explore image compression using the Singular Value Decomposition (SVD).","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"using Images, TestImages\nusing LinearAlgebra\n\nimg = float.(testimage(\"mandrill\"))\nchannels = channelview(img)\n\nfunction rank_approx(F::SVD, k)\n    U, S, V = F\n    M = U[:, 1:k] * Diagonal(S[1:k]) * V[:, 1:k]'\n    clamp01!(M)\nend\nnothing #hide","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"For each channel, we do SVD decomposition, and then reconstruct the channel using only the K largest singular values.","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"The image is compressed because for each channel we only need to save two small matrices and one vector – truncated part of (U, S, V). For example, if the original image is gray image of size (512, 512), and we rebuild the image with 50 singular values, then we only need to save 2 times 512 times 50 + 50 numbers to rebuild the image, while original image has 512 times 512 numbers. Hence this gives us a compression ratio 1955 if we don't consider the storage type.","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"# before julia v1.1:\n# svdfactors = (svd(channels[1,:,:]), svd(channels[2,:,:]), svd(channels[3,:,:]))\nsvdfactors = svd.(eachslice(channels; dims=1))\nimgs = map((10, 50, 100)) do k\n    colorview(RGB, rank_approx.(svdfactors, k)...)\nend\n\nmosaicview(img, imgs...; nrow=1, npad=10)","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"From left to right: original image, reconstructed images using 10, 50, 100 largest singular values. We can see that 50 largest singular values are capable of rebuilding a pretty good image.","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"","category":"page"},{"location":"democards/examples/color_channels/color_separations_svd/","page":"Image Compression using SVD","title":"Image Compression using SVD","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"tutorials/arrays_colors/#page_arrays_colors","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"DocTestSetup = quote\n    using Random\n    Random.seed!(2)\nend","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"In Julia, an image is just an array, and many of the ways you manipulate images come from the general methods to work with multidimensional arrays. For example,","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> img = rand(2,2)\n2×2 Array{Float64,2}:\n 0.366796  0.210256\n 0.523879  0.819338","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"defines an \"image\" img of 64-bit floating-point numbers. You should be able to use this as an image in most or all functions in JuliaImages.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"We'll be talking quite a bit about handling arrays. This page will focus on the \"element type\" (eltype) stored in the array. In case you're new to Julia, if a is an array of integers:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> a = [1,2,3,4]\n4-element Array{Int64,1}:\n 1\n 2\n 3\n 4","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"then either of the following creates a new array where the element type is Float64:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"map(Float64, a)\nFloat64.(a)     # short for broadcast(Float64, a)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"For example,","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> Float64.(a)\n4-element Array{Float64,1}:\n 1.0\n 2.0\n 3.0\n 4.0","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Arrays are indexed with square brackets (a[1]), with indexing starting at 1 by default. A two-dimensional array like img can be indexed as img[2,1], which would be the second row, first column. Julia also supports \"linear indexing,\" using a single integer to address elements of an arbitrary multidimensional array in a manner that (in simple cases) reflects the memory offset of the particular element. For example, img[3] corresponds to img[1,2] (numbering goes down columns, and then wraps around at the top of the next column, because Julia arrays are stored in \"column major\" order where the fastest dimension is the first dimension).","category":"page"},{"location":"tutorials/arrays_colors/#Numbers-versus-colors","page":"Arrays, Numbers, and Colors","title":"Numbers versus colors","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"For the array img we created above, you can display it as a grayscale image using ImageView. But if you happen to be following along in Juno or IJulia, you might notice that img does not display as an image: instead, it prints as an array of numbers as shown above.  Arrays of \"plain numbers\" are not displayed graphically, because they might represent something numerical (e.g., a matrix used for linear algebra) rather than an image. To indicate that this is worthy of graphical display, convert the element type to a color chosen from the Colors package:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: float_gray)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Here we used Gray to indicate that this array should be interpreted as a grayscale image. (Note that the Images package re-exports Colors, so you can alternatively say using Images.)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Under the hood, what is Gray doing?  It's informative to see the \"raw\" object, displayed as text:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: float_gray_text)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Users of Juno or the Julia command-line REPL interface will see this representation immediately.)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"You can see this is a 2×2 array of Gray{Float64} objects. You might be curious how these Gray objects are represented. In the command-line REPL, it looks like this (the same command works with IJulia):","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> dump(imgg[1,1])\nColorTypes.Gray{Float64}\n  val: Float64 0.36679641243992434","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"dump shows the \"internal\" representation of an object.  You can see that Gray is a type (technically, an immutable struct) with a single field val; for Gray{Float64}, val is a 64-bit floating point number. Using val directly is not recommended: you can extract the Float64 value with the accessor functions real or gray (the reason for the latter name will be clearer when we discuss RGB colors).","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"What kind of overhead do these objects incur?","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> sizeof(img)\n32\n\njulia> sizeof(imgg)\n32","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"The answer is \"none\": they don't take up any memory of their own, nor do they typically require any additional processing time. The Gray \"wrapper\" is just an interpretation of the values, one that helps clarify that this should be displayed as a grayscale image.  Indeed, img and imgg compare as equal:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> img == imgg\ntrue","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"There's more to say on this topic, but we'll wait until we discuss Conversions vs. views.","category":"page"},{"location":"tutorials/arrays_colors/#Colors-beyond-the-pale","page":"Arrays, Numbers, and Colors","title":"Colors beyond the pale","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Gray is not the only color in the universe:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: randrgb)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Let's look at imgc as text (shown here in the REPL):","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> imgc\n2×2 Array{ColorTypes.RGB{Float32},2}:\n RGB{Float32}(0.75509,0.965058,0.65486)     RGB{Float32}(0.696203,0.142474,0.783316)\n RGB{Float32}(0.705195,0.953892,0.0744661)  RGB{Float32}(0.571945,0.42736,0.548254)\n\njulia> size(imgc)\n(2,2)\n\njulia> dump(imgc[1,1])\nColorTypes.RGB{Float32}\n  r: Float32 0.7550899\n  g: Float32 0.9650581\n  b: Float32 0.65485954","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Here we see one of the primary differences between Julia's approach to images and that of several other popular frameworks: imgc does not have a dimension of the array devoted to the \"color channel.\" Instead, every element of the array corresponds to a complete pixel's worth of information. Often this simplifies the logic of many algorithms, sometimes allowing a single implementation to work for both color and grayscale images.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"You can extract the individual color channels using their field names (r, g, and b), but as you'll see in a moment, a more universal approach is to use accessor functions:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> c = imgc[1,1]; (red(c), green(c), blue(c))\n(0.7550899f0,0.9650581f0,0.65485954f0)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Julia's Colors package allows the same color to be represented in several different ways, and this can facilitate interaction with other tools. For example, certain C libraries permit or prefer the order of the color channels to be different:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> dump(BGR(c))\nColorTypes.BGR{Float32}\n  b: Float32 0.65485954\n  g: Float32 0.9650581\n  r: Float32 0.7550899","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"or even to pack the red, green, and blue colors–-together with a dummy \"alpha\" (transparency) channel–-into a single 32-bit integer:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> c24 = RGB24(c); dump(c24)\nColorTypes.RGB24\n  color: UInt32 12711591\n\njulia> c24.color\n0x00c1f6a7","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"From first (the first two hex-digits after the \"0x\") to last (the final two hex-digits), the order of the channels here is alpha, red, green, blue:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> 0xc1/0xff\n0.7568627450980392\n\njulia> 0xf6/0xff\n0.9647058823529412\n\njulia> 0xa7/0xff\n0.6549019607843137","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"These values are close to the channels of c, but have been rounded off–-each channel is encoded with only 8 bits, so some approximation of the exact floating-point value is unavoidable.","category":"page"},{"location":"tutorials/arrays_colors/#fixedpoint","page":"Arrays, Numbers, and Colors","title":"A consistent scale for floating-point and \"integer\" colors: fixed-point numbers","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"c24 does not have an r field, but we can still use red to extract the red channel:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> r = red(c24)\n0.757N0f8","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"This may look fairly strange at first, so let's unpack this carefully. Notice first that the \"floating-point\" portion of this number matches (to within the precision of the rounding) the value of red(c). The N0f8 means \"Normalized with 8 fractional bits, with 0 bits left for representing values higher than 1.\" This is a fixed-point number–-rather like floating-point numbers, except that the decimal does not \"float\". Internally, these are represented in terms of the 8-bit unsigned integer UInt8","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> dump(r)\nFixedPointNumbers.Normed{UInt8,8}\n  i: UInt8 193","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Note that N0f8 is an abbreviation; the full typename is Normed{UInt8, 8}.) N0f8 interprets this 8-bit integer as a value lying between 0 and 1, with 0 corresponding to 0x00 and 1 corresponding to 0xff. This interpretation affects how the number is used for arithmetic and conversion to and from other values. Stated another way, r behaves as","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> r == 193/255\ntrue","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"for essentially all purposes (but see A note on arithmetic overflow).","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"This has a very important consequence: in many other image frameworks, the \"meaning\" of an image depends on how it is stored, but in Julia the meaning can be assigned independently of storage representation. For example, in a different language/framework, the following sequence:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"img = uint8(255*rand(10, 10, 3));\nfigure; image(img)\nimgd = double(img);   % convert to double-precision, but don't change the values\nfigure; image(imgd)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"might produce the following images:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"img imgd\n(Image: checker) (Image: checker2)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"The one on the right looks white because floating-point types are interpreted on a 0-to-1 colorscale (and all of the entries in img happen to be 1 or higher), whereas uint8 is interpreted on a 0-to-255 colorscale. Unfortunately, two arrays that are numerically identical have very different meanings as images.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Many frameworks offer convenience functions for converting images from one representation to another, but this can be a source of bugs if we go to compare images: in most number systems we would agree that 255 != 1.0, and this fact means that you sometimes need to be quite careful when converting from one representation to another. Conversely, using these Julia packages there is no discrepancy in \"meaning\" between the encoding of images represented as floating point or 8-bit (or 16-bit) fixed-point numbers: 0 always means \"black\" and 1 always means \"white\" or \"saturated.\"","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Now, this doesn't prevent you from constructing pixels with values out of this range:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: saturated_spectrum)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Notice that the first two yellows look identical, because both the red and green color channels are 1 or higher and consequently are saturated.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"However, you should be aware that for integer inputs, the default is to use the N0f8 element type, and this type cannot represent values outside the range from 0 to 1:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"using ColorTypes # hide\nRGB(8,2,0)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"The error message here reminds you how to resolve a common mistake, trying to construct red as RGB(255, 0, 0). In Julia, that should always be RGB(1, 0, 0).","category":"page"},{"location":"tutorials/arrays_colors/#More-fixed-point-numbers","page":"Arrays, Numbers, and Colors","title":"More fixed-point numbers","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"16-bit images can be expressed in terms of the N0f16 type. Let's compare the maximum values (typemax) and smallest-difference (eps) representable with N0f8 and N0f16:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> using FixedPointNumbers\n\njulia> (typemax(N0f8), eps(N0f8))\n(1.0N0f8, 0.004N0f8)\n\njulia> (typemax(N0f16), eps(N0f16))\n(1.0N0f16, 2.0e-5N0f16)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"You can see that this type also has a maximum value of 1, but is higher precision, with the gap between adjacent numbers being much smaller.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Many cameras (particularly, scientific cameras) now return 16-bit values. However, some cameras do not provide a full 16 bits worth of information; for example, the camera might be 12-bit and return values between 0x0000 and 0x0fff.  As an N0f16, the latter displays as nearly black:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: 12bit_black)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Since the camera is saturated, this is quite misleading–-it should instead display as white.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"This again illustrates one of the fundamental problems about assuming that the representation (a 16-bit integer) also describes the meaning of the number. In Julia, we decouple these by providing many different fixed-point number types. In this case, the natural way to interpret these values is by using a fixed-point number with 12 fractional bits; this leaves 4 bits that we can use to represent values bigger than 1, so the number type is called N4f12:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> (typemax(N4f12), eps(N4f12))\n(16.0037N4f12, 0.0002N4f12)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"You can see that the maximum value achievable by an N4f12 is approximately 16 = 2^4.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Using this N4f12 interpretation of the 16 bits, the color displays correctly as white:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"(Image: 12bit_black)","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"and acts like 1 for all arithmetic purposes. Even though the raw representation as 0x0fff is the same, we can endow the number with appropriate meaning through its type.","category":"page"},{"location":"tutorials/arrays_colors/#A-note-on-arithmetic-overflow","page":"Arrays, Numbers, and Colors","title":"A note on arithmetic overflow","text":"","category":"section"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Sometimes, being able to construct a color values outside 0 to 1 is useful. For example, if you want to compute the average color in an image, the natural approach is to first sum all the pixels and then divide by the total number of pixels. At an intermediate stage, the sum will typically result in a color that is well beyond saturation.","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"It's important to note that arithmetic with N0f8 numbers, like arithmetic with UInt8, overflows:","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"julia> 0xff + 0xff\n0xfe\n\njulia> 1N0f8 + 1N0f8\n0.996N0f8\n\njulia> 0xfe/0xff      # the first result corresponds to the second result\n0.996078431372549","category":"page"},{"location":"tutorials/arrays_colors/","page":"Arrays, Numbers, and Colors","title":"Arrays, Numbers, and Colors","text":"Consequently, if you're accumulating values, it's advisable to accumulate them in an appropriate floating-point type, such as Float32, Gray{Float64}, or RGB{Float32}.","category":"page"},{"location":"pkgs/segmentation/#ImageSegmentation.jl","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"","category":"section"},{"location":"pkgs/segmentation/#Introduction","page":"ImageSegmentation.jl","title":"Introduction","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Image Segmentation is the process of partitioning the image into regions that have similar attributes. Image segmentation has various applications e.g, medical image segmentation, image compression and is used as a preprocessing step in higher level vision tasks like object detection and optical flow. This package is a collection of image segmentation algorithms written in Julia.","category":"page"},{"location":"pkgs/segmentation/#Installation","page":"ImageSegmentation.jl","title":"Installation","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(v1.0) pkg> add ImageSegmentation","category":"page"},{"location":"pkgs/segmentation/#Example","page":"ImageSegmentation.jl","title":"Example","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Image segmentation is not a mathematically well-defined problem: for example, the only lossless representation of the input image would be to say that each pixel is its own segment. Yet this does not correspond to our own intuitive notion that some pixels are naturally grouped together. As a consequence, many algorithms require parameters, often some kind of threshold expressing your willingness to tolerate a certain amount of variation among the pixels within a single segment.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Let's see an example on how to use the segmentation algorithms in this package. We will try to separate the horse, the ground and the sky in the image below. We will explore two algorithms - seeded region growing and felzenszwalb. Seeded region growing requires us to know the number of segments and some points on each segment beforehand whereas felzenszwalb uses a more abstract parameter controlling degree of within-segment similarity.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"source","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"The documentation for seeded_region_growing says that it needs two arguments - the image to be segmented and a set of seed points for each region. The seed points have to be stored as a vector of (position, label) tuples, where position is a CartesianIndex and label is an integer. We will start by opening the image using ImageView and reading the coordinates of the seed points.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"using Images, ImageView\n\nimg = load(\"src/pkgs/segmentation/assets/horse.jpg\")\nimshow(img)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Hover over the different objects you'd like to segment, and read out the coordinates of one or more points inside each object. We will store the seed points as a vector of (seed position, label) tuples and use seeded_region_growing with the recorded seed points.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"using ImageSegmentation\nseeds = [(CartesianIndex(126,81),1), (CartesianIndex(93,255),2), (CartesianIndex(213,97),3)]\nsegments = seeded_region_growing(img, seeds)\n\n# output\n\nSegmented Image with:\n  labels map: 240×360 Array{Int64,2}\n  number of labels: 3","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"All the segmentation algorithms (except Fuzzy C-means) return a struct SegmentedImage that stores the segmentation result. SegmentedImage contains a list of applied labels, an array containing the assigned label for each pixel, and mean color and number of pixels in each segment. This section explains how to access information about the segments.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"DocTestSetup = quote\n    using Images, ImageSegmentation\n    img = load(\"src/pkgs/segmentation/assets/horse.jpg\")\n    seeds = [(CartesianIndex(126,81),1), (CartesianIndex(93,255),2), (CartesianIndex(213,97),3)]\n    segments = seeded_region_growing(img, seeds)\nend","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> length(segment_labels(segments))\n3\n\njulia> segment_mean(segments)\nDict{Int64,RGB{Float64}} with 3 entries:\n  2 => RGB{Float64}(0.793598,0.839543,0.932374)\n  3 => RGB{Float64}(0.329863,0.35779,0.237457)\n  1 => RGB{Float64}(0.0646509,0.0587034,0.0743471)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"We can visualize each segment using its mean color:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> imshow(map(i->segment_mean(segments,i), labels_map(segments)));","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"This display form is used for many of the demonstrations below.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"You can see that the algorithm did a fairly good job of segmenting the three objects. The only obvious error is the fact that elements of the sky that were \"framed\" by the horse ended up being grouped with the ground. This is because seeded_region_growing always returns connected regions, and there is no path connecting those portions of sky to the larger image. If we add some additional seed points in those regions, and give them the same label 2 that we used for the rest of the sky, we will get a result that is more or less perfect.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"seeds = [(CartesianIndex(126,81), 1), (CartesianIndex(93,255), 2), (CartesianIndex(171,103), 2),\n         (CartesianIndex(172,142), 2), (CartesianIndex(182,72), 2), (CartesianIndex(213,97), 3)]\nsegments = seeded_region_growing(img, seeds)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"DocTestSetup = nothing","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Now let's segment this image using felzenszwalb algorithm. felzenswalb only needs a single parameter k which controls the size of segments. Larger k will result in bigger segments. Using k=5 to k=500 generally gives good results.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using Images, ImageSegmentation\n\njulia> img = load(\"src/pkgs/segmentation/assets/horse.jpg\");\n\njulia> segments = felzenszwalb(img, 100)\nSegmented Image with:\n  labels map: 240×360 Array{Int64,2}\n  number of labels: 43\n\njulia> segments = felzenszwalb(img, 10)  #smaller segments but noisy segmentation\nSegmented Image with:\n  labels map: 240×360 Array{Int64,2}\n  number of labels: 312","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"k = 100 k = 10\n(Image: Original) (Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"We only got two \"major\" segments with k = 100. Setting k = 10 resulted in smaller but rather noisy segments. felzenzwalb also takes an optional argument min_size - it removes all segments smaller than min_size pixels. (Most methods don't remove small segments in their core algorithm. We can use the prune_segments method to postprocess the segmentation result and remove small segments.)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"segments = felzenszwalb(img, 10, 100)  # removes segments with fewer than 100 pixels\nimshow(map(i->segment_mean(segments,i), labels_map(segments)))","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/#Result","page":"ImageSegmentation.jl","title":"Result","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"All segmentation algorithms (except Fuzzy C-Means) return a struct SegmentedImage as its output. SegmentedImage contains all the necessary information about the segments. The following functions can be used to get the information about the segments:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"labels_map : It returns an array containing the labels assigned to each pixel\nsegment_labels : It returns a list of all the assigned labels\nsegment_mean : It returns the mean intensity of the supplied label.\nsegment_pixel_count : It returns the count of the pixels that are assigned the supplied label.","category":"page"},{"location":"pkgs/segmentation/#Demo","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> img = fill(1, 4, 4);\n\njulia> img[1:2,1:2] .= 2;\n\njulia> img\n4×4 Array{Int64,2}:\n 2  2  1  1\n 2  2  1  1\n 1  1  1  1\n 1  1  1  1\n\njulia> seg = fast_scanning(img, 0.5);\n\njulia> labels_map(seg) # returns the assigned labels map\n4×4 Array{Int64,2}:\n 1  1  3  3\n 1  1  3  3\n 3  3  3  3\n 3  3  3  3\n\njulia> segment_labels(seg) # returns a list of all assigned labels\n2-element Array{Int64,1}:\n 1\n 3\n\njulia> segment_mean(seg, 1) # returns the mean intensity of label 1\n2.0\n\njulia> segment_pixel_count(seg, 1) # returns the pixel count of label 1\n4","category":"page"},{"location":"pkgs/segmentation/#Algorithms","page":"ImageSegmentation.jl","title":"Algorithms","text":"","category":"section"},{"location":"pkgs/segmentation/#Seeded-Region-Growing","page":"ImageSegmentation.jl","title":"Seeded Region Growing","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Seeded region growing segments an image with respect to some user-defined seeds. Each seed is a (position, label) tuple, where position is a CartesianIndex and label is a positive integer. Each label corresponds to a unique partition of the image. The algorithm tries to assign these labels to each of the remaining points. If more than one point has the same label then they will be contribute to the same segment.","category":"page"},{"location":"pkgs/segmentation/#Demo-2","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using Images, ImageSegmentation\n\njulia> img = load(\"src/pkgs/segmentation/assets/worm.jpg\");\n\njulia> seeds = [(CartesianIndex(104, 48), 1), (CartesianIndex( 49, 40), 1),\n                (CartesianIndex( 72,131), 1), (CartesianIndex(109,217), 1),\n                (CartesianIndex( 28, 87), 2), (CartesianIndex( 64,201), 2),\n                (CartesianIndex(104, 72), 2), (CartesianIndex( 86,138), 2)];\n\njulia> seg = seeded_region_growing(img, seeds)\nSegmented Image with:\n  labels map: 183×275 Array{Int64,2}\n  number of labels: 2","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Original (source):","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Segmented Image with labels replaced by their intensity means:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: SegmentedImage)","category":"page"},{"location":"pkgs/segmentation/#Unseeded-Region-Growing","page":"ImageSegmentation.jl","title":"Unseeded Region Growing","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"This algorithm is similar to Seeded Region Growing but does not require any prior information about the seed points. The segmentation process initializes with region A_1 containing a single pixel of the image. Let an intermediate state of the algorithm consist of a set of identified regions A_1 A_2  A_n. Let T be the set of all unallocated pixels which borders at least one of these regions. The growing process involves selecting a point z in T and region A_j where j in   1n   such that","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"delta (  z A_j )  = min_x in T k in   1n     delta (  x A_k )  ","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"where delta (  x A_i )  =  img (  x )  - mean_y in A_i   img (  y )    ","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"If delta (  z A_j )  is less than threshold then the pixel z is added to A_j. Otherwise we choose the most similar region alpha such that","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"alpha = argmin_A_k  delta (  z A_k)  ","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"If delta (  z alpha )  is less than threshold then the pixel z is added to alpha. If neither of the two conditions is satisfied, then the pixel is assigned a new region A_n+1. After assignment of z, we update the statistic of the assigned region. The algorithm halts when all the pixels have been assigned to some region.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"unseeded_region_growing requires the image img and threshold as its parameters.","category":"page"},{"location":"pkgs/segmentation/#Demo-3","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using ImageSegmentation, Images\n\njulia> img = load(\"src/pkgs/segmentation/assets/tree.jpg\");\n\njulia> seg = unseeded_region_growing(img, 0.05) # here 0.05 is the threshold\nSegmented Image with:\n  labels map: 320×480 Array{Int64,2}\n  number of labels: 698","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Threshold Output Compression percentage\nOriginal (source) (Image: tree) 0 %\n0.05 (Image: tree_seg1) 60.63%\n0.1 (Image: tree_seg2) 71.27%\n0.2 (Image: tree_seg3) 79.96%","category":"page"},{"location":"pkgs/segmentation/#Felzenswalb's-Region-Merging-Algorithm","page":"ImageSegmentation.jl","title":"Felzenswalb's Region Merging Algorithm","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"This algorithm operates on a Region Adjacency Graph (RAG). Each pixel/region is a node in the graph and adjacent pixels/regions have edges between them with weight measuring the dissimilarity between pixels/regions. The algorithm repeatedly merges similar regions till we get the final segmentation. It efficiently computes oversegmented “superpixels” in an image. The function can be directly called with an image (the implementation internally creates a RAG of the image first and then proceeds).","category":"page"},{"location":"pkgs/segmentation/#Demo-4","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using Images, ImageSegmentation, TestImages\n\njulia> img = Gray.(testimage(\"house\"));\n\njulia> segments = felzenszwalb(img, 300, 100) # k=300 (the merging threshold), min_size = 100 (smallest number of pixels/region)\nSegmented Image with:\n  labels map: 512×512 Array{Int64,2}\n  number of labels: 11","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Here let's visualize segmentation by creating an image with each label replaced by a random color:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"function get_random_color(seed)\n    Random.seed!(seed)\n    rand(RGB{N0f8})\nend\nimshow(map(i->get_random_color(i), labels_map(segments)))","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: img1) (Image: img2)","category":"page"},{"location":"pkgs/segmentation/#MeanShift-Segmentation","page":"ImageSegmentation.jl","title":"MeanShift Segmentation","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"MeanShift is a clustering technique. Its primary advantages are that it doesn't assume a prior on the shape of the cluster (e.g, gaussian for k-means) and we don't need to know the number of clusters beforehand. The algorithm doesn't scale well with size of image.","category":"page"},{"location":"pkgs/segmentation/#Demo-5","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using Images, ImageSegmentation, TestImages\n\njulia> img = Gray.(testimage(\"house\"));\n\njulia> img = imresize(img, (128, 128));\n\njulia> segments = meanshift(img, 16, 8/255) # parameters are smoothing radii: spatial=16, intensity-wise=8/255\nSegmented Image with:\n  labels map: 128×128 Array{Int64,2}\n  number of labels: 44","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: img1) (Image: img2)","category":"page"},{"location":"pkgs/segmentation/#Fast-Scanning","page":"ImageSegmentation.jl","title":"Fast Scanning","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Fast scanning algorithm segments the image by scanning it once and comparing each pixel to its upper and left neighbor. The algorithm starts from the first pixel and assigns it to a new segment A_1. Label count lc is assigned 1. Then it starts a column-wise traversal of the image and for every pixel, it computes the difference measure diff_fn between the pixel and its left neighbor, say delta_l and between the pixel and its top neighbor, say delta_t. Four cases arise:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"delta_l >= threshold and delta_t < threshold : We can say that the point has similar intensity to that its top neighbor. Hence, we assign the point to the segment that contains its top neighbor.\ndelta_l < threshold and delta_t >= threshold : Similar to case 1, we assign the point to the segment that contains its left neighbor.\ndelta_l >= threshold and delta_t >= threshold : Point is significantly different from its top and left neighbors and is assigned a new label A_lc+1 and lc is incremented.\ndelta_l < threshold and delta_t < threshold : In this case, we merge the top and left semgents together and assign the point under consideration to this merged segment.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"This algorithm segments the image in just two passes (one for segmenting and other for merging), hence it is very fast and can be used in real time applications.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Time Complexity: O(n) where n is the number of pixels","category":"page"},{"location":"pkgs/segmentation/#Demo-6","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using ImageSegmentation, TestImages\n\njulia> img = testimage(\"camera\");\n\njulia> seg = fast_scanning(img, 0.1)  # threshold = 0.1\nSegmented Image with:\n  labels map: 512×512 Array{Int64,2}\n  number of labels: 2538\n\njulia> seg = prune_segments(seg, i->(segment_pixel_count(seg,i)<50), (i,j)->(-segment_pixel_count(seg,j)))\nSegmented Image with:\n  labels map: 512×512 Array{Int64,2}\n  number of labels: 65","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Original:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Segmented Image:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: SegmentedImage)","category":"page"},{"location":"pkgs/segmentation/#Region-Splitting-using-RegionTrees","page":"ImageSegmentation.jl","title":"Region Splitting using RegionTrees","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"This algorithm follows the divide and conquer methodology. If the input image is homogeneous then nothing is to be done. In the other case, the image is split into two across every dimension and the smaller parts are segmented recursively. This procedure generates a region tree which can be used to create a segmented image.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Time Complexity: O(n*log(n)) where n is the number of pixels","category":"page"},{"location":"pkgs/segmentation/#Demo-7","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using TestImages, ImageSegmentation\n\njulia> img = testimage(\"lena_gray\");\n\njulia> function homogeneous(img)\n           min, max = extrema(img)\n           max - min < 0.2\n       end\nhomogeneous (generic function with 1 method)\n\njulia> seg = region_splitting(img, homogeneous)\nSegmented Image with:\n  labels map: 256×256 Array{Int64,2}\n  number of labels: 8836","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Original:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Segmented Image with labels replaced by their intensity means:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: SegmentedImage)","category":"page"},{"location":"pkgs/segmentation/#Fuzzy-C-means","page":"ImageSegmentation.jl","title":"Fuzzy C-means","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Fuzzy C-means clustering is widely used for unsupervised image segmentation. It is an iterative algorithm which tries to minimize the cost function:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"J = displaystylesum_i=1^N sum_j=1^C delta_ij  x_i - c_j ^2","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Unlike K-means, it allows pixels to belong to two or more clusters. It is widely used for medical imaging like in the soft segmentation of brain tissue model. Note that both Fuzzy C-means and K-means have an element of randomness, and it's possible to get results that vary considerably from one run to the next.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Time Complexity: O(n*C^2*iter) where n is the number of pixels, C is number of clusters and iter is the number of iterations.","category":"page"},{"location":"pkgs/segmentation/#Demo-8","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using ImageSegmentation, Images\n\njulia> img = load(\"src/pkgs/segmentation/assets/flower.jpg\");\n\njulia> r = fuzzy_cmeans(img, 3, 2)\nFuzzyCMeansResult: 3 clusters for 135360 points in 3 dimensions (converged in 27 iterations)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Briefly, r contains two fields of interest:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"centers, a 3×C matrix of center positions for C clusters in RGB colorspace. You can extract it as a vector of colors using centers = colorview(RGB, r.centers).\nweights, a n×C matrix such that r.weights[10,2] would be the weight of the 10th pixel in the green color channel (color channel 2).  You can visualize this component as centers[i]*reshape(r.weights[:,i], axes(img)).","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"See the documentation in Clustering.jl for further details.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Original (source)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"(Image: Original)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Output with pixel intensity = cluster center intensity * membership of pixel in that class","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Magenta petals Greenish Leaves White background\n(Image: SegmentedImage1) (Image: SegmentedImage2) (Image: SegmentedImage3)","category":"page"},{"location":"pkgs/segmentation/#Watershed","page":"ImageSegmentation.jl","title":"Watershed","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"The watershed algorithm treats an image as a topographic surface where bright pixels correspond to peaks and dark pixels correspond to valleys. The algorithm starts flooding from valleys (local minima) of this topographic surface and region boundaries are formed when water from different sources merge. If the image is noisy, this approach leads to oversegmetation. To prevent oversegmentation, marker-based watershed is used i.e. the topographic surface is flooded from a predefined set of markers.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Let's see an example on how to use watershed to segment touching objects. To use watershed, we need to modify the image such that in the new image flooding the topographic surface from the markers separates each coin. If this modified image is noisy, flooding from local minima may lead to oversegmentation and so we also need a way to find the marker positions. In this example, the inverted distance_transform of the thresholded image (dist image) has the required topographic structure (This page explains why this works). We can threshold the dist image to get the marker positions.","category":"page"},{"location":"pkgs/segmentation/#Demo-9","page":"ImageSegmentation.jl","title":"Demo","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using Images, ImageSegmentation\n\njulia> img = load(download(\"http://docs.opencv.org/3.1.0/water_coins.jpg\"));\n\njulia> bw = Gray.(img) .> 0.5;\n\njulia> dist = 1 .- distance_transform(feature_transform(bw));\n\njulia> markers = label_components(dist .< -15);\n\njulia> segments = watershed(dist, markers)\nSegmented Image with:\n  labels map: 312×252 Array{Int64,2}\n  number of labels: 24\n\njulia> imshow(map(i->get_random_color(i), labels_map(segments)) .* (1 .-bw))       #shows segmented image","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Original Image Thresholded Image\n(Image: img1) (Image: img1)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Inverted Distance Transform Image Markers\n(Image: img1) (Image: img1)","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Segmented Image\n(Image: img2)","category":"page"},{"location":"pkgs/segmentation/#Some-helpful-functions","page":"ImageSegmentation.jl","title":"Some helpful functions","text":"","category":"section"},{"location":"pkgs/segmentation/#Creating-a-Region-Adjacency-Graph-(RAG)","page":"ImageSegmentation.jl","title":"Creating a Region Adjacency Graph (RAG)","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"A region adjacency graph can directly be constructed from a SegmentedImage using the region_adjacency_graph function. Each segment is denoted by a vertex and edges are constructed between adjacent segments. The output is a tuple of SimpleWeightedGraph and a Dict(label=>vertex) with weights assigned according to weight_fn.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using ImageSegmentation, Distances, TestImages\n\njulia> img = testimage(\"camera\");\n\njulia> seg = felzenszwalb(img, 10, 100);\n\njulia> weight_fn(i,j) = euclidean(segment_pixel_count(seg,i), segment_pixel_count(seg,j));\n\njulia> G, vert_map = region_adjacency_graph(seg, weight_fn);\n\njulia> G\n{70, 139} undirected simple Int64 graph with Float64 weights","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"Here, the difference in pixel count has been used as the weight of the connecting edges. This difference measure can be useful if one wants to use this region adjacency graph to remove smaller segments by merging them with their neighbouring largest segment. Another useful difference measure is the euclidean distance between the mean intensities of the two segments.","category":"page"},{"location":"pkgs/segmentation/#Creating-a-Region-Tree","page":"ImageSegmentation.jl","title":"Creating a Region Tree","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"A region tree can be constructed from an image using region_tree function. If the image is not homogeneous, then it is split into half along each dimension and the function is called recursively for each portion of the image. The output is a RegionTree.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> using ImageSegmentation\n\njulia> function homogeneous(img)\n           min, max = extrema(img)\n           max - min < 0.2\n       end\nhomogeneous (generic function with 1 method)\n\njulia> t = region_tree(img, homogeneous)        # `img` is an image\nCell: RegionTrees.HyperRectangle{2,Float64}([1.0, 1.0], [300.0, 300.0])","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"For more information regarding RegionTrees, see this.","category":"page"},{"location":"pkgs/segmentation/#Pruning-unnecessary-segments","page":"ImageSegmentation.jl","title":"Pruning unnecessary segments","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"All the unnecessary segments can be easily removed from a SegmentedImage using prune_segments. It removes a segment by replacing it with the neighbor which has the least value of diff_fn. A list of the segments to be removed can be supplied. Alternately, a function can be supplied that returns true for the labels that must be removed.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"note: Note\nThe resultant SegmentedImage might have the different labels compared to the original SegmentedImage.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"For this example and the next one (in Removing a segment), a sample SegmentedImage has been used. It can be generated as:","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> img = fill(1, (4, 4));\n\njulia> img[3:4,:] .= 2;\n\njulia> img[1:2,3:4] .= 3;\n\njulia> seg = fast_scanning(img, 0.5);\n\njulia> labels_map(seg)\n4×4 Array{Int64,2}:\n 1  1  3  3\n 1  1  3  3\n 2  2  2  2\n 2  2  2  2\n\njulia> seg.image_indexmap\n4×4 Array{Int64,2}:\n 1  1  3  3\n 1  1  3  3\n 2  2  2  2\n 2  2  2  2\n\njulia> diff_fn(rem_label, neigh_label) = segment_pixel_count(seg,rem_label) - segment_pixel_count(seg,neigh_label);\n\njulia> new_seg = prune_segments(seg, [3], diff_fn);\n\njulia> labels_map(new_seg)\n4×4 Array{Int64,2}:\n 1  1  2  2\n 1  1  2  2\n 2  2  2  2\n 2  2  2  2","category":"page"},{"location":"pkgs/segmentation/#Removing-a-segment","page":"ImageSegmentation.jl","title":"Removing a segment","text":"","category":"section"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"If only one segment is to be removed, then rem_segment! can be used. It removes a segment from a SegmentedImage in place, replacing it with the neighbouring segment having least diff_fn value.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"note: Note\nIf multiple segments need to be removed then prune_segments should be preferred as it is much more time efficient than calling rem_segment! multiple times.","category":"page"},{"location":"pkgs/segmentation/","page":"ImageSegmentation.jl","title":"ImageSegmentation.jl","text":"julia> seg.image_indexmap\n4×4 Array{Int64,2}:\n 1  1  3  3\n 1  1  3  3\n 2  2  2  2\n 2  2  2  2\n\njulia> diff_fn(rem_label, neigh_label) = segment_pixel_count(seg,rem_label) - segment_pixel_count(seg,neigh_label);\n\njulia> rem_segment!(seg, 3, diff_fn);\n\njulia> labels_map(new_seg)\n4×4 Array{Int64,2}:\n 1  1  2  2\n 1  1  2  2\n 2  2  2  2\n 2  2  2  2","category":"page"},{"location":"pkgs/transformations/#ImageTransformations.jl","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"","category":"section"},{"location":"pkgs/transformations/#Introduction","page":"ImageTransformations.jl","title":"Introduction","text":"","category":"section"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"Image Transformation is the process of changing the coordinate system of an image by resizing, rotating, etc. These operations have various applications, e.g, medical image registration.","category":"page"},{"location":"pkgs/transformations/#Installation","page":"ImageTransformations.jl","title":"Installation","text":"","category":"section"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"(v1.0) pkg> add ImageTransformations","category":"page"},{"location":"pkgs/transformations/#Key-functions","page":"ImageTransformations.jl","title":"Key functions","text":"","category":"section"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"The exported functions in this package include","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"restrict for 2-fold down sampling\nimresize for arbitrary resizing\nimrotate for image rotation\nwarp for general image warping, and related functions\nWarpedView\nwarpedview\nInvWarpedView\ninvwarpedview","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"These functions all have docstrings that give more details about their usage.","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"There are in-place version of many of the functions, e.g., imresize! etc.","category":"page"},{"location":"pkgs/transformations/#Examples","page":"ImageTransformations.jl","title":"Examples","text":"","category":"section"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"Resize","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"using ImageTransformations, TestImages\nimg = testimage(\"mandrill\")\nimg_small = imresize(img, ratio=1/8)\nimg_medium = imresize(img_small, size(img_small).*2)","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"Resulting images (small and medium): (Image: img_small) (Image: img_medium)","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"Warping","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"using ImageTransformations, TestImages, CoordinateTransformations, Rotations\n\nimg = testimage(\"camera\");\n\n# define transformation\ntrfm = recenter(RotMatrix(pi/8), center(img));\nimgw = warp(img, trfm);","category":"page"},{"location":"pkgs/transformations/","page":"ImageTransformations.jl","title":"ImageTransformations.jl","text":"Resulting image:   (Image: img_warp)","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/#RGB-to-GrayScale","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"","category":"section"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"This example illustrates RGB to Grayscale Conversion","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"using ImageCore, TestImages\n\nrgb_image = testimage(\"lighthouse\")","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"I = Gray.(rgb_image) converts an RGB image to Grayscale.","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"gray_image = Gray.(rgb_image)\nmosaicview(rgb_image, gray_image; nrow = 1)","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"Gray scale conversion form RGB follows a weighted sum of all channels, the coffecients are computed according to Rec. ITU-R BT.601-7 rounding off to 3 decimal places 0.299 * R + 0.587 * G + 0.114 * B","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"","category":"page"},{"location":"democards/examples/color_channels/rgb_grayscale/","page":"RGB to GrayScale","title":"RGB to GrayScale","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"pkgs/features/#ImageFeatures.jl","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"","category":"section"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"The ImageFeatures package allows you to compute compact \"descriptors\" of images or image regions.  These descriptors are in a form that permits comparison against similar descriptors in other images or other portions of the same image. This can be useful in many applications, such as object recognition, localization, or image registration.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"ImagesFeatures has its own documentation, and you should consult that for a comprehensive overview of the functionality of the package. Here, we'll briefly illustrate one type of feature and its application to image registration, the BRISK descriptor.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"The BRISK descriptor examines the structure of an image around a keypoint. Given a keypoint, the mean intensity (loosely-speaking) is computed in a set of patches surrounding the point:","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"(Image: BRISK Sampling Pattern)","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"BRISK then re-represents these intensities in a way that is invariant under rotations. This allows you to compare descriptors in two images, one of which might be a rotated version of the other.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"Let us take a look at a simple example where the BRISK descriptor is used to match two images where one has been translated by (50, 40) pixels and then rotated by an angle of 75 degrees. We will use the lighthouse image from the TestImages package for this example.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"First, let us create the two images we will match using BRISK.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"using ImageFeatures, TestImages, Images, ImageDraw, CoordinateTransformations, Rotations\n\nimg = testimage(\"lighthouse\")\nimg1 = Gray.(img)\nrot = recenter(RotMatrix(5pi/6), [size(img1)...] .÷ 2)  # a rotation around the center\ntform = rot ∘ Translation(-50, -40)\nimg2 = warp(img1, tform, axes(img1))\nnothing # hide","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"To calculate the descriptors, we first need to get the keypoints. For this tutorial, we will use the FAST corners to generate keypoints (see fastcorners).","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"features_1 = Features(fastcorners(img1, 12, 0.35))\nfeatures_2 = Features(fastcorners(img2, 12, 0.35))\nnothing # hide","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"To create the BRISK descriptor, we first need to define the parameters by calling the BRISK constructor.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"brisk_params = BRISK()\nnothing # hide","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"Now pass the image with the keypoints and the parameters to the create_descriptor function.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"desc_1, ret_features_1 = create_descriptor(img1, features_1, brisk_params)\ndesc_2, ret_features_2 = create_descriptor(img2, features_2, brisk_params)\nnothing # hide","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"The obtained descriptors can be used to find the matches between the two images using the match_keypoints function.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"matches = match_keypoints(Keypoints(ret_features_1), Keypoints(ret_features_2), desc_1, desc_2, 0.1)\nnothing # hide","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"We can use the ImageDraw.jl package to view the results.","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"\ngrid = hcat(img1, img2)\noffset = CartesianIndex(0, size(img1, 2))\nmap(m -> draw!(grid, LineSegment(m[1], m[2] + offset)), matches)\nsave(\"assets/brisk_example.jpg\", grid); nothing # hide\n","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"(Image: )","category":"page"},{"location":"pkgs/features/","page":"ImageFeatures.jl","title":"ImageFeatures.jl","text":"You can see that the points have been accurately matched despite the large magnitude of this rotation.","category":"page"},{"location":"democards/examples/#demonstrations","page":"Demos","title":"Demonstrations","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"","category":"page"},{"location":"democards/examples/#Color-channels","page":"Demos","title":"Color channels","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card-section\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demonstration shows how to work with color channels to explore image compression using the Singular Value Decomposition (SVD).","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Image Compression using SVD","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This example illustrates RGB to Grayscale Conversion","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"RGB to GrayScale","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This example illustrates how RGB to HSV (Hue, Saturation, Value) conversion can be used to facilitate segmentation processes.","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"RGB to HSV and thresholding","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/#Contours","page":"Demos","title":"Contours","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card-section\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demonstration shows how to detect contours on binary images The algorithm used is &quot;Topological Structural Analysis of Digitized Binary Images by Border Following&quot; by Suzuki and Abe (Same as OpenCV).#","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Contour Detection and Drawing","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Corner Detection, which is a subset of Interest Point Detection, tries to detect points in the image which have a well-defined position and can be robustly detected in multiple images of the same scene. Very often, these points lie along the corners or edges of objects in the image - hence the name.","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Detecting Corners","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/#Image-quality-and-benchmarks","page":"Demos","title":"Image quality and benchmarks","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card-section\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demo shows how SSIM and MSE are used to evaluate the image quality","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Structural Similarity Index","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/#Spatial-transformation","page":"Demos","title":"Spatial transformation","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card-section\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demonstration shows how alpha compositing can be done in 10 lines of code using OffsetArrays and PaddedViews","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Alpha Compositing","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demo issustrates the use of Histogram equalization, gamma correction matching and Contrast Limited Adaptive Histogram Equalization","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Histogram equalisation","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"<div class=\"card\">\n<div class=\"card-cover\">\n<div class=\"card-description\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"This demonstration shows some common tricks in image comparision – difference view","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"(Image: card-cover-image)","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n<div class=\"card-text\">","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Image Difference View","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>\n</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"</div>","category":"page"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"","category":"page"},{"location":"democards/examples/#Contributions","page":"Demos","title":"Contributions","text":"","category":"section"},{"location":"democards/examples/","page":"Demos","title":"Demos","text":"Users are invited to contribute demonstrations of their own.","category":"page"},{"location":"api_comparison/#page_api_comparison","page":"Comparison with other image processing frameworks","title":"Comparison with other image processing frameworks","text":"","category":"section"},{"location":"api_comparison/","page":"Comparison with other image processing frameworks","title":"Comparison with other image processing frameworks","text":"The following table may be useful for people migrating from other frameworks, and for identifying missing functionality in JuliaImages. Note that there are relevant packages which have not been integrated into more general frameworks or hosted at JuliaImages (e.g., DICOM.jl, etc.); such functionality is not documented here. This table is certainly not complete, and additions/corrections are welcome.","category":"page"},{"location":"api_comparison/","page":"Comparison with other image processing frameworks","title":"Comparison with other image processing frameworks","text":"Operation JuliaImages scikit-image + NumPy Matlab (ImageProcessing + ComputerVision)\nInput/output   \nRead image file load imread imread\nWrite image file save imsave imwrite\nImage file metadata magickinfo (ImageMagick.jl)  imfinfo\nTest images testimage astronaut etc. “cameraman.tif” etc\n   \nElement type and color   \nChange numeric precision float32, float64, n0f8, etc. img_as_float etc im2double etc\nChange color space HSV.(img) etc. rgb2hsv etc. rgb2lab etc.\nWhitepoint adjustment map whitebalance (Colors.jl)  makecform\nHigh dynamic range   tonemap\n   \nIntensity & quantization   \nClamping clamp01, clamp01nan  \nLinear scaling LinearStretching, scaleminmax, etc. rescale_intensity imadjust\nNonlinear scaling GammaCorrection adjust_gamma imadjust\nCompute histogram build_histogram histogram imhist\nHistogram equalization Equalization equalize_hist histeq\nAdaptive equalization AdaptiveEqualization equalize_adapthist adapthisteq\nReference histogram matching Matching match_histograms imhistmatch\nQuantization map anonymous function  imquantize\nThreshold estimation otsu_threshold threshold_otsu etc. graythresh etc.\n   \nVisualization and interactivity   \nVisualization imshow (ImageView.jl), mosaicview imshow imshow, implay, montage, etc.\nContrast adjustment ImageView.jl  imcontrast\nPixel information ImageView.jl  impixelinfo\nDistance measurement   imdistline\nText display of region   impixelregion\nZooming/scrolling ImageView.jl and GtkUtilities.jl imshow imscrollpanel etc.\nInteractive colormap   imcolormaptool\nRegion selection  RecatangleTool etc. imrect, imellipse, imfreehand, etc.\nImage comparison colorview, mosaicview  imshowpair, imfuse\nLabel colorization IndirectArray, ColorizedArray label2rgb label2rgb\n   \nAnnotation   \nDraw lines line, line! (ImageDraw.jl) line, polygon line (visualization only)\nDraw circles/ellipses circle!, ellipse! (ImageDraw.jl) circle, ellipse viscircles (visualization only)\n   \nTransformations   \nResize imresize, restrict resize imresize\nImage pyramids gaussian_pyramid, restrict pyramid_gaussian etc. impyramid\nRotate imrotate rotate imrotate\nTranslate warp  imtranslate\nGeneral geometric transformation warp warp imwarp\nHough transform hough_transform_standard, hough_circle_gradient hough_circle, etc. hough\nRadon transform  radon, iradon radon, iradon\nDistance transform feature_transform, distance_transform  bwdist, graydist\n   \nRegistration   \n   \nStatistics and image comparison   \nImage differences ssd, sad, mse, rmse etc. compare_mse, compare_nrmse immse\nMin/max/mean minfinite, maxfinite, meanfinite minimum, maximum, mean nanmax, etc.\nEntropy entropy entropy entropy\nquality assessment PSNR, SSIM compare_psnr, compare_ssim psnr, ssim\ncolorfulness colorfulness  \n   \nFiltering and padding   \nLinear filtering imfilter gaussian, etc. imfilter\nMedian/max/quantile filtering mapwindow median/max etc. nlfilter, medfilt2, etc.\nOther nonlinear filtering (e.g., std) mapwindow  nlfilter, stdfilt\nGradients imgradients sobel_h etc. imgradientxy etc.\nIntegral image integral_image integral_image integralImage\nPadding padarray pad padarray\nDeconvolution wiener (Deconvolution.jl) richardson_lucy, weiner, etc. deconvlucy, deconvwnr, etc.\n   \nFeatures   \nEdge detection imedge, canny canny edge\nCorner detection imcorner, fastcorners corner_harris etc. detectFASTFeatures\nBlob detection blob_LoG blob_log etc. \nLocal binary patterns lbp etc. (ImageFeatures.jl) local_binary_pattern extractLBPFeatures\nHistogram of oriented gradients HOG (ImageFeatures.jl) hog extractHOGFeatures\nGray-level co-occurence glcm etc. (ImageFeatures.jl) greycomatrix graycomatrix\nPoint descriptors BRIEF, ORB, etc. (ImageFeatures.jl) BRIEF, ORB, etc. detectBRISK etc.\nFeature matching match_keypoints (ImageFeatures.jl) match_descriptors matchFeatures\n   \nSegmentation   \nConnected components label_components label bwconncomp, bwlabel\nForeground/background  active_contour activecontour\nClustering kmeans, fuzzy_cmeans, mean_shift (Clustering.jl) quickshift, slic \nMarker segmentation seeded_region_growing random_walker imsegfmm\nWatershed watershed watershed watershed\n   \nMorphological operations   \nDilation dilate dilation, binary_dilation imdilate\nErosion erode  \nOpening opening opening imopen\nClosing closing closing imclose\nTop-hat filtering tophat tophat etc. imtophat\nBottom-hat filtering bothat bottomhat imbothat\nRegional max/min mapwindow filters.rank.maximum etc. imregionalmax etc.\nConvex hull convexhull convex_hull_image bwconvhull\nBorders clearborder clear_border imclearborder\nBoundaries  find_boundaries boundarymask\nFilling imfill remove_small_holes imfill, regionfill","category":"page"},{"location":"democards/examples/contours/contour_detection/#Contour-Detection-and-Drawing","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"","category":"section"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"This demonstration shows how to detect contours on binary images The algorithm used is \"Topological Structural Analysis of Digitized Binary Images by Border Following\" by Suzuki and Abe (Same as OpenCV).#","category":"page"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"Points are represented using CartesianIndex. Contours are represented as a vector of Points. Direction is a single number between 1 to 8. Steps inside functions are marked as they are in the original paper","category":"page"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"using Images, TestImages, FileIO\n\n#              N          NE      E       SE      S       SW        W      NW\n# direction between two pixels\n\n# rotate direction clocwise\nfunction clockwise(dir)\n    return (dir)%8 + 1\nend\n\n# rotate direction counterclocwise\nfunction counterclockwise(dir)\n    return (dir+6)%8 + 1\nend\n\n# move from current pixel to next in given direction\nfunction move(pixel, image, dir, dir_delta)\n    newp = pixel + dir_delta[dir]\n    height, width = size(image)\n    if (0 < newp[1] <= height) &&  (0 < newp[2] <= width)\n        if image[newp]!=0\n            return newp\n        end\n    end\n    return CartesianIndex(0, 0)\nend\n\n# finds direction between two given pixels\nfunction from_to(from, to, dir_delta)\n    delta = to-from\n    return findall(x->x == delta, dir_delta)[1]\nend\n\n\n\nfunction detect_move(image, p0, p2, nbd, border, done, dir_delta)\n    dir = from_to(p0, p2, dir_delta)\n    moved = clockwise(dir)\n    p1 = CartesianIndex(0, 0)\n    while moved != dir ## 3.1\n        newp = move(p0, image, moved, dir_delta)\n        if newp[1]!=0\n            p1 = newp\n            break\n        end\n        moved = clockwise(moved)\n    end\n\n    if p1 == CartesianIndex(0, 0)\n        return\n    end\n\n    p2 = p1 ## 3.2\n    p3 = p0 ## 3.2\n    done .= false\n    while true\n        dir = from_to(p3, p2, dir_delta)\n        moved = counterclockwise(dir)\n        p4 = CartesianIndex(0, 0)\n        done .= false\n        while true ## 3.3\n            p4 = move(p3, image, moved, dir_delta)\n            if p4[1] != 0\n                break\n            end\n            done[moved] = true\n            moved = counterclockwise(moved)\n        end\n        push!(border, p3) ## 3.4\n        if p3[1] == size(image, 1) || done[3]\n            image[p3] = -nbd\n        elseif image[p3] == 1\n            image[p3] = nbd\n        end\n\n        if (p4 == p0 && p3 == p1) ## 3.5\n            break\n        end\n        p2 = p3\n        p3 = p4\n    end\nend\n\n\nfunction find_contours(image)\n    nbd = 1\n    lnbd = 1\n    image = Float64.(image)\n    contour_list =  Vector{typeof(CartesianIndex[])}()\n    done = [false, false, false, false, false, false, false, false]\n\n    # Clockwise Moore neighborhood.\n    dir_delta = [CartesianIndex(-1, 0) , CartesianIndex(-1, 1), CartesianIndex(0, 1), CartesianIndex(1, 1), CartesianIndex(1, 0), CartesianIndex(1, -1), CartesianIndex(0, -1), CartesianIndex(-1,-1)]\n\n    height, width = size(image)\n\n    for i=1:height\n        lnbd = 1\n        for j=1:width\n            fji = image[i, j]\n            is_outer = (image[i, j] == 1 && (j == 1 || image[i, j-1] == 0)) ## 1 (a)\n            is_hole = (image[i, j] >= 1 && (j == width || image[i, j+1] == 0))\n\n            if is_outer || is_hole\n                # 2\n                border = CartesianIndex[]\n\n                from = CartesianIndex(i, j)\n\n                if is_outer\n                    nbd += 1\n                    from -= CartesianIndex(0, 1)\n\n                else\n                    nbd += 1\n                    if fji > 1\n                        lnbd = fji\n                    end\n                    from += CartesianIndex(0, 1)\n                end\n\n                p0 = CartesianIndex(i,j)\n                detect_move(image, p0, from, nbd, border, done, dir_delta) ## 3\n                if isempty(border) ##TODO\n                    push!(border, p0)\n                    image[p0] = -nbd\n                end\n                push!(contour_list, border)\n            end\n            if fji != 0 && fji != 1\n                lnbd = abs(fji)\n            end\n\n        end\n    end\n\n    return contour_list\n\n\nend\n\n# a contour is a vector of 2 int arrays\nfunction draw_contour(image, color, contour)\n    for ind in contour\n        image[ind] = color\n    end\nend\nfunction draw_contours(image, color, contours)\n    for cnt in contours\n        draw_contour(image, color, cnt)\n    end\nend\n\n# load images\nimg1 = testimage(\"mandrill\")\nimg2 = testimage(\"lighthouse\")\n\n# convert to grayscale\nimgg1 = Gray.(img1)\nimgg2 = Gray.(img2)\n\n# threshold\nimgg1 = imgg1 .> 0.45\nimgg2 = imgg2 .> 0.45\n\n# calling find_contours\ncnts1 = find_contours(imgg1)\ncnts2 = find_contours(imgg2)\n\nimg3 = copy(img1)\nimg4 = copy(img2)\n\n# finally, draw the detected contours\ndraw_contours(img3, RGB(1,0,0), cnts1)\ndraw_contours(img4, RGB(1,0,0), cnts2)\n\n\nvcat([img1 img2], [img3 img4])","category":"page"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"","category":"page"},{"location":"democards/examples/contours/contour_detection/","page":"Contour Detection and Drawing","title":"Contour Detection and Drawing","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"democards/examples/contours/detecting_corners/#Detecting-Corners","page":"Detecting Corners","title":"Detecting Corners","text":"","category":"section"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"Corner Detection, which is a subset of Interest Point Detection, tries to detect points in the image which have a well-defined position and can be robustly detected in multiple images of the same scene. Very often, these points lie along the corners or edges of objects in the image - hence the name.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"Corner detection is useful in several computer vision tasks - such as Image Registration, Motion Detection and Panaroma Stitching. This is because if the locations of the same points are known in two different images, it gives a reference to align those images. Corners, with their well-defined positions serve as good candidates for such points.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"using Images, TestImages\nimg = Gray.(testimage(\"house\"))","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"We use this image of a house, with numerous edges and corners.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"The imcorner function can be used to detect corners in the image - and it returns an array of booleans, where a true value denotes that the corresponding pixel may be a corner. We use this to mark those pixels in red on a copy of the image.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"corners = imcorner(img)\nimg_copy = RGB.(img)\nimg_copy[corners] .= RGB(1.0, 0.0, 0.0)\nimg_copy","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"As you can see, several points which lie in the interior of the object (the house) have also been detected as corners. We can fix this by specifying a higher threshold percentile.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"corners = imcorner(img, Percentile(98.5))\nimg_copy2 = RGB.(img)\nimg_copy2[corners] .= RGB(1.0, 0.0, 0.0)\nimg_copy2","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"This seems much better. A detailed documentation of the function parameters can be found in the documentation of imcorner.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"Internally, imcorner uses one of three algorithms: Harris, Shi Tomasi or Kitchen Rosenfield to detect corners. Which one to use can be specified using the method parameter to imcorner. Each algorithm also has a separate method.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"detection_methods = [harris, shi_tomasi, kitchen_rosenfeld]\nimg_copies = [RGB.(img) for i in 1:length(detection_methods)]\nfor i in 1:length(detection_methods)\n    corners = imcorner(img, Percentile(98.5); method=detection_methods[i])\n    img_copies[i] = RGB.(img)\n    img_copies[i][corners] .= RGB(1.0, 0.0, 0.0)\nend\nmosaicview(img_copies; nrow=1)","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"These algorithms use the gradient of the image to identify corners, because intensities change abruptly at corner points, giving rise to large gradients. However, this makes them computationally expensive.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"The FAST (Features from Accelarated Segment Test) Corner Detector is a feature detection algorithm which is designed to be computationally cheaper, and hence much faster. It classifies a pixel P as a corner if at least n contiguous points out of the 16 points in a circle around it have intensities either higher (or lower) than that of P by a certain threshold t.","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"corners = fastcorners(img, 11, 0.1) # fastcorners(img, n, t) where n and t are optional\nimg_copy3 = RGB.(img)\nimg_copy3[corners] .= RGB(1.0, 0.0, 0.0)\nimg_copy3","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"","category":"page"},{"location":"democards/examples/contours/detecting_corners/","page":"Detecting Corners","title":"Detecting Corners","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/#Alpha-Compositing","page":"Alpha Compositing","title":"Alpha Compositing","text":"","category":"section"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"This demonstration shows how alpha compositing can be done in 10 lines of code using OffsetArrays and PaddedViews","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"using Images\nusing OffsetArrays # provide `OffsetArray`","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/#.-basic-offset-and-pad-operation","page":"Alpha Compositing","title":"1. basic offset and pad operation","text":"","category":"section"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Assume that we have two objects with transparent color and we want to place them on a canvas with partially overlapping.","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"red_patch = fill(RGBA(1., 0., 0., 1), 24, 24)\ngreen_patch = fill(RGBA(0., 1., 0., 1), 32, 32)\nmosaicview(red_patch, green_patch; npad=20, nrow=1, fillvalue=colorant\"white\")","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Assume the canvas axes starts from (1, 1), here we keep the red patch unshifted, and shift the green patch 6 pixels downward and 16 pixels rightward, and then pad them to a common axes","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"green_o = OffsetArray(green_patch, 6, 16)\nr, g = paddedviews(Gray(0.2), red_patch, green_o)\nmosaicview(r, g; npad=20, nrow=1, fillvalue=colorant\"white\")","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Note that their axes and sizes are changed after shifting and padding:","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"# Regardless of implementaion details, `Base.OneTo(32)` is mostly equivalent to `1:32`\nprintln(\"before shifting -- size: \", size(green_patch), \" axes: \", axes(green_patch))\nprintln(\"after shifting  -- size: \", size(green_o), \" axes: \", axes(green_o))\nprintln(\"after padding   -- size: \", size(g), \" axes: \", axes(g))","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Axes are preserved after padding, which means you can easily get original image from padded results using","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"r[axes(red_patch)...]\ng[axes(green_o)...]\nnothing #hide","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"As described here, there are several compositing methods:","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"# add operation\nout_add = r .+ g\n\n# clear operation\nout_clear = copy(r)\nout_clear[axes(green_o)...] .= colorant\"black\"\n\n# multiply operation\nout_mul = copy(r)\n# channel-wise multiplication\nchannelview(out_mul)[:, axes(green_o)...] .*= channelview(green_o)\n\n# overlap operation\nout_over = copy(r)\nout_over[axes(green_o)...] .= green_o\n\n# display the results of these operation\nmosaicview(out_add, out_clear, out_mul, out_over;\n           npad=20, nrow=1, fillvalue=colorant\"white\")","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/#.-build-the-three-primary-color-panel","page":"Alpha Compositing","title":"2. build the three-primary color panel","text":"","category":"section"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Now, let's use the same trick to build something more meaningful. First we create three circles with colors red, green and blue","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"using ImageDraw\nfunction make_circle(sz, c::T) where T\n    # fill with transparent color to avoid black region\n    fillvalue = ARGB(c)\n    img = fill(ARGB{eltype(T)}(0., 0., 0., 0.), sz...)\n    origin = sz .÷ 2\n    r = sz .÷ 4\n    draw!(img, Ellipse(origin..., r...), fillvalue)\n    img\nend\n\n# create three circles with color red, green and blue\nred_c   = make_circle((256, 256), ARGB(1., 0., 0., 1.))\ngreen_c = make_circle((256, 256), ARGB(0., 1., 0., 1.))\nblue_c  = make_circle((256, 256), ARGB(0., 0., 1., 1.))\n\nmosaicview(red_c, green_c, blue_c; nrow=1)","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"Then, shift these circles to appropriate positions, pad them to common axes, and finally composite using the add operation:","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"r = size(red_c, 1) ÷ 8\nred_o   = OffsetArray(red_c,    r,  r)\ngreen_o = OffsetArray(green_c, -r,  0)\nblue_o  = OffsetArray(blue_c,   r, -r)\n\ncolor_panel = sum(paddedviews(zero(eltype(red_o)), red_o, green_o, blue_o))\ncolor_panel = color_panel[axes(red_c)...] # crop empty region","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"","category":"page"},{"location":"democards/examples/spatial_transformation/alpha_compositing/","page":"Alpha Compositing","title":"Alpha Compositing","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"pkgs/metadata/#ImageMetadata.jl","page":"ImageMetaData.jl","title":"ImageMetadata.jl","text":"","category":"section"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"ImageMetadata (a package incorporated into Images) allows you to add metadata to images: for example, the date and time at which it was collected, identifiers for the location or subject, etc. This metadata is stored as a dictionary, and the ImageMeta type combines properties of arrays and Dict.","category":"page"},{"location":"pkgs/metadata/#Introduction","page":"ImageMetaData.jl","title":"Introduction","text":"","category":"section"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"You typically create an ImageMeta using keyword arguments:","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> using Colors, ImageMetadata, Dates\n\njulia> img = ImageMeta(fill(RGB(1,0,0), 3, 2), date=Date(2016, 7, 31), time=\"high noon\")\nRGB ImageMeta with:\n  data: 3×2 Array{RGB{N0f8},2} with eltype RGB{Normed{UInt8,8}}\n  properties:\n    date: 2016-07-31\n    time: high noon","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"DocTestSetup = quote\n    using Colors, ImageMetadata, Dates\n    img = ImageMeta(fill(RGB(1,0,0), 3, 2), date=Date(2016, 7, 31), time=\"high noon\")\nend","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"You can then index elements of img like this:","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> img[1,2]\nRGB{N0f8}(1.0,0.0,0.0)","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"and access and set properties like this:","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> img.time\n\"high noon\"\n\njulia> img.time = \"evening\"\n\"evening\"\n\njulia> img\nRGB ImageMeta with:\n  data: 3×2 Array{RGB{N0f8},2} with eltype RGB{Normed{UInt8,8}}\n  properties:\n    date: 2016-07-31\n    time: evening","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"You can extract the data matrix with arraydata(img):","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> arraydata(img)\n3×2 Array{RGB{N0f8},2} with eltype RGB{FixedPointNumbers.Normed{UInt8,8}}:\n RGB{N0f8}(1.0,0.0,0.0)  RGB{N0f8}(1.0,0.0,0.0)\n RGB{N0f8}(1.0,0.0,0.0)  RGB{N0f8}(1.0,0.0,0.0)\n RGB{N0f8}(1.0,0.0,0.0)  RGB{N0f8}(1.0,0.0,0.0)","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"and the properties dictionary with properties:","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> properties(img)\nDict{Symbol,Any} with 2 entries:\n  :date => Date(\"2016-07-31\")\n  :time => \"high noon\"","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"Properties are not accessed or modified by most of Images' algorithms–-the traits that most affect processing are encoded through Julia's type system.  However, functions that receive an ImageMeta should return an ImageMeta when appropriate. Naturally, in your own code it's fine to use properties to your advantage for custom tasks.","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"If you index a scalar location (a single pixel), img[i,j,...] will return just the value of that pixel. But if you index a range, you get another ImageMeta:","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> c = img[1:2, 1:2]\nRGB ImageMeta with:\n  data: 2×2 Array{RGB{N0f8},2} with eltype RGB{Normed{UInt8,8}}\n  properties:\n    date: 2016-07-31\n    time: high noon","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"This copies both the data (just the relevant portions) and the properties dictionary. In contrast,","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> v = view(img, 1:2, 1:2)\nRGB ImageMeta with:\n  data: 2×2 view(::Array{RGB{N0f8},2}, 1:2, 1:2) with eltype RGB{Normed{UInt8,8}}\n  properties:\n    date: 2016-07-31\n    time: high noon","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"shares both the data and the properties with the original image img. Modifying values or properties in c has no impact on img, but modifying values or properties in v does.","category":"page"},{"location":"pkgs/metadata/#copyproperties/shareproperties","page":"ImageMetaData.jl","title":"copyproperties/shareproperties","text":"","category":"section"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"Two convenient ways to construct a new image with the \"same\" properties are copyproperties (makes a copy of the properties dictionary) and shareproperties (shares the properties dictionary).","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"Incidentally, similar makes a copy of the properties dictionary.","category":"page"},{"location":"pkgs/metadata/#spatialproperties","page":"ImageMetaData.jl","title":"spatialproperties","text":"","category":"section"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"Occasionally you may have a property that is linked to the spatial axes of the image. In such cases, one source for potential confusion is permutedims, which swaps the order of the dimensions in the array: if the order is not also swapped in the appropriate properties, chaos could result.","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"You can declare that certain properties are coupled to spatial axes using \"spatialproperties\":","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"julia> using ImageMetadata\n\njulia> A = reshape(1:15, 3, 5)\n3×5 reshape(::UnitRange{Int64}, 3, 5) with eltype Int64:\n 1  4  7  10  13\n 2  5  8  11  14\n 3  6  9  12  15\n\njulia> img = ImageMeta(A, spatialproperties=Set([:maxsum]), maxsum=[maximum(sum(A,dims=1)), maximum(sum(A,dims=2))])\nInt64 ImageMeta with:\n  data: 3×5 reshape(::UnitRange{Int64}, 3, 5) with eltype Int64\n  properties:\n    maxsum: [42, 45]\n    spatialproperties: Set([:maxsum])\n\njulia> imgp = permutedims(img, (2,1))\nInt64 ImageMeta with:\n  data: 5×3 Array{Int64,2}\n  properties:\n    maxsum: [45, 42]\n    spatialproperties: Set([:maxsum])\n\njulia> maximum(sum(imgp, dims=1))\n45","category":"page"},{"location":"pkgs/metadata/","page":"ImageMetaData.jl","title":"ImageMetaData.jl","text":"It's not possible to anticipate all the possible transformations that might be necessary, but at least simple swaps are handled automatically.","category":"page"},{"location":"function_reference/#page_references","page":"References","title":"Summary and function reference","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Below, [] in an argument list means an optional argument.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"Pages=[\"function_reference.md\"]\ndepth=3","category":"page"},{"location":"function_reference/#ref_io","page":"References","title":"Image loading and saving","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"FileIO.jl is an IO frontend that provides save and load to load images easily. The current available backends for image files are:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"ImageMagick.jl covers most image formats and has extra functionality. This can be your first choice if you don't have a preference.\nQuartzImageIO.jl exposes macOS's native image IO functionality to Julia. In some cases it's faster than ImageMagick, but it might not cover all your needs.\nImageIO.jl is a new image IO backend (requires julia >=v\"1.3\") that provides an optimized performance for PNG files. Check benchmark here\nOMETIFF.jl supports OME-TIFF files. If you don't know what it is, then it is likely that you don't need this package.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"Standard test images are provided by TestImages.jl","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"load\nsave\ntestimage\nshepp_logan","category":"page"},{"location":"function_reference/#FileIO.load","page":"References","title":"FileIO.load","text":"load(filename) loads the contents of a formatted file, trying to infer\n\nthe format from filename and/or magic bytes in the file.\n\nload(strm) loads from an IOStream or similar object. In this case,\n\nthere is no filename extension, so we rely on the magic bytes for format identification.\n\nload(File(format\"PNG\", filename)) specifies the format directly, and bypasses inference.\nload(Stream(format\"PNG\", io)) specifies the format directly, and bypasses inference.\nload(f; options...) passes keyword arguments on to the loader.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#FileIO.save","page":"References","title":"FileIO.save","text":"save(filename, data...) saves the contents of a formatted file,\n\ntrying to infer the format from filename.\n\nsave(Stream(format\"PNG\",io), data...) specifies the format directly, and bypasses inference.\nsave(File(format\"PNG\",filename), data...) specifies the format directly, and bypasses inference.\nsave(f, data...; options...) passes keyword arguments on to the saver.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#TestImages.testimage","page":"References","title":"TestImages.testimage","text":"img = testimage(filename; download_only=false, [ops...])\n\nLoad test image that partially matches filename, the first match is used if there're more than one.\n\nIf download_only=true, the full filepath is returned. Any other keyword arguments ops will be passed to image IO backend through FileIO.load.\n\nExample\n\njulia> using TestImages\njulia> img = testimage(\"cameraman.tif\"); # fullname\njulia> img = testimage(\"cameraman\"); # without extension works\njulia> img = testimage(\"c\"); # with only partial name also works\n\nExtended help\n\nThe following is a complete list of testimages, you can also check them at https://testimages.juliaimages.org/\n\n\"autumn_leaves\"\n\"blobs\"\n\"cameraman\"\n\"earth_apollo17\"\n\"fabio_color_256\"\n\"fabio_color_512\"\n\"fabio_gray_256\"\n\"fabio_gray_512\"\n\"hela-cells\"\n\"house\"\n\"jetplane\"\n\"lake_color\"\n\"lake_gray\"\n\"lena_color_256\"\n\"lena_color_512\"\n\"lena_gray_16bit\"\n\"lena_gray_256\"\n\"lena_gray_512\"\n\"lighthouse\"\n\"livingroom\"\n\"m51\"\n\"mandril_color\"\n\"mandril_gray\"\n\"mandrill\"\n\"moonsurface\"\n\"mountainstream\"\n\"mri-stack\"\n\"multi-channel-time-series.ome\"\n\"peppers_color\"\n\"peppers_gray\"\n\"pirate\"\n\"toucan\"\n\"walkbridge\"\n\"woman_blonde\"\n\"woman_darkhair\"\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.shepp_logan","page":"References","title":"Images.shepp_logan","text":"phantom = shepp_logan(N,[M]; highContrast=true)\n\noutput the NxM Shepp-Logan phantom, which is a standard test image usually used for comparing image reconstruction algorithms in the field of computed tomography (CT) and magnetic resonance imaging (MRI). If the argument M is omitted, the phantom is of size NxN. When setting the keyword argument highConstrast to false, the CT version of the phantom is created. Otherwise, the high contrast MRI version is calculated.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ref_construction","page":"References","title":"Image construction, conversion, and views","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Any array can be treated as an Image.  In graphical environments, only arrays with Colorant element types (Gray, RGB, ARGB, etc.) are automatically displayed as images.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"colorview\nchannelview\nnormedview\nrawview\nStackedView\nPaddedView\npaddedviews\nsym_paddedviews\nMosaicView\nmosaicview\nStreamingContainer","category":"page"},{"location":"function_reference/#ImageCore.colorview","page":"References","title":"ImageCore.colorview","text":"colorview(C, A)\n\nreturns a view of the numeric array A, interpreting successive elements of A as if they were channels of Colorant C.\n\nOf relevance for types like RGB and BGR, the elements of A are interpreted in constructor-argument order, not memory order (see reinterpretc if you want to use memory order).\n\nExample\n\nA = rand(3, 10, 10)\nimg = colorview(RGB, A)\n\nSee also: channelview\n\n\n\n\n\ncolorview(C, gray1, gray2, ...) -> imgC\n\nCombine numeric/grayscale images gray1, gray2, etc., into the separate color channels of an array imgC with element type C<:Colorant.\n\nAs a convenience, the constant zeroarray fills in an array of matched size with all zeros.\n\nExample\n\nimgC = colorview(RGB, r, zeroarray, b)\n\ncreates an image with r in the red chanel, b in the blue channel, and nothing in the green channel.\n\nSee also: StackedView.\n\n\n\n\n\ncolorview(C)\n\nCreate a function that is equivalent to (As...) -> colorview(C, Ax...).\n\nExamples\n\njulia> ones(Float32, 2, 2) |> colorview(Gray)\n2×2 reinterpret(Gray{Float32}, ::Array{Float32,2}):\n Gray{Float32}(1.0)  Gray{Float32}(1.0)\n Gray{Float32}(1.0)  Gray{Float32}(1.0)\n\nThis can be slightly convenient when you want to convert a batch of channel data, for example:\n\njulia> Rs, Gs, Bs = ntuple( i -> [randn(2, 2) for _ in 1:4], 3)\n\njulia> map(colorview(RGB), Rs, Gs, Bs)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.channelview","page":"References","title":"ImageCore.channelview","text":"channelview(A)\n\nreturns a view of A, splitting out (if necessary) the color channels of A into a new first dimension.\n\nOf relevance for types like RGB and BGR, the channels of the returned array will be in constructor-argument order, not memory order (see reinterpretc if you want to use memory order).\n\nExample\n\nimg = rand(RGB{N0f8}, 10, 10)\nA = channelview(img)   # a 3×10×10 array\n\nSee also: colorview\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.normedview","page":"References","title":"ImageCore.normedview","text":"normedview([T], img::AbstractArray{Unsigned})\n\nreturns a \"view\" of img where the values are interpreted in terms of Normed number types. For example, if img is an Array{UInt8}, the view will act like an Array{N0f8}.  Supply T if the element type of img is UInt16, to specify whether you want a N6f10, N4f12, N2f14, or N0f16 result.\n\nSee also: rawview\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.rawview","page":"References","title":"ImageCore.rawview","text":"rawview(img::AbstractArray{FixedPoint})\n\nreturns a \"view\" of img where the values are interpreted in terms of their raw underlying storage. For example, if img is an Array{N0f8}, the view will act like an Array{UInt8}.\n\nSee also: normedview\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.StackedView","page":"References","title":"ImageCore.StackedView","text":"StackedView(B, C, ...) -> A\n\nPresent arrays B, C, etc, as if they are separate channels along the first dimension of A. In particular,\n\nB == A[1,:,:...]\nC == A[2,:,:...]\n\nand so on. Combined with colorview, this allows one to combine two or more grayscale images into a single color image.\n\nSee also: colorview.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#PaddedViews.PaddedView","page":"References","title":"PaddedViews.PaddedView","text":"datapadded = PaddedView(fillvalue, data, padded_axes)\ndatapadded = PaddedView(fillvalue, data, padded_axes, data_axes)\ndatapadded = PaddedView(fillvalue, data, sz)\ndatapadded = PaddedView(fillvalue, data, sz, first_datum)\ndatapadded = PaddedView{T}(args...)\n\nCreate a padded version of the array data, where any elements within the span of padded_axes not assigned in data will have value fillvalue.\n\nSupply data_axes to specify an alterate set of axes for data, effectively relocating data to a different set of indices. This is shorthand for\n\noffsetdata = OffsetArray(data, data_axes)\ndatapadded = PaddedView(fillvalue, offsetdata, padded_axes)\n\nusing the OffsetArrays package.\n\nAlternately, the padded array size sz can be specified, in which case datapadded starts indexing at 1. One may optionally specify the location of the [1, 1, ...] element of data with first_datum. Specifically, datapadded[first_datum...] corresponds to data[1, 1, ...]. first_datum defaults to all-1s.\n\nThe view eltype T is optional. If not specified, then in most cases, T is inferred to be eltype(data). In cases when fillvalue can't be converted to eltype(data), T will be promoted the one that does. For example, when fillvalue == nothing and eltype(data) == Float32, the inferred eltype T will be Union{Nothing, Float32}.\n\nExample\n\njulia> using PaddedViews\n\njulia> a = collect(reshape(1:9, 3, 3))\n3×3 Array{Int64,2}:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> PaddedView(-1, a, (4, 5))\n4×5 PaddedView(-1, ::Array{Int64,2}, (Base.OneTo(4), Base.OneTo(5))) with eltype Int64:\n  1   4   7  -1  -1\n  2   5   8  -1  -1\n  3   6   9  -1  -1\n -1  -1  -1  -1  -1\n\njulia> PaddedView(-1, a, (1:5,1:5), (2:4,2:4))\n5×5 PaddedView(-1, OffsetArray(::Array{Int64,2}, 2:4, 2:4), (1:5, 1:5)) with eltype Int64 with indices 1:5×1:5:\n -1  -1  -1  -1  -1\n -1   1   4   7  -1\n -1   2   5   8  -1\n -1   3   6   9  -1\n -1  -1  -1  -1  -1\n\njulia> PaddedView(-1, a, (0:4, 0:4))\n5×5 PaddedView(-1, ::Array{Int64,2}, (0:4, 0:4)) with eltype Int64 with indices 0:4×0:4:\n -1  -1  -1  -1  -1\n -1   1   4   7  -1\n -1   2   5   8  -1\n -1   3   6   9  -1\n -1  -1  -1  -1  -1\n\njulia> PaddedView(-1, a, (5,5), (2,2))\n5×5 PaddedView(-1, OffsetArray(::Array{Int64,2}, 2:4, 2:4), (Base.OneTo(5), Base.OneTo(5))) with eltype Int64:\n -1  -1  -1  -1  -1\n -1   1   4   7  -1\n -1   2   5   8  -1\n -1   3   6   9  -1\n -1  -1  -1  -1  -1\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#PaddedViews.paddedviews","page":"References","title":"PaddedViews.paddedviews","text":"Aspad = paddedviews(fillvalue, A1, A2, ....)\n\nPad the arrays A1, A2, ..., to a common size or set of axes, chosen as the span of axes enclosing all of the input arrays.\n\nThe padding is applied to one direction. For example, values are filled to bottom-right part of the new array in two-dimensional case. Use sym_paddedviews if both directions need to be padded.\n\nThe axes of original array A will be preserved in the padded result Ap, hence it's true that Ap[CartesianIndices(A)] == A.\n\nExample:\n\njulia> using PaddedViews\n\njulia> a1 = reshape([1, 2, 3], 3, 1)\n3×1 Array{Int64,2}:\n 1\n 2\n 3\n\njulia> a2 = [4 5 6]\n1×3 Array{Int64,2}:\n 4  5  6\n\njulia> a1p, a2p = paddedviews(-1, a1, a2);\n\njulia> a1p\n3×3 PaddedView(-1, ::Array{Int64,2}, (Base.OneTo(3), Base.OneTo(3))) with eltype Int64:\n 1  -1  -1\n 2  -1  -1\n 3  -1  -1\n\njulia> a2p\n3×3 PaddedView(-1, ::Array{Int64,2}, (Base.OneTo(3), Base.OneTo(3))) with eltype Int64:\n  4   5   6\n -1  -1  -1\n -1  -1  -1\n\njulia> a1p[CartesianIndices(a1)]\n3×1 Array{Int64,2}:\n 1\n 2\n 3\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#PaddedViews.sym_paddedviews","page":"References","title":"PaddedViews.sym_paddedviews","text":"Aspad = sym_paddedviews(fillvalue, A1, A2, ....)\n\nPad the arrays A1, A2, ..., to a common size or set of axes, chosen as the span of axes enclosing all of the input arrays.\n\nThe padding is applied to both directions, which means original array located at the center the padded result. Use paddedviews if only one direction need to be padded.\n\nThe axes of original array A will be preserved in the padded result Ap, hence it's true that Ap[CartesianIndices(A)] == A.\n\n```jldoctest julia> using PaddedViews\n\njulia> a1 = reshape([1, 2, 3], 3, 1) 3×1 Array{Int64,2}:  1  2  3\n\njulia> a2 = [4 5 6] 1×3 Array{Int64,2}:  4  5  6\n\njulia> a1p, a2p = sym_paddedviews(-1, a1, a2);\n\njulia> a1p 3×3 PaddedView(-1, ::Array{Int64,2}, (1:3, 0:2)) with eltype Int64 with indices 1:3×0:2:  -1  1  -1  -1  2  -1  -1  3  -1\n\njulia> a2p 3×3 PaddedView(-1, ::Array{Int64,2}, (0:2, 1:3)) with eltype Int64 with indices 0:2×1:3:  -1  -1  -1   4   5   6  -1  -1  -1\n\njulia> a1p[CartesianIndices(a1)] 3×1 Array{Int64,2}:  1  2  3  ```\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#MosaicViews.MosaicView","page":"References","title":"MosaicViews.MosaicView","text":"MosaicView(A::AbstractArray)\n\nCreate a two dimensional \"view\" of the three or four dimensional array A. The resulting MosaicView will display the data in A such that it emulates using vcat for all elements in the third dimension of A, and hcat for all elements in the fourth dimension of A.\n\nFor example, if size(A) is (2,3,4), then the resulting MosaicView will have the size (2*4,3) which is (8,3). Alternatively, if size(A) is (2,3,4,5), then the resulting size will be (2*4,3*5) which is (8,15).\n\nAnother way to think about this is that MosaicView creates a mosaic of all the individual matrices enumerated in the third (and optionally fourth) dimension of the given 3D or 4D array A. This can be especially useful for creating a single composite image from a set of equally sized images.\n\njulia> using MosaicViews\n\njulia> A = [(k+1)*l-1 for i in 1:2, j in 1:3, k in 1:2, l in 1:2]\n2×3×2×2 Array{Int64,4}:\n[:, :, 1, 1] =\n 1  1  1\n 1  1  1\n\n[:, :, 2, 1] =\n 2  2  2\n 2  2  2\n\n[:, :, 1, 2] =\n 3  3  3\n 3  3  3\n\n[:, :, 2, 2] =\n 5  5  5\n 5  5  5\n\njulia> MosaicView(A)\n4×6 MosaicViews.MosaicView{Int64,4,Array{Int64,4}}:\n 1  1  1  3  3  3\n 1  1  1  3  3  3\n 2  2  2  5  5  5\n 2  2  2  5  5  5\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#MosaicViews.mosaicview","page":"References","title":"MosaicViews.mosaicview","text":"mosaicview(A::AbstractArray;\n           [fillvalue=<zero unit>], [npad=0],\n           [nrow], [ncol], [rowmajor=false],\n           [center=true]) -> MosaicView\nmosaicview(As::AbstractArray...; kwargs...)\nmosaicview(As::Union{Tuple, AbstractVector}; kwargs...)\n\nCreate a two dimensional \"view\" from array A or a list of arrays As.\n\nThe resulting MosaicView will display all the matrix slices of the first two dimensions of A arranged as a single large mosaic (in the form of a matrix).\n\nIf multiple arrays in passed, they'll will be center-padded to a common size, and then be concatenated to create a higher dimensional array.\n\nArguments\n\nIn contrast to using the constructor of MosaicView directly, the function mosaicview also allows for a couple of convenience keywords. A typical use case would be to create an image mosaic from a set of input images.\n\nThe parameter fillvalue defines the value that that should be used for empty space. This can be padding caused by npad, or empty mosaic tiles in case the number of matrix slices in A is smaller than nrow*ncol.\nThe parameter npad defines the empty padding space between adjacent mosaic tiles. This can be especially useful if the individual tiles (i.e. matrix slices in A) are images that should be visually separated by some grid lines.\nThe parameters nrow and ncol can be used to choose the number of tiles in row and/or column direction the mosaic should be arranged in. Note that it suffices to specify one of the two parameters, as the other one can be inferred accordingly. The default in case none of the two are specified is nrow = size(A,3).\nIf rowmajor is set to true, then the slices will be arranged left-to-right-top-to-bottom, instead of top-to-bottom-left-to-right (default). The layout only differs in non-trivial cases, i.e., when nrow != 1 and ncol != 1.\nIf center is set to true, then the padded arrays will be shifted to the center instead of in the top-left corner (default). This parameter is only useful when arrays are of different sizes.\n\ntip: Tip\nThis function is not type stable and should only be used if performance is not a priority. To achieve optimized performance, you need to manually construct a MosaicView.\n\nExamples\n\nThe simplest usage is to cat two arrays of the same dimension.\n\njulia> A1 = fill(1, 3, 1)\n3×1 Array{Int64,2}:\n 1\n 1\n 1\n\njulia> A2 = fill(2, 1, 3)\n1×3 Array{Int64,2}:\n 2  2  2\n\njulia> mosaicview(A1, A2)\n6×3 MosaicView{Int64,4, ...}:\n 0  1  0\n 0  1  0\n 0  1  0\n 0  0  0\n 2  2  2\n 0  0  0\n\njulia> mosaicview(A1, A2; center=false)\n 6×3 MosaicView{Int64,4, ...}:\n  1  0  0\n  1  0  0\n  1  0  0\n  2  2  2\n  0  0  0\n  0  0  0\n\nOther keyword arguments can be useful to get a nice looking results.\n\njulia> using MosaicViews\n\njulia> A = [k for i in 1:2, j in 1:3, k in 1:5]\n2×3×5 Array{Int64,3}:\n[:, :, 1] =\n 1  1  1\n 1  1  1\n\n[:, :, 2] =\n 2  2  2\n 2  2  2\n\n[:, :, 3] =\n 3  3  3\n 3  3  3\n\n[:, :, 4] =\n 4  4  4\n 4  4  4\n\n[:, :, 5] =\n 5  5  5\n 5  5  5\n\njulia> mosaicview(A, ncol=2)\n6×6 MosaicViews.MosaicView{Int64,4,...}:\n 1  1  1  4  4  4\n 1  1  1  4  4  4\n 2  2  2  5  5  5\n 2  2  2  5  5  5\n 3  3  3  0  0  0\n 3  3  3  0  0  0\n\njulia> mosaicview(A, nrow=2)\n4×9 MosaicViews.MosaicView{Int64,4,...}:\n 1  1  1  3  3  3  5  5  5\n 1  1  1  3  3  3  5  5  5\n 2  2  2  4  4  4  0  0  0\n 2  2  2  4  4  4  0  0  0\n\njulia> mosaicview(A, nrow=2, rowmajor=true)\n4×9 MosaicViews.MosaicView{Int64,4,...}:\n 1  1  1  2  2  2  3  3  3\n 1  1  1  2  2  2  3  3  3\n 4  4  4  5  5  5  0  0  0\n 4  4  4  5  5  5  0  0  0\n\njulia> mosaicview(A, nrow=2, npad=1, rowmajor=true)\n5×11 MosaicViews.MosaicView{Int64,4,...}:\n 1  1  1  0  2  2  2  0  3  3  3\n 1  1  1  0  2  2  2  0  3  3  3\n 0  0  0  0  0  0  0  0  0  0  0\n 4  4  4  0  5  5  5  0  0  0  0\n 4  4  4  0  5  5  5  0  0  0  0\n\njulia> mosaicview(A, fillvalue=-1, nrow=2, npad=1, rowmajor=true)\n5×11 MosaicViews.MosaicView{Int64,4,...}:\n  1   1   1  -1   2   2   2  -1   3   3   3\n  1   1   1  -1   2   2   2  -1   3   3   3\n -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1\n  4   4   4  -1   5   5   5  -1  -1  -1  -1\n  4   4   4  -1   5   5   5  -1  -1  -1  -1\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageAxes.StreamingContainer","page":"References","title":"ImageAxes.StreamingContainer","text":"A = StreamingContainer{T}(f!, parent, streamingaxes::Axis...)\n\nAn array-like object possessing one or more axes for which changing \"slices\" may be expensive or subject to restrictions. A canonical example would be an AVI stream, where addressing pixels within the same frame is fast but jumping between frames might be slow.\n\nHere's a simple example of dividing by the mean of each slice of an image before returning values.\n\nA = AxisArrays.AxisArray(reshape(1:36, 3, 3, 4))\n\nfunction f!(buffer, slice)\n    meanslice = mean(slice)\n    buffer .= slice./meanslice\nend\n\nB = StreamingContainer{Float64}(f!, A, AxisArrays.axes(A)[3])\n\njulia> A[:,:,1]\n3×3 AxisArray{Int64,2,Array{Int64,2},Tuple{Axis{:row,Base.OneTo{Int64}},Axis{:col,Base.OneTo{Int64}}}}:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> B[:,:,1]\n3×3 Array{Float64,2}:\n 0.2  0.8  1.4\n 0.4  1.0  1.6\n 0.6  1.2  1.8\n\nThe user-provided f! function should take arguments:\n\nf!(buffer, slice)\n\nWhere buffer will be an empty array that can hold a slice of your series, and slice will hold the current input slice.\n\nIt's worth noting that StreamingContainer is not a subtype of AbstractArray, but that much of the array interface (eltype, ndims, axes, size, getindex, and IndexStyle) is supported. A StreamingContainer A can be built from an AxisArray, but it may also be constructed from other \"parent\" objects, even non-arrays, as long as they support the same functions. In either case, the parent should also support the standard AxisArray functions axes, axisnames, axisvalues, and axisdim; this support will be extended to the StreamingContainer.\n\nAdditionally, a StreamingContainer A supports\n\ngetindex!(dest, A, axt::Axis{:time}, ...)\n\nto obtain slices along the streamed axes (here it is assumed that :time is a streamed axis of A). You can implement this directly (dispatching on the parameters of A), or (if the parent is an AbstractArray) rely on the fallback\n\nA.getindex!(dest, view(parent, axs...))\n\nwhere A.getindex! = f! as passed as an argument at construction. dest should have dimensionality ndims(parent)-length(streamingaxes).\n\nOptionally, define StreamIndexStyle(typeof(parent),typeof(f!)).\n\n\n\n\n\n","category":"type"},{"location":"function_reference/","page":"References","title":"References","text":"Images with defined geometry and axis meaning can be constructed using the AxisArrays package:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"using AxisArrays\nimg = AxisArray(A, (:y, :x, :time), (0.25μm, 0.25μm, 0.125s))  # see Unitful.jl for units","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"Custom metadata can be added as follows:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"img = ImageMeta(A, date=now(), patientID=12345)","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"Any of these operations may be composed together, e.g., if you have an m×n×3 UInt8 array, you can put it in canonical RGB format and add metadata:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"img = ImageMeta(colorview(RGB, normedview(permuteddimsview(A, (3,1,2)))), sample=\"control\")","category":"page"},{"location":"function_reference/#Traits","page":"References","title":"Traits","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"These functions are the preferred way to access certain types of \"internal\" data about an image. They can sometimes be useful in allowing you to write generic code.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"pixelspacing\nspacedirections\nsdims\ncoords_spatial\nsize_spatial\nindices_spatial\nnimages\ntimeaxis\nistimeaxis\ntimedim\nassert_timedim_last\nStreamIndexStyle\nIndexAny\nIndexIncremental","category":"page"},{"location":"function_reference/#ImageCore.pixelspacing","page":"References","title":"ImageCore.pixelspacing","text":"pixelspacing(img) -> (sx, sy, ...)\n\nReturn a tuple representing the separation between adjacent pixels along each axis of the image.  Defaults to (1,1,...).  Use ImagesAxes for images with anisotropic spacing or to encode the spacing using physical units.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.spacedirections","page":"References","title":"ImageCore.spacedirections","text":"spacedirections(img)\n\nUsing ImageMetadata, you can set this property manually. For example, you could indicate that a photograph was taken with the camera tilted 30-degree relative to vertical using\n\nimg[\"spacedirections\"] = ((0.866025,-0.5),(0.5,0.866025))\n\nIf not specified, it will be computed from pixelspacing(img), placing the spacing along the \"diagonal\".  If desired, you can set this property in terms of physical units, and each axis can have distinct units.\n\n\n\n\n\nspacedirections(img) -> (axis1, axis2, ...)\n\nReturn a tuple-of-tuples, each axis[i] representing the displacement vector between adjacent pixels along spatial axis i of the image array, relative to some external coordinate system (\"physical coordinates\").\n\nBy default this is computed from pixelspacing, but you can set this manually using ImagesMeta.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.sdims","page":"References","title":"ImageCore.sdims","text":"sdims(img)\n\nReturn the number of spatial dimensions in the image. Defaults to the same as ndims, but with ImagesAxes you can specify that some axes correspond to other quantities (e.g., time) and thus not included by sdims.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.coords_spatial","page":"References","title":"ImageCore.coords_spatial","text":"coords_spatial(img)\n\nReturn a tuple listing the spatial dimensions of img.\n\nNote that a better strategy may be to use ImagesAxes and take slices along the time axis.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.size_spatial","page":"References","title":"ImageCore.size_spatial","text":"size_spatial(img)\n\nReturn a tuple listing the sizes of the spatial dimensions of the image. Defaults to the same as size, but using ImagesAxes you can mark some axes as being non-spatial.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.indices_spatial","page":"References","title":"ImageCore.indices_spatial","text":"indices_spatial(img)\n\nReturn a tuple with the indices of the spatial dimensions of the image. Defaults to the same as indices, but using ImagesAxes you can mark some axes as being non-spatial.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.nimages","page":"References","title":"ImageCore.nimages","text":"nimages(img)\n\nReturn the number of time-points in the image array. Defaults to\n\nUse ImagesAxes if you want to use an explicit time dimension.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageAxes.timeaxis","page":"References","title":"ImageAxes.timeaxis","text":"timeaxis(A)\n\nReturn the time axis, if present, of the array A, and nothing otherwise.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageAxes.istimeaxis","page":"References","title":"ImageAxes.istimeaxis","text":"istimeaxis(ax)\n\nTest whether the axis ax corresponds to time.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageAxes.timedim","page":"References","title":"ImageAxes.timedim","text":"timedim(img) -> d::Int\n\nReturn the dimension of the array used for encoding time, or 0 if not using an axis for this purpose.\n\nNote: if you want to recover information about the time axis, it is generally better to use timeaxis.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.assert_timedim_last","page":"References","title":"ImageCore.assert_timedim_last","text":"assert_timedim_last(img)\n\nThrow an error if the image has a time dimension that is not the last dimension.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageAxes.StreamIndexStyle","page":"References","title":"ImageAxes.StreamIndexStyle","text":"style = StreamIndexStyle(A)\n\nA trait that indicates the degree of support for indexing the streaming axes of A. Choices are IndexAny() and IndexIncremental() (for arrays that only permit advancing the time axis, e.g., a video stream from a webcam). The default value is IndexAny().\n\nThis should be specialized for the type rather than the instance. For a StreamingContainer S, you can define this trait via\n\nStreamIndexStyle(::Type{P}, ::typeof(f!)) = IndexIncremental()\n\nwhere P = typeof(parent(S)).\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageAxes.IndexAny","page":"References","title":"ImageAxes.IndexAny","text":"IndexAny()\n\nIndicates that an axis supports full random-access indexing.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageAxes.IndexIncremental","page":"References","title":"ImageAxes.IndexIncremental","text":"IndexIncremental()\n\nIndicates that an axis supports only incremental indexing, i.e., from i to i+1. This is commonly used for the temporal axis with media streams.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Element-transformation-and-intensity-scaling","page":"References","title":"Element transformation and intensity scaling","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"clamp01\nclamp01!\nclamp01nan\nclamp01nan!\nscaleminmax\nscalesigned\ncolorsigned\ntakemap","category":"page"},{"location":"function_reference/#ImageCore.clamp01","page":"References","title":"ImageCore.clamp01","text":"clamp01(x) -> y\n\nProduce a value y that lies between 0 and 1, and equal to x when x is already in this range. Equivalent to clamp(x, 0, 1) for numeric values. For colors, this function is applied to each color channel separately.\n\nSee also: clamp01!, clamp01nan.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.clamp01!","page":"References","title":"ImageCore.clamp01!","text":"clamp01!(array::AbstractArray)\n\nRestrict values in array to [0, 1], in-place. See also clamp01.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.clamp01nan","page":"References","title":"ImageCore.clamp01nan","text":"clamp01nan(x) -> y\n\nSimilar to clamp01, except that any NaN values are changed to 0.\n\nSee also: clamp01nan!, clamp01.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.clamp01nan!","page":"References","title":"ImageCore.clamp01nan!","text":"clamp01nan!(array::AbstractArray)\n\nSimilar to clamp01!, except that any NaN values are changed to 0.\n\nSee also: clamp01!, clamp01nan\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.scaleminmax","page":"References","title":"ImageCore.scaleminmax","text":"scaleminmax(min, max) -> f\nscaleminmax(T, min, max) -> f\n\nReturn a function f which maps values less than or equal to min to 0, values greater than or equal to max to 1, and uses a linear scale in between. min and max should be real values.\n\nOptionally specify the return type T. If T is a colorant (e.g., RGB), then scaling is applied to each color channel.\n\nExamples\n\nExample 1\n\njulia> f = scaleminmax(-10, 10)\n(::#9) (generic function with 1 method)\n\njulia> f(10)\n1.0\n\njulia> f(-10)\n0.0\n\njulia> f(5)\n0.75\n\nExample 2\n\njulia> c = RGB(255.0,128.0,0.0)\nRGB{Float64}(255.0,128.0,0.0)\n\njulia> f = scaleminmax(RGB, 0, 255)\n(::#13) (generic function with 1 method)\n\njulia> f(c)\nRGB{Float64}(1.0,0.5019607843137255,0.0)\n\nSee also: takemap.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.scalesigned","page":"References","title":"ImageCore.scalesigned","text":"scalesigned(maxabs) -> f\n\nReturn a function f which scales values in the range [-maxabs, maxabs] (clamping values that lie outside this range) to the range [-1, 1].\n\nSee also: colorsigned.\n\n\n\n\n\nscalesigned(min, center, max) -> f\n\nReturn a function f which scales values in the range [min, center] to [-1,0] and [center,max] to [0,1]. Values smaller than min/max get clamped to min/max, respectively.\n\nSee also: colorsigned.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.colorsigned","page":"References","title":"ImageCore.colorsigned","text":"colorsigned()\ncolorsigned(colorneg, colorpos) -> f\ncolorsigned(colorneg, colorcenter, colorpos) -> f\n\nDefine a function that maps negative values (in the range [-1,0]) to the linear colormap between colorneg and colorcenter, and positive values (in the range [0,1]) to the linear colormap between colorcenter and colorpos.\n\nThe default colors are:\n\ncolorcenter: white\ncolorneg: green1\ncolorpos: magenta\n\nSee also: scalesigned.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.takemap","page":"References","title":"ImageCore.takemap","text":"takemap(f, A) -> fnew\ntakemap(f, T, A) -> fnew\n\nGiven a value-mapping function f and an array A, return a \"concrete\" mapping function fnew. When applied to elements of A, fnew should return valid values for storage or display, for example in the range from 0 to 1 (for grayscale) or valid colorants. fnew may be adapted to the actual values present in A, and may not produce valid values for any inputs not in A.\n\nOptionally one can specify the output type T that fnew should produce.\n\nExample:\n\njulia> A = [0, 1, 1000];\n\njulia> f = takemap(scaleminmax, A)\n(::#7) (generic function with 1 method)\n\njulia> f.(A)\n3-element Array{Float64,1}:\n 0.0\n 0.001\n 1.0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Storage-type-transformation","page":"References","title":"Storage-type transformation","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"float32\nfloat64\nn0f8\nn6f10\nn4f12\nn2f14\nn0f16","category":"page"},{"location":"function_reference/#ImageCore.float32","page":"References","title":"ImageCore.float32","text":"float32.(img)\n\nconverts the raw storage type of img to Float32, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.float64","page":"References","title":"ImageCore.float64","text":"float64.(img)\n\nconverts the raw storage type of img to Float64, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.n0f8","page":"References","title":"ImageCore.n0f8","text":"n0f8.(img)\n\nconverts the raw storage type of img to N0f8, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.n6f10","page":"References","title":"ImageCore.n6f10","text":"n6f10.(img)\n\nconverts the raw storage type of img to N6f10, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.n4f12","page":"References","title":"ImageCore.n4f12","text":"n4f12.(img)\n\nconverts the raw storage type of img to N4f12, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.n2f14","page":"References","title":"ImageCore.n2f14","text":"n2f14.(img)\n\nconverts the raw storage type of img to N2f14, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageCore.n0f16","page":"References","title":"ImageCore.n0f16","text":"n0f16.(img)\n\nconverts the raw storage type of img to N0f16, without changing the color space.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Color-channels","page":"References","title":"Color channels","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"You can extract the numeric value of particular color channels:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"gray\nred\ngreen\nblue\nalpha","category":"page"},{"location":"function_reference/#ColorTypes.gray","page":"References","title":"ColorTypes.gray","text":"gray(c) returns the gray component of a grayscale opaque or transparent color.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.red","page":"References","title":"ColorTypes.red","text":"red(c) returns the red component of an AbstractRGB opaque or transparent color.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.green","page":"References","title":"ColorTypes.green","text":"green(c) returns the green component of an AbstractRGB opaque or transparent color.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.blue","page":"References","title":"ColorTypes.blue","text":"blue(c) returns the blue component of an AbstractRGB opaque or transparent color.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.alpha","page":"References","title":"ColorTypes.alpha","text":"alpha(p) extracts the alpha component of a color. For a color without an alpha channel, it will always return 1.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/","page":"References","title":"References","text":"You can also perform operations on channels:","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"mapc\nreducec\nmapreducec","category":"page"},{"location":"function_reference/#ColorTypes.mapc","page":"References","title":"ColorTypes.mapc","text":"mapc(f, rgb) -> rgbf\nmapc(f, rgb1, rgb2) -> rgbf\n\nmapc applies the function f to each color channel of the input color(s), returning an output color in the same colorspace.\n\nExamples:\n\njulia> mapc(x->clamp(x,0,1), RGB(-0.2,0.3,1.2))\nRGB{Float64}(0.0,0.3,1.0)\n\njulia> mapc(max, RGB(0.1,0.8,0.3), RGB(0.5,0.5,0.5))\nRGB{Float64}(0.5,0.8,0.5)\n\njulia> mapc(+, RGB(0.1,0.8,0.3), RGB(0.5,0.5,0.5))\nRGB{Float64}(0.6,1.3,0.8)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.reducec","page":"References","title":"ColorTypes.reducec","text":"reducec(op, v0, c)\n\nReduce across color channels of c with the binary operator op. v0 is the neutral element used to initiate the reduction. For grayscale,\n\nreducec(op, v0, c::Gray) = op(v0, comp1(c))\n\nwhereas for RGB\n\nreducec(op, v0, c::RGB) = op(comp3(c), op(comp2(c), op(v0, comp1(c))))\n\nIf c has an alpha channel, it is always the last one to be folded into the reduction.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ColorTypes.mapreducec","page":"References","title":"ColorTypes.mapreducec","text":"mapreducec(f, op, v0, c)\n\nReduce across color channels of c with the binary operator op, first applying f to each channel. v0 is the neutral element used to initiate the reduction. For grayscale,\n\nmapreducec(f, op, v0, c::Gray) = op(v0, f(comp1(c)))\n\nwhereas for RGB\n\nmapreducec(f, op, v0, c::RGB) = op(f(comp3(c)), op(f(comp2(c)), op(v0, f(comp1(c)))))\n\nIf c has an alpha channel, it is always the last one to be folded into the reduction.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Color-conversion","page":"References","title":"Color conversion","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"imgg = Gray.(img)","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"calculates a grayscale representation of a color image using the Rec 601 luma.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"imghsv = HSV.(img)","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"converts to an HSV representation of color information.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"The ColorTypes package has a rich set of traits that allow you to perform generic operations on color types, see its README for more information.","category":"page"},{"location":"function_reference/#Image-algorithms","page":"References","title":"Image algorithms","text":"","category":"section"},{"location":"function_reference/#Linear-filtering","page":"References","title":"Linear filtering","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"imfilter\nimfilter!\nimgradients","category":"page"},{"location":"function_reference/#ImageFiltering.imfilter","page":"References","title":"ImageFiltering.imfilter","text":"imfilter([T], img, kernel, [border=\"replicate\"], [alg]) --> imgfilt\nimfilter([r], img, kernel, [border=\"replicate\"], [alg]) --> imgfilt\nimfilter(r, T, img, kernel, [border=\"replicate\"], [alg]) --> imgfilt\n\nFilter a one, two or multidimensional array img with a kernel by computing their correlation.\n\nDetails\n\nThe term filtering emerges in the context of a Fourier transformation of an image, which maps an image from its canonical spatial domain to its concomitant frequency domain. Manipulating an image in the frequency domain amounts to retaining or discarding particular frequency components—a process analogous to sifting or filtering [1].  Because the Fourier transform establishes a link between the spatial and frequency representation of an image, one can interpret various image manipulations in the spatial domain as filtering operations which accept or reject specific frequencies.\n\nThe phrase spatial filtering is often used to emphasise that an operation is, at least conceptually, devised in the context of the spatial domain of an image. One further distinguishes between linear and non-linear spatial filtering. A filter is called linear if the operation performed on the pixels is linear, and is labeled non-linear otherwise.\n\nAn image filter can be represented by a function\n\n w sin mathbbZ mid -k_1 le s le k_1   times  t in mathbbZ mid -k_2 le t le k_2     rightarrow mathbbR\n\nwhere k_i  in mathbbN (i = 1,2). It is common to define k_1 = 2a+1 and k_2 = 2b + 1, where a and b are integers, which ensures that the filter dimensions are of odd size. Typically, k_1 equals k_2 and so, dropping the subscripts, one speaks of a k times k filter. Since the domain of the filter represents a grid of spatial coordinates, the filter is often called a mask and is visualized as a grid. For example, a 3 times 3 mask can be potrayed as follows:\n\nscriptsize\nbeginmatrix\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(-1-1) \nphantomw(-9-9) \nendmatrix\n\n\n\n\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(-10) \nphantomw(-9-9) \nendmatrix\n\n \nboxed\nbeginmatrix\nphantomw(-9-9) \nw(-11) \nphantomw(-9-9) \nendmatrix\n\n\n\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(0-1) \nphantomw(-9-9) \nendmatrix\n\n\n\n\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(00) \nphantomw(-9-9) \nendmatrix\n\n \nboxed\nbeginmatrix\nphantomw(-9-9) \nw(01) \nphantomw(-9-9) \nendmatrix\n\n\n\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(1-1) \nphantomw(-9-9) \nendmatrix\n\n\n\n\nboxed\nbeginmatrix\nphantomw(-9-9) \nw(10) \nphantomw(-9-9) \nendmatrix\n\n \nboxed\nbeginmatrix\nphantomw(-9-9) \nw(11) \nphantomw(-9-9) \nendmatrix\n\nendmatrix\n\nThe values of w(st) are referred to as filter coefficients.\n\nDiscrete convolution versus correlation\n\nThere are two fundamental and closely related operations that one regularly performs on an image with a filter. The operations are called discrete correlation and convolution.\n\nThe correlation operation, denoted by the symbol star,  is given in two dimensions by the expression\n\nbeginaligned\ng(xy) = w(xy) star f(xy) = sum_s = -a^a sum_t=-b^b w(st) f(x+s y+t)\nendaligned\n\nwhereas the comparable convolution operation, denoted by the symbol ast, is given in two dimensions by\n\nbeginaligned\nh(xy) = w(xy) ast f(xy) = sum_s = -a^a sum_t=-b^b w(st) f(x-s y-t)\nendaligned\n\nSince a digital image is of finite extent, both of these operations are undefined at the borders of the image. In particular, for an image of size M times N, the function f(x pm s y pm t) is only defined for 1 le x pm s le N and 1 le y pm t le M. In practice one addresses this problem by artificially expanding the domain of the image. For example, one can pad the image with zeros. Other padding strategies are possible, and they are discussed in more detail in the Options section of this documentation.\n\nOne-dimensional illustration\n\nThe difference between correlation and convolution is best understood with recourse to a one-dimensional example  adapted from [1]. Suppose that a filter w-101rightarrow mathbbR has coefficients\n\nbeginmatrix\nboxed1  boxed2  boxed3\nendmatrix\n\nConsider a discrete unit impulse function f x in mathbbZ mid 1 le x le 7   rightarrow 01  that has been padded with zeros. The function can be visualised as an image\n\nboxed\nbeginmatrix\n0  boxed0  boxed0  boxed0  boxed1  boxed0  boxed0  boxed0  0\nendmatrix\n\nThe correlation operation can be interpreted as sliding w along the image and computing the sum of products at each location. For example,\n\nbeginmatrix\n0  0  0  0  1  0  0  0  0 \n1  2  3         \n 1  2  3         \n  1  2  3        \n   1  2  3       \n    1  2  3      \n     1  2  3    \n      1  2  3\nendmatrix\n\nyields the output g x in mathbbZ mid 1 le x le 7   rightarrow mathbbR, which when visualized as a digital image, is equal to\n\nboxed\nbeginmatrix\nboxed0  boxed0  boxed3  boxed2  boxed1  boxed0  boxed0\nendmatrix\n\nThe interpretation of the convolution operation is analogous to correlation, except that the filter w has been rotated by 180 degrees. In particular,\n\nbeginmatrix\n0  0  0  0  1  0  0  0  0 \n3  2  1         \n 3  2  1         \n  3  2  1        \n   3  2  1       \n    3  2  1      \n     3  2  1    \n      3  2  1\nendmatrix\n\nyields the output h x in mathbbZ mid 1 le x le 7   rightarrow mathbbR equal to\n\nboxed\nbeginmatrix\nboxed0  boxed0  boxed1  boxed2  boxed3  boxed0  boxed0\nendmatrix\n\nInstead of rotating the filter mask, one could instead rotate f and still obtained the same convolution result. In fact, the conventional notation for convolution indicates that f is flipped and not w. If w is symmetric, then convolution and correlation give the same outcome.\n\nTwo-dimensional illustration\n\nFor a two-dimensional example, suppose the filter w-1 0 1 times  -101 rightarrow mathbbR  has coefficients\n\n beginmatrix\n boxed1  boxed2  boxed3  \n boxed4  boxed5  boxed6  \n boxed7  boxed8  boxed9\n endmatrix\n\nand consider a two-dimensional discrete unit impulse function\n\n fx in mathbbZ mid 1 le x le 7   times  y in mathbbZ mid 1 le y le 7  rightarrow  01\n\nthat has been padded with zeros:\n\n boxed\n beginmatrix\n   0         0          0          0           0          0     0   \n   0  boxed0  boxed0  boxed0   boxed0  boxed0    0   \n   0  boxed0  boxed0  boxed0   boxed0  boxed0    0  \n   0  boxed0  boxed0  boxed1   boxed0  boxed0    0  \n   0  boxed0  boxed0  boxed0   boxed0  boxed0    0  \n   0  boxed0  boxed0  boxed0   boxed0  boxed0    0  \n   0         0          0          0           0          0     0\n endmatrix\n\nThe correlation operation w(xy) star f(xy)  yields the output\n\n boxed\n beginmatrix\n boxed0  boxed0   boxed0  boxed0  boxed0  \n boxed0   boxed9  boxed8  boxed7  boxed0  \n boxed0   boxed6  boxed5  boxed4  boxed0  \n boxed0   boxed3  boxed2  boxed1  boxed0  \n boxed0  boxed0   boxed0  boxed0  boxed0\n endmatrix\n\nwhereas the convolution operation w(xy) ast f(xy) produces\n\n boxed\n beginmatrix\n boxed0  boxed0  boxed0  boxed0  boxed0  \n boxed0  boxed1  boxed2  boxed3  boxed0 \n boxed0  boxed4  boxed5  boxed6  boxed0  \n boxed0  boxed7  boxed8  boxed9  boxed0  \n boxed0  boxed0  boxed0  boxed0  boxed0\n endmatrix\n\nDiscrete convolution and correlation as matrix multiplication\n\nDiscrete convolution and correlation operations can also be formulated as a matrix multiplication, where one of the inputs is converted to a Toeplitz matrix, and the other is represented as a column vector. For example, consider a function fx in mathbbN mid 1 le x le M  rightarrow mathbbR and a filter w s in mathbbN mid  -k_1 le s le k_1   rightarrow mathbbR. Then the matrix multiplication\n\nbeginbmatrix\nw(-k_1) \t  0\t     ldots\t 0\t\t    0\t\t\t\nvdots \t w(-k_1) \t ldots\t vdots   0\t        \nw(k_1) \t     vdots    ldots\t 0\t\t    vdots    \n0 \t    \t w(k_1)\t ldots    w(-k_1)   0\t\t    \n0 \t         0\t\t     ldots\t vdots   w(-k_1)\t\nvdots      vdots\t ldots\t w(k_1)    vdots\t\n0            0          0\t\t\t 0\t\t    w(k_1)\nendbmatrix\nbeginbmatrix\nf(1) \nf(2) \nf(3) \nvdots \nf(M)\nendbmatrix\n\nis equivalent to the convolution w(s) ast f(x) assuming that the border of f(x) has been padded with zeros.\n\nTo represent multidimensional convolution as matrix multiplication one reshapes the multidimensional arrays into column vectors and proceeds in an analogous manner. Naturally, the result of the matrix multiplication will need to be reshaped into an appropriate multidimensional array.\n\nOptions\n\nThe following subsections describe valid options for the function arguments in more detail.\n\nChoices for r\n\nYou can dispatch to different implementations by passing in a resource r as defined by the ComputationalResources package. For example,\n\n    imfilter(ArrayFireLibs(), img, kernel)\n\nwould request that the computation be performed on the GPU using the ArrayFire libraries.\n\nChoices for T\n\nOptionally, you can control the element type of the output image by passing in a type T as the first argument.\n\nChoices for img\n\nYou can specify a one, two or multidimensional array defining your image.\n\nChoices for kernel\n\nThe kernel[0,0,..] parameter corresponds to the origin (zero displacement) of the kernel; you can use centered to place the origin at the array center, or use the OffsetArrays package to set kernel's indices manually. For example, to filter with a random centered 3x3 kernel, you could use either of the following:\n\nkernel = centered(rand(3,3))\nkernel = OffsetArray(rand(3,3), -1:1, -1:1)\n\nThe kernel parameter can be specified as an array or as a \"factored kernel\", a tuple (filt1, filt2, ...) of filters to apply along each axis of the image. In cases where you know your kernel is separable, this format can speed processing. Each of these should have the same dimensionality as the image itself, and be shaped in a manner that indicates the filtering axis, e.g., a 3x1 filter for filtering the first dimension and a 1x3 filter for filtering the second dimension. In two dimensions, any kernel passed as a single matrix is checked for separability; if you want to eliminate that check, pass the kernel as a single-element tuple, (kernel,).\n\nChoices for border\n\nAt the image edge, border is used to specify the padding which will be used to extrapolate the image beyond its original bounds. As an indicative example of each option the results of the padding are illustrated on an image consisting of a row of six pixels which are specified alphabetically: boxeda  b  c  d  e  f. We show the effects of padding only on the left and right border, but analogous consequences hold for the top and bottom border.\n\n\"replicate\" (default)\n\nThe border pixels extend beyond the image boundaries.\n\nboxed\nbeginarraylcr\n  a a a a    a  b  c  d  e  f  f  f  f  f\nendarray\n\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\n\"circular\"\n\nThe border pixels wrap around. For instance, indexing beyond the left border returns values starting from the right border.\n\nboxed\nbeginarraylcr\n  c d e f    a  b  c  d  e  f  a  b  c  d\nendarray\n\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\n\"reflect\"\n\nThe border pixels reflect relative to a position between pixels. That is, the border pixel is omitted when mirroring.\n\nboxed\nbeginarraylcr\n  e d c b    a  b  c  d  e  f  e  d  c  b\nendarray\n\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\n\"symmetric\"\n\nThe border pixels reflect relative to the edge itself.\n\nboxed\nbeginarraylcr\n  d c b a    a  b  c  d  e  f  f  e  d  c\nendarray\n\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\nFill(m)\n\nThe border pixels are filled with a specified value m.\n\nboxed\nbeginarraylcr\n  m m m m    a  b  c  d  e  f  m  m  m  m\nendarray\n\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\nInner()\n\nIndicate that edges are to be discarded in filtering, only the interior of the result is to be returned.\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\nNA()\n\nChoose filtering using \"NA\" (Not Available) boundary conditions. This is most appropriate for filters that have only positive weights, such as blurring filters.\n\nSee also: Pad, padarray, Inner, NA  and NoPad\n\nChoices for alg\n\nThe alg parameter allows you to choose the particular algorithm: FIR() (finite impulse response, aka traditional digital filtering) or FFT() (Fourier-based filtering). If no choice is specified, one will be chosen based on the size of the image and kernel in a way that strives to deliver good performance. Alternatively you can use a custom filter type, like KernelFactors.IIRGaussian.\n\nExamples\n\nThe following subsections highlight some common use cases.\n\nConvolution versus correlation\n\n\n# Create a two-dimensional discrete unit impulse function.\nf = fill(0,(9,9));\nf[5,5] = 1;\n\n# Specify a filter coefficient mask and set the center of the mask as the origin.\nw = centered([1 2 3; 4 5 6 ; 7 8 9]);\n\n#=\n The default operation of `imfilter` is correlation.  By reflecting `w` we\n compute the convolution of `f` and `w`.  `Fill(0,w)` indicates that we wish to\n pad the border of `f` with zeros. The amount of padding is automatically\n determined by considering the length of w.\n=#\ncorrelation = imfilter(f,w,Fill(0,w))\nconvolution = imfilter(f,reflect(w),Fill(0,w))\n\n\nMiscellaneous border padding options\n\n# Example function values f, and filter coefficients w.\nf = reshape(1.0:81.0,9,9)\nw = centered(reshape(1.0:9.0,3,3))\n\n# You can designate the type of padding by specifying an appropriate string.\nimfilter(f,w,\"replicate\")\nimfilter(f,w,\"circular\")\nimfilter(f,w,\"symmetric\")\nimfilter(f,w,\"reflect\")\n\n# Alternatively, you can explicitly use the Pad type to designate the padding style.\nimfilter(f,w,Pad(:replicate))\nimfilter(f,w,Pad(:circular))\nimfilter(f,w,Pad(:symmetric))\nimfilter(f,w,Pad(:reflect))\n\n# If you want to pad with a specific value then use the Fill type.\nimfilter(f,w,Fill(0,w))\nimfilter(f,w,Fill(1,w))\nimfilter(f,w,Fill(-1,w))\n\n#=\n  Specify 'Inner()' if you want to retrieve the interior sub-array of f for which\n  the filtering operation is defined without padding.\n=#\nimfilter(f,w,Inner())\n\nReferences\n\nR. C. Gonzalez and R. E. Woods. Digital Image Processing (3rd Edition).  Upper Saddle River, NJ, USA: Prentice-Hall,  2006.\n\nSee also: imfilter!, centered, padarray, Pad, Fill, Inner, KernelFactors.IIRGaussian.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.imfilter!","page":"References","title":"ImageFiltering.imfilter!","text":"imfilter!(imgfilt, img, kernel, [border=\"replicate\"], [alg])\nimfilter!(r, imgfilt, img, kernel, border::Pad)\nimfilter!(r, imgfilt, img, kernel, border::NoPad, [inds=axes(imgfilt)])\n\nFilter an array img with kernel kernel by computing their correlation, storing the result in imgfilt.\n\nThe indices of imgfilt determine the region over which the filtered image is computed–-you can use this fact to select just a specific region of interest, although be aware that the input img might still get padded.  Alteratively, explicitly provide the indices inds of imgfilt that you want to calculate, and use NoPad boundary conditions. In such cases, you are responsible for supplying appropriate padding: img must be indexable for all of the locations needed for calculating the output. This syntax is best-supported for FIR filtering; in particular, that that IIR filtering can lead to results that are inconsistent with respect to filtering the entire array.\n\nSee also: imfilter.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.imgradients","page":"References","title":"ImageFiltering.imgradients","text":"    imgradients(img, kernelfun=KernelFactors.ando3, border=\"replicate\") -> gimg1, gimg2, ...\n\nEstimate the gradient of img in the direction of the first and second dimension at all points of the image, using a kernel specified by kernelfun.\n\nOutput\n\nThe gradient is returned as a tuple-of-arrays, one for each dimension of the input; gimg1 corresponds to the derivative with respect to the first dimension, gimg2 to the second, and so on.\n\nDetails\n\nTo appreciate the difference between various gradient estimation methods it is helpful to distinguish between: (1) a continuous scalar-valued analogue image f_textrmA(x_1x_2), where x_1x_2 in mathbbR, and (2) its discrete digital realization f_textrmD(x_1x_2), where x_1x_2 in mathbbN, 1 le x_1 le M and 1 le x_2 le N.\n\nAnalogue image\n\nThe gradient of a continuous analogue image f_textrmA(x_1x_2) at location (x_1x_2) is defined as the vector\n\nnabla mathbff_textrmA(x_1x_2) = fracpartial\nf_textrmA(x_1x_2)partial x_1 mathbfe_1 +\nfracpartial f_textrmA(x_1x_2)partial x_2 mathbfe_2\n\nwhere mathbfe_d (d = 12) is the unit vector in the x_d-direction. The gradient points in the direction of maximum rate of change of f_textrmA at the coordinates (x_1x_2). The gradient can be used to compute the derivative of a function in an arbitrary direction. In particular, the derivative of f_textrmA in the direction of a unit vector mathbfu is given by nabla_mathbfuf_textrmA(x_1x_2) = nabla mathbff_textrmA(x_1x_2) cdot mathbfu, where cdot denotes the dot product.\n\nDigital image\n\nIn practice, we acquire a digital image f_textrmD(x_1x_2) where the light intensity is known only at a discrete set of locations. This means that the required partial derivatives are undefined and need to be approximated using discrete difference formulae [1].\n\nA straightforward way to approximate the partial derivatives is to use central-difference formulae\n\n fracpartial f_textrmD(x_1x_2)partial x_1  approx\n        fracf_textrmD(x_1+1x_2) - f_textrmD(x_1-1x_2) 2\n\nand\n\n fracpartial f_textrmD(x_1x_2)partial x_2   approx\n         fracf_textrmD(x_1x_2+1) - f_textrmD(x_1x_2+1)2\n\nHowever, the central-difference formulae are very sensitive to noise. When working with noisy image data, one can obtain a better approximation of the partial derivatives by using a suitable weighted combination of the neighboring image intensities. The weighted combination can be represented as a discrete convolution operation between the image and a kernel which characterizes the requisite weights. In particular, if h_x_d (d = 12) represents a 2r+1 times 2r+1 kernel, then\n\n fracpartial f_textrmD(x_1x_2)partial x_d  approx\nsum_i = -r^r sum_j = -r^r\nf_textrmD(x_1-ix_2-j)\n  h_x_d(ij)\n\nThe kernel is frequently also called a mask or convolution matrix.\n\nWeighting schemes and approximation error\n\nThe choice of weights determines the magnitude of the approximation error and whether the finite-difference scheme is isotropic. A finite-difference scheme is isotropic if the approximation error does not depend on the orientation of the coordinate system and anisotropic if the approximation error has a directional bias [2]. With a continuous analogue image the magnitude of the gradient would be invariant upon rotation of the coordinate system, but in practice one cannot obtain perfect isotropy with a finite set of discrete points. Hence a finite-difference scheme is typically considered isotropic if the leading error term in the approximation does not have preferred directions.\n\nMost finite-difference schemes that are used in image processing are based on 3 times 3 kernels, and as noted by [7], many can also be parametrized by a single parameter alpha as follows:\n\nmathbfH_x_1 =\nfrac14 + 2alpha\nbeginbmatrix\n-1  -alpha  -1 \n0  0  0 \n 1  alpha  1\nendbmatrix\nquad\ntextand\nquad\nmathbfH_x_2 =\nfrac12 + 4alpha\nbeginbmatrix\n-1  0  1 \n-alpha  0  alpha \n -1  0  1\nendbmatrix\n\nwhere\n\nalpha =\nbegincases\n0   textSimple Finite Difference \n1   textPrewitt \n2   textSobel \n24351   textAndo \nfrac103   textScharr \n4   textBickley\nendcases\n\nSeparable kernel\n\nA kernel is called separable if it can be expressed as the convolution of two one-dimensional filters. With a matrix representation of the kernel, separability means that the kernel matrix can be written as an outer product of two vectors. Separable kernels offer computational advantages since instead of performing a two-dimensional convolution one can perform a sequence of one-dimensional convolutions.\n\nOptions\n\nYou can specify your choice of the finite-difference scheme via the kernelfun parameter. You can also indicate how to deal with the pixels on the border of the image with the border parameter.\n\nChoices for kernelfun\n\nIn general kernelfun can be any function which satisfies the following interface:\n\n    kernelfun(extended::NTuple{N,Bool}, d) -> kern_d,\n\nwhere kern_d is the kernel for producing the derivative with respect to the dth dimension of an N-dimensional array. The parameter extended[i] is true if the image is of size > 1 along dimension i. The parameter kern_d may be provided as a dense or factored kernel, with factored representations recommended when the kernel is separable.\n\nSome valid kernelfun options are described below.\n\nKernelFactors.prewitt\n\nWith the prewit option [3] the computation of the gradient is based on the kernels\n\nbeginaligned\nmathbfH_x_1  = frac16\n    beginbmatrix\n    -1  -1  -1 \n    0  0  0 \n    1  1  1\n    endbmatrix\n\nmathbfH_x_2  =  frac16\n    beginbmatrix\n    -1  0  1 \n    -1  0  1 \n    -1  0  1\n    endbmatrix \n = frac16\n    beginbmatrix\n    1 \n    1  \n    1\n    endbmatrix\n    beginbmatrix\n    -1  0  1\n    endbmatrix\n\n = frac16\n    beginbmatrix\n    -1 \n    0  \n    1\n    endbmatrix\n    beginbmatrix\n    1  1  1\n    endbmatrix\nendaligned\n\nSee also: KernelFactors.prewitt and Kernel.prewitt\n\nKernelFactors.sobel\n\nThe sobel option [4] designates the kernels\n\nbeginaligned\nmathbfH_x_1  = frac18\n    beginbmatrix\n    -1  -2  -1 \n     0  0  0 \n     1  2  1\n    endbmatrix\n\nmathbfH_x_2  = frac18\n    beginbmatrix\n    -1  0  1 \n    -2  0  2 \n    -1  0  1\n    endbmatrix \n = frac18\n    beginbmatrix\n    -1 \n    0  \n    1\n    endbmatrix\n    beginbmatrix\n    1  2  1\n    endbmatrix\n\n = frac18\n    beginbmatrix\n    1 \n    2  \n    1\n    endbmatrix\n    beginbmatrix\n    -1  0  1\n    endbmatrix\nendaligned\n\nSee also:  KernelFactors.sobel and Kernel.sobel\n\nKernelFactors.ando3\n\nThe ando3 option [5] specifies the kernels\n\nbeginaligned\nmathbfH_x_1   =\n    beginbmatrix\n    -0112737  -0274526  -0112737 \n     0  0  0 \n     0112737  0274526  0112737\n    endbmatrix\n\nmathbfH_x_2   =\n    beginbmatrix\n    -0112737  0  0112737 \n    -0274526  0  0274526 \n    -0112737  0  0112737\n    endbmatrix \n  = beginbmatrix\n    -1 \n    0  \n    1\n    endbmatrix\n    beginbmatrix\n    0112737  0274526  0112737\n    endbmatrix\n\n  = beginbmatrix\n    0112737 \n    0274526  \n    0112737\n    endbmatrix\n    beginbmatrix\n    -1  0  1\n    endbmatrix\nendaligned\n\nSee also:  KernelFactors.ando3, and Kernel.ando3;  KernelFactors.ando4, and Kernel.ando4; KernelFactors.ando5, and Kernel.ando5\n\nKernelFactors.scharr\n\nThe scharr option [6] designates the kernels\n\nbeginaligned\nmathbfH_x_1  =\nfrac132\nbeginbmatrix\n-3  -10  -3 \n0  0  0 \n 3  10  3\nendbmatrix\n\nmathbfH_x_2  =\nfrac132\nbeginbmatrix\n-3  0  3 \n-10  0  10\n-3  0  3\nendbmatrix \n = frac132\nbeginbmatrix\n    -1 \n    0  \n    1\nendbmatrix\nbeginbmatrix\n    3  10  3\nendbmatrix\n\n = frac132\nbeginbmatrix\n    3 \n    10  \n    3\nendbmatrix\nbeginbmatrix\n    -1  0  1\nendbmatrix\nendaligned\n\nSee also:  KernelFactors.scharr and Kernel.scharr\n\nKernelFactors.bickley\n\nThe bickley option [7,8] designates the kernels\n\nbeginaligned\nmathbfH_x_1  = frac112\n    beginbmatrix\n        -1  -4  -1 \n         0  0  0 \n         1  4  1\n    endbmatrix\n\nmathbfH_x_2  = frac112\n    beginbmatrix\n        -1  0  1 \n        -4  0  4 \n        -1  0  1\n    endbmatrix \n = frac112\n    beginbmatrix\n        -1 \n        0  \n        1\n    endbmatrix\n    beginbmatrix\n        1  4  1\n    endbmatrix\n\n  = frac112\n   beginbmatrix\n        1 \n        4  \n        1\n   endbmatrix\n   beginbmatrix\n        -1  0  1\n   endbmatrix\nendaligned\n\nSee also:  KernelFactors.bickley and Kernel.bickley\n\nChoices for border\n\nAt the image edge, border is used to specify the padding which will be used to extrapolate the image beyond its original bounds. As an indicative example of each option the results of the padding are illustrated on an image consisting of a row of six pixels which are specified alphabetically: boxeda  b  c  d  e  f. We show the effects of padding only on the left and right border, but analogous consequences hold for the top and bottom border.\n\n\"replicate\"\n\nThe border pixels extend beyond the image boundaries.\n\nboxed\nbeginarraylcr\n  a a a a    a  b  c  d  e  f  f  f  f  f\nendarray\n\n\nSee also: Pad, padarray, Inner and NoPad\n\n\"circular\"\n\nThe border pixels wrap around. For instance, indexing beyond the left border returns values starting from the right border.\n\nboxed\nbeginarraylcr\n  c d e f    a  b  c  d  e  f  a  b  c  d\nendarray\n\n\nSee also: Pad, padarray, Inner and NoPad\n\n\"symmetric\"\n\nThe border pixels reflect relative to a position between pixels. That is, the border pixel is omitted when mirroring.\n\nboxed\nbeginarraylcr\n  e d c b    a  b  c  d  e  f  e  d  c  b\nendarray\n\n\nSee also: Pad, padarray, Inner and NoPad\n\n\"reflect\"\n\nThe border pixels reflect relative to the edge itself.\n\nboxed\nbeginarraylcr\n  d c b a    a  b  c  d  e  f  f  e  d  c\nendarray\n\n\nSee also: Pad, padarray, Inner and NoPad\n\nExample\n\nThis example compares the quality of the gradient estimation methods in terms of the accuracy with which the orientation of the gradient is estimated.\n\nusing Images\n\nvalues = LinRange(-1,1,128);\nw = 1.6*pi;\n\n# Define a function of a sinusoidal grating, f(x,y) = sin( (w*x)^2 + (w*y)^2 ),\n# together with its exact partial derivatives.\nI = [sin( (w*x)^2 + (w*y)^2 ) for y in values, x in values];\nIx = [2*w*x*cos( (w*x)^2 + (w*y)^2 ) for y in values, x in values];\nIy = [2*w*y*cos( (w*x)^2 + (w*y)^2 ) for y in values, x in values];\n\n# Determine the exact orientation of the gradients.\ndirection_true = atan.(Iy./Ix);\n\nfor kernelfunc in (KernelFactors.prewitt, KernelFactors.sobel,\n                   KernelFactors.ando3, KernelFactors.scharr,\n                   KernelFactors.bickley)\n\n    # Estimate the gradients and their orientations.\n    Gy, Gx = imgradients(I,kernelfunc, \"replicate\");\n    direction_estimated = atan.(Gy./Gx);\n\n    # Determine the mean absolute deviation between the estimated and true\n    # orientation. Ignore the values at the border since we expect them to be\n    # erroneous.\n    error = mean(abs.(direction_true[2:end-1,2:end-1] -\n                     direction_estimated[2:end-1,2:end-1]));\n\n    error = round(error, digits=5);\n    println(\"Using $kernelfunc results in a mean absolute deviation of $error\")\nend\n\n# output\n\nUsing ImageFiltering.KernelFactors.prewitt results in a mean absolute deviation of 0.01069\nUsing ImageFiltering.KernelFactors.sobel results in a mean absolute deviation of 0.00522\nUsing ImageFiltering.KernelFactors.ando3 results in a mean absolute deviation of 0.00365\nUsing ImageFiltering.KernelFactors.scharr results in a mean absolute deviation of 0.00126\nUsing ImageFiltering.KernelFactors.bickley results in a mean absolute deviation of 0.00038\n\nReferences\n\nB. Jahne, Digital Image Processing (5th ed.). Springer Publishing Company, Incorporated, 2005. 10.1007/3-540-27563-0\nM. Patra  and  M. Karttunen, \"Stencils with isotropic discretization error for differential operators,\" Numer. Methods Partial Differential Eq., vol. 22, pp. 936–953, 2006. doi:10.1002/num.20129\nJ. M. Prewitt, \"Object enhancement and extraction,\" Picture processing and Psychopictorics, vol. 10, no. 1, pp. 15–19, 1970.\nP.-E. Danielsson and O. Seger, \"Generalized and separable sobel operators,\" in  Machine Vision for Three-Dimensional Scenes,  H. Freeman, Ed.  Academic Press, 1990,  pp. 347–379. doi:10.1016/b978-0-12-266722-0.50016-6\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\nH. Scharr and  J. Weickert, \"An anisotropic diffusion algorithm with optimized rotation invariance,\" Mustererkennung 2000, pp. 460–467, 2000. doi:10.1007/978-3-642-59802-9_58\nA. Belyaev, \"Implicit image differentiation and filtering with applications to image sharpening,\" SIAM Journal on Imaging Sciences, vol. 6, no. 1, pp. 660–679, 2013. doi:10.1137/12087092x\nW. G. Bickley, \"Finite difference formulae for the square lattice,\" The Quarterly Journal of Mechanics and Applied Mathematics, vol. 1, no. 1, pp. 35–42, 1948.  doi:10.1093/qjmam/1.1.35\n\n\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Kernel","page":"References","title":"Kernel","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Kernel.sobel\nKernel.prewitt\nKernel.ando3\nKernel.ando4\nKernel.ando5\nKernel.gaussian\nKernel.DoG\nKernel.LoG\nKernel.gabor\nKernel.Laplacian\nKernel.bickley\nKernel.scharr","category":"page"},{"location":"function_reference/#ImageFiltering.Kernel.sobel","page":"References","title":"ImageFiltering.Kernel.sobel","text":"    diff1, diff2 = sobel()\n\nReturn 3 times 3 correlation kernels for two-dimensional gradient compution using the Sobel operator. The diff1 kernel computes the gradient along the y-axis (first dimension), and the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = sobel(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using the Sobel operator. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nP.-E. Danielsson and O. Seger, \"Generalized and separable sobel operators,\" in  Machine Vision for Three-Dimensional Scenes,  H. Freeman, Ed.  Academic Press, 1990,  pp. 347–379. doi:10.1016/b978-0-12-266722-0.50016-6\n\nSee also: KernelFactors.sobel, Kernel.prewitt, Kernel.ando3, Kernel.scharr, Kernel.bickley and imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.prewitt","page":"References","title":"ImageFiltering.Kernel.prewitt","text":"    diff1, diff2 = prewitt()\n\nReturn 3 times 3 correlation kernels for two-dimensional gradient compution using the Prewitt operator. The diff1 kernel computes the gradient along the y-axis (first dimension), and the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = prewitt(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using the Prewitt operator. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nJ. M. Prewitt, \"Object enhancement and extraction,\" Picture processing and Psychopictorics, vol. 10, no. 1, pp. 15–19, 1970.\n\nSee also: KernelFactors.prewitt, Kernel.sobel, Kernel.ando3, Kernel.scharr,Kernel.bickley and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.ando3","page":"References","title":"ImageFiltering.Kernel.ando3","text":"    diff1, diff2 = ando3()\n\nReturn 3 times 3 correlation kernels for two-dimensional gradient compution using Ando's \"optimal\" filters. The diff1 kernel computes the gradient along the y-axis (first dimension), and the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = ando3(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using Ando's \"optimal\" filters of size 3. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: KernelFactors.ando3, Kernel.ando4, Kernel.ando5 and  ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.ando4","page":"References","title":"ImageFiltering.Kernel.ando4","text":"    diff1, diff2 = ando4()\n\nReturn 4 times 4 correlation  kernels for two-dimensional gradient compution using Ando's \"optimal\" filters.  The diff1 kernel computes the gradient along the y-axis (first dimension), and  the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = ando4(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using Ando's \"optimal\" filters of size 4. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: KernelFactors.ando4, Kernel.ando3, Kernel.ando5 and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.ando5","page":"References","title":"ImageFiltering.Kernel.ando5","text":"    diff1, diff2 = ando5()\n\nReturn 5 times 5 correlation  kernels for two-dimensional gradient compution using Ando's \"optimal\" filters.  The diff1 kernel computes the gradient along the y-axis (first dimension), and  the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = ando5(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using Ando's \"optimal\" filters of size 5. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: KernelFactors.ando5, Kernel.ando3, Kernel.ando4 and  ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.gaussian","page":"References","title":"ImageFiltering.Kernel.gaussian","text":"gaussian((σ1, σ2, ...), [(l1, l2, ...)]) -> g\ngaussian(σ)                  -> g\n\nConstruct a multidimensional gaussian filter, with standard deviation σd along dimension d. Optionally provide the kernel length l, which must be a tuple of the same length.\n\nIf σ is supplied as a single number, a symmetric 2d kernel is constructed.\n\nSee also: KernelFactors.gaussian.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.DoG","page":"References","title":"ImageFiltering.Kernel.DoG","text":"DoG((σp1, σp2, ...), (σm1, σm2, ...), [l1, l2, ...]) -> k\nDoG((σ1, σ2, ...))                                   -> k\nDoG(σ::Real)                                         -> k\n\nConstruct a multidimensional difference-of-gaussian kernel k, equal to gaussian(σp, l)-gaussian(σm, l).  When only a single σ is supplied, the default is to choose σp = σ, σm = √2 σ. Optionally provide the kernel length l; the default is to extend by two max(σp,σm) in each direction from the center. l must be odd.\n\nIf σ is provided as a single number, a symmetric 2d DoG kernel is returned.\n\nSee also: KernelFactors.IIRGaussian.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.LoG","page":"References","title":"ImageFiltering.Kernel.LoG","text":"LoG((σ1, σ2, ...)) -> k\nLoG(σ)             -> k\n\nConstruct a Laplacian-of-Gaussian kernel k. σd is the gaussian width along dimension d.  If σ is supplied as a single number, a symmetric 2d kernel is returned.\n\nSee also: KernelFactors.IIRGaussian and Kernel.Laplacian.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.gabor","page":"References","title":"ImageFiltering.Kernel.gabor","text":"gabor(size_x,size_y,σ,θ,λ,γ,ψ) -> (k_real,k_complex)\n\nReturns a 2 Dimensional Complex Gabor kernel contained in a tuple where\n\nsize_x, size_y denote the size of the kernel\nσ denotes the standard deviation of the Gaussian envelope\nθ represents the orientation of the normal to the parallel stripes of a Gabor function\nλ represents the wavelength of the sinusoidal factor\nγ is the spatial aspect ratio, and specifies the ellipticity of the support of the Gabor function\nψ is the phase offset\n\n#Citation N. Petkov and P. Kruizinga, “Computational models of visual neurons specialised in the detection of periodic and aperiodic oriented visual stimuli: bar and grating cells,” Biological Cybernetics, vol. 76, no. 2, pp. 83–96, Feb. 1997. doi.org/10.1007/s004220050323\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.Laplacian","page":"References","title":"ImageFiltering.Kernel.Laplacian","text":"Laplacian((true,true,false,...))\nLaplacian(dims, N)\nLaplacian()\n\nLaplacian kernel in N dimensions, taking derivatives along the directions marked as true in the supplied tuple. Alternatively, one can pass dims, a listing of the dimensions for differentiation. (However, this variant is not inferrable.)\n\nLaplacian() is the 2d laplacian, equivalent to Laplacian((true,true)).\n\nThe kernel is represented as an opaque type, but you can use convert(AbstractArray, L) to convert it into array format.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Kernel.bickley","page":"References","title":"ImageFiltering.Kernel.bickley","text":"    diff1, diff2 = bickley()\n\nReturn 3 times 3 correlation kernels for two-dimensional gradient compution using the Bickley operator. The diff1 kernel computes the gradient along the y-axis (first dimension), and the diff2 kernel computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = bickley(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using the Bickley operator. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nW. G. Bickley, \"Finite difference formulae for the square lattice,\" The Quarterly Journal of Mechanics and Applied Mathematics, vol. 1, no. 1, pp. 35–42, 1948.  doi:10.1093/qjmam/1.1.35\n\nSee also: KernelFactors.bickley, Kernel.prewitt, Kernel.ando3,  Kernel.scharr and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.scharr","page":"References","title":"ImageFiltering.Kernel.scharr","text":"    diff1, diff2 = scharr()\n\nReturn 3 times 3 correlation kernels for two-dimensional gradient compution using the Scharr operator. The diff1 kernel computes the gradient along the y-axis (first dimension), and the diff2 kernel  computes the gradient along the x-axis (second dimension). diff1 == rotr90(diff2)\n\n    (diff,) = scharr(extended::NTuple{N,Bool}, d)\n\nReturn (a tuple of) the N-dimensional correlation kernel for gradient compution along the dimension d using the Scharr operator. If extended[dim] is false, diff will have size 1 along that dimension.\n\nCitation\n\nH. Scharr and  J. Weickert, \"An anisotropic diffusion algorithm with optimized rotation invariance,\" Mustererkennung 2000, pp. 460–467, 2000. doi:10.1007/978-3-642-59802-9_58\n\nSee also: KernelFactors.scharr, Kernel.prewitt, Kernel.ando3, Kernel.bickley and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#KernelFactors","page":"References","title":"KernelFactors","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"KernelFactors.sobel\nKernelFactors.prewitt\nKernelFactors.ando3\nKernelFactors.ando4\nKernelFactors.ando5\nKernelFactors.gaussian\nKernelFactors.IIRGaussian\nKernelFactors.TriggsSdika\nKernelFactors.bickley\nKernelFactors.scharr","category":"page"},{"location":"function_reference/#ImageFiltering.KernelFactors.sobel","page":"References","title":"ImageFiltering.KernelFactors.sobel","text":"    kern1, kern2 = sobel()\n\nReturn factored  Sobel filters for dimensions 1 and 2 of a two-dimensional image. Each is a 2-tuple of one-dimensional filters.\n\nCitation\n\nP.-E. Danielsson and O. Seger, \"Generalized and separable sobel operators,\" in  Machine Vision for Three-Dimensional Scenes,  H. Freeman, Ed.  Academic Press, 1990,  pp. 347–379. doi:10.1016/b978-0-12-266722-0.50016-6\n\nSee also: Kernel.sobel  and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = sobel(extended::NTuple{N,Bool}, d)\n\nReturn a factored Sobel filter for computing the gradient in N dimensions along axis d. If extended[dim] is false, kern will have size 1 along that dimension.\n\nSee also: Kernel.sobel and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.prewitt","page":"References","title":"ImageFiltering.KernelFactors.prewitt","text":"    kern1, kern2 = prewitt()\n\nReturn factored Prewitt filters for dimensions 1 and 2 of your image. Each is a 2-tuple of one-dimensional filters.\n\nCitation\n\nJ. M. Prewitt, \"Object enhancement and extraction,\" Picture processing and Psychopictorics, vol. 10, no. 1, pp. 15–19, 1970.\n\nSee also: Kernel.prewitt and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = prewitt(extended::NTuple{N,Bool}, d)\n\nReturn a factored Prewitt filter for computing the gradient in N dimensions along axis d. If extended[dim] is false, kern will have size 1 along that dimension.\n\nSee also: Kernel.prewitt and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.ando3","page":"References","title":"ImageFiltering.KernelFactors.ando3","text":"    kern1, kern2 = ando3()\n\nReturn a factored form of Ando's \"optimal\" 3 times 3 gradient filters for dimensions 1 and 2 of your image.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: Kernel.ando3,KernelFactors.ando4, KernelFactors.ando5 and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = ando3(extended::NTuple{N,Bool}, d)\n\nReturn a factored Ando filter (size 3) for computing the gradient in N dimensions along axis d.  If extended[dim] is false, kern will have size 1 along that dimension.\n\nSee also: KernelFactors.ando4, KernelFactors.ando5 and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.ando4","page":"References","title":"ImageFiltering.KernelFactors.ando4","text":"    kern1, kern2 = ando4()\n\nReturn separable approximations of Ando's \"optimal\" 4x4 filters for dimensions 1 and 2 of your image.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: Kernel.ando4 and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = ando4(extended::NTuple{N,Bool}, d)\n\nReturn a factored Ando filter (size 4) for computing the gradient in N dimensions along axis d.  If extended[dim] is false, kern will have size 1 along that dimension.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: Kernel.ando4 and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.ando5","page":"References","title":"ImageFiltering.KernelFactors.ando5","text":"    kern1, kern2 = ando5()\n\nReturn a separable approximations of Ando's \"optimal\" 5x5 gradient filters for dimensions 1 and 2 of your image.\n\nCitation\n\nS. Ando, \"Consistent gradient operators,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.3, pp. 252–265, 2000. doi:10.1109/34.841757\n\nSee also: Kernel.ando5 and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = ando5(extended::NTuple{N,Bool}, d)\n\nReturn a factored Ando filter (size 5) for computing the gradient in N dimensions along axis d.  If extended[dim] is false, kern will have size 1 along that dimension.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.gaussian","page":"References","title":"ImageFiltering.KernelFactors.gaussian","text":"gaussian(σ::Real, [l]) -> g\n\nConstruct a 1d gaussian kernel g with standard deviation σ, optionally providing the kernel length l. The default is to extend by two σ in each direction from the center. l must be odd.\n\n\n\n\n\ngaussian((σ1, σ2, ...), [l]) -> (g1, g2, ...)\n\nConstruct a multidimensional gaussian filter as a product of single-dimension factors, with standard deviation σd along dimension d. Optionally provide the kernel length l, which must be a tuple of the same length.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.IIRGaussian","page":"References","title":"ImageFiltering.KernelFactors.IIRGaussian","text":"IIRGaussian([T], σ; emit_warning::Bool=true)\n\nConstruct an infinite impulse response (IIR) approximation to a Gaussian of standard deviation σ. σ may either be a single real number or a tuple of numbers; in the latter case, a tuple of such filters will be created, each for filtering a different dimension of an array.\n\nOptionally specify the type T for the filter coefficients; if not supplied, it will match σ (unless σ is not floating-point, in which case Float64 will be chosen).\n\nCitation\n\nI. T. Young, L. J. van Vliet, and M. van Ginkel, \"Recursive Gabor Filtering\". IEEE Trans. Sig. Proc., 50: 2798-2805 (2002).\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.TriggsSdika","page":"References","title":"ImageFiltering.KernelFactors.TriggsSdika","text":"TriggsSdika(a, b, scale, M)\n\nDefines a kernel for one-dimensional infinite impulse response (IIR) filtering. a is a \"forward\" filter, b a \"backward\" filter, M is a matrix for matching boundary conditions at the right edge, and scale is a constant scaling applied to each element at the conclusion of filtering.\n\nCitation\n\nB. Triggs and M. Sdika, \"Boundary conditions for Young-van Vliet recursive filtering\". IEEE Trans. on Sig. Proc. 54: 2365-2367 (2006).\n\n\n\n\n\nTriggsSdika(ab, scale)\n\nCreate a symmetric Triggs-Sdika filter (with a = b = ab). M is calculated for you. Only length 3 filters are currently supported.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.KernelFactors.bickley","page":"References","title":"ImageFiltering.KernelFactors.bickley","text":"    kern1, kern2 = bickley()\n\nReturn factored Bickley filters for dimensions 1 and 2 of your image.  Each is a 2-tuple of one-dimensional filters.\n\nCitation\n\nW. G. Bickley, \"Finite difference formulae for the square lattice,\" The Quarterly Journal of Mechanics and Applied Mathematics, vol. 1, no. 1, pp. 35–42, 1948.  doi:10.1093/qjmam/1.1.35\n\nSee also: Kernel.bickley and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = bickley(extended::NTuple{N,Bool}, d)\n\nReturn a factored Bickley filter for computing the gradient in N dimensions along axis d. If extended[dim] is false, kern will have size 1 along that dimension.\n\nSee also: Kernel.bickley and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.scharr","page":"References","title":"ImageFiltering.KernelFactors.scharr","text":"    kern1, kern2 = scharr()\n\nReturn factored Scharr filters for dimensions 1 and 2 of your image.  Each is a 2-tuple of one-dimensional filters.\n\nCitation\n\nH. Scharr and  J. Weickert, \"An anisotropic diffusion algorithm with optimized rotation invariance,\" Mustererkennung 2000, pp. 460–467, 2000. doi:10.1007/978-3-642-59802-9_58\n\nSee also: Kernel.scharr and ImageFiltering.imgradients.\n\n\n\n\n\n    kern = scharr(extended::NTuple{N,Bool}, d)\n\nReturn a factored Scharr filter for computing the gradient in N dimensions along axis d. If extended[dim] is false, kern will have size 1 along that dimension.\n\nSee also: Kernel.scharr and ImageFiltering.imgradients.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Kernel-utilities","page":"References","title":"Kernel utilities","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"centered\nkernelfactors\nreflect","category":"page"},{"location":"function_reference/#ImageFiltering.centered","page":"References","title":"ImageFiltering.centered","text":"shiftedkernel = centered(kernel)\n\nShift the origin-of-coordinates to the center of kernel. The center-element of kernel will be accessed by shiftedkernel[0, 0, ...].\n\nThis function makes it easy to supply kernels using regular Arrays, and provides compatibility with other languages that do not support arbitrary axes.\n\nSee also: imfilter.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.KernelFactors.kernelfactors","page":"References","title":"ImageFiltering.KernelFactors.kernelfactors","text":"kernelfactors(factors::Tuple)\n\nPrepare a factored kernel for filtering. If passed a 2-tuple of vectors of lengths m and n, this will return a 2-tuple of ReshapedVectors that are effectively of sizes m×1 and 1×n. In general, each successive factor will be reshaped to extend along the corresponding dimension.\n\nIf passed a tuple of general arrays, it is assumed that each is shaped appropriately along its \"leading\" dimensions; the dimensionality of each is \"extended\" to N = length(factors), appending 1s to the size as needed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Kernel.reflect","page":"References","title":"ImageFiltering.Kernel.reflect","text":"reflect(kernel) --> reflectedkernel\n\nCompute the pointwise reflection around 0, 0, ... of the kernel kernel.  Using imfilter with a reflectedkernel performs convolution, rather than correlation, with respect to the original kernel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Boundaries-and-padding","page":"References","title":"Boundaries and padding","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"padarray\nPad\nFill\nInner\nNA\nNoPad","category":"page"},{"location":"function_reference/#ImageFiltering.padarray","page":"References","title":"ImageFiltering.padarray","text":"    padarray([T], img, border) --> imgpadded\n\nGenerate a padded image from an array img and a specification border of the boundary conditions and amount of padding to add.\n\nOutput\n\nAn expansion of the input image in which additional pixels are derived from the border of the input image using the extrapolation scheme specified by border.\n\nDetails\n\nThe function supports one, two or multi-dimensional images. You can specify the element type T of the output image.\n\nOptions\n\nValid border options are described below.\n\nPad\n\nThe type Pad designates the form of padding which should be used to extrapolate pixels beyond the boundary of an image. Instances must set style, a Symbol specifying the boundary conditions of the image.\n\nSymbol must be on one of:\n\n:replicate (repeat edge values to infinity),\n:circular (image edges \"wrap around\"),\n:symmetric (the image reflects relative to a position between pixels),\n:reflect (the image reflects relative to the edge itself).\n\nRefer to the documentation of Pad for more details and examples for each option.\n\nFill\n\nThe type Fill designates a particular value which will be used to extrapolate pixels beyond the boundary of an image. Refer to the documentation of Fill for more details and illustrations.\n\n2D Examples\n\nEach example is based on the input array\n\nmathbfA =\nboxed\nbeginmatrix\n 1   2    3    4  5   6 \n 2   4    6    8  10  12 \n 3   6    9   12  15  18 \n 4   8   12   16  20  24 \n 5   10  15   20  25  30 \n 6   12  18   24  30  36\n endmatrix\n\nExamples with Pad\n\nThe command padarray(A, Pad(:replicate,4,4)) yields\n\nboxed\nbeginarrayccccccccccccc\n1  1  1  1          1             2             3             4             5             6     6    6    6    6 \n1  1  1  1          1             2             3             4             5             6     6    6    6    6 \n1  1  1  1          1             2             3             4             5             6     6    6    6    6 \n1  1  1  1          1             2             3             4             5             6     6    6    6    6 \n1  1  1  1   boxed1     boxed2     boxed3     boxed4     boxed5     boxed6    6    6    6    6 \n2  2  2  2   boxed2     boxed4     boxed6     boxed8    boxed10    boxed12   12   12   12   12 \n3  3  3  3   boxed3     boxed6     boxed9    boxed12    boxed15    boxed18   18   18   18   18 \n4  4  4  4   boxed4     boxed8    boxed12    boxed16    boxed20    boxed24   24   24   24   24 \n5  5  5  5   boxed5    boxed10    boxed15    boxed20    boxed25    boxed30   30   30   30   30 \n6  6  6  6   boxed6    boxed12    boxed18    boxed24    boxed30    boxed36   36   36   36   36 \n6  6  6  6          6            12            18            24            30            36    36   36   36   36 \n6  6  6  6          6            12            18            24            30            36    36   36   36   36 \n6  6  6  6          6            12            18            24            30            36    36   36   36   36 \n6  6  6  6          6            12            18            24            30            36    36   36   36   36\n endarray\n\n\nThe command padarray(A, Pad(:circular,4,4)) yields\n\nboxed\nbeginarrayccccccccccccc\n9   12  15  18          3           6            9            12            15           18   3   6   9  12 \n12  16  20  24          4           8           12            16            20           24   4   8  12  16 \n15  20  25  30          5          10           15            20            25           30   5  10  15  20 \n18  24  30  36          6          12           18            24            30           36   6  12  18  24 \n3    4   5   6   boxed1   boxed2    boxed3    boxed4    boxed5     boxed6   1   2   3   4 \n6    8  10  12   boxed2   boxed4    boxed6    boxed8    boxed10    boxed12  2   4   6   8 \n9   12  15  18   boxed3   boxed6    boxed9    boxed12   boxed15    boxed18  3   6   9  12 \n12  16  20  24   boxed4   boxed8    boxed12   boxed16   boxed20    boxed24  4   8  12  16 \n15  20  25  30   boxed5   boxed10   boxed15   boxed20   boxed25    boxed30  5  10  15  20 \n18  24  30  36   boxed6   boxed12   boxed18   boxed24   boxed30    boxed36  6  12  18  24 \n3    4   5   6          1            2            3            4             5            6   1   2   3   4 \n6    8  10  12          2            4            6            8            10           12   2   4   6   8 \n9   12  15  18          3            6            9           12            15           18   3   6   9  12 \n12  16  20  24          4            8           12           16            20           24   4   8  12  16\nendarray\n\n\nThe command padarray(A, Pad(:symmetric,4,4)) yields\n\nboxed\nbeginarrayccccccccccccc\n16  12   8  4          4            8           12            16           20          24   24  20  16  12 \n12   9   6  3          3            6           9             12           15          18   18  15  12   9 \n 8   6   4  2          2            4           6             8            10          12   12  10   8   6 \n 4   3   2  1          1            2           3             4            5           6     6   5   4   3 \n 4   3   2  1   boxed1    boxed2   boxed3     boxed4   boxed5    boxed6    6   5   4   3 \n 8   6   4  2   boxed2    boxed4   boxed6     boxed8   boxed10   boxed12  12  10   8   6 \n12   9   6  3   boxed3    boxed6   boxed9    boxed12   boxed15   boxed18  18  15  12   9 \n16  12   8  4   boxed4    boxed8   boxed12   boxed16   boxed20   boxed24  24  20  16  12 \n20  15  10  5   boxed5   boxed10   boxed15   boxed20   boxed25   boxed30  30  25  20  15 \n24  18  12  6   boxed6   boxed12   boxed18   boxed24   boxed30   boxed36  36  30  24  18 \n24  18  12  6          6           12           18           24           30           36   36  30  24  18 \n20  15  10  5          5           10           15           20           25           30   30  25  20  15 \n16  12   8  4          4            8           12           16           20           24   24  20  16  12 \n12   9   6  3          3            6            9           12           15           18   18  15  12   9\nendarray\n\n\nThe command padarray(A, Pad(:reflect,4,4)) yields\n\nboxed\nbeginarrayccccccccccccc\n25  20  15  10          5           10           15            20            25           30   25  20  15  10 \n20  16  12   8          4           8            12            16            20           24   20  16  12   8 \n15  12   9   6          3           6             9            12            15           18   15  12   9   6 \n10   8   6   4          2           4             6            8             10           12   10   8   6   4 \n5    4   3   2   boxed1   boxed2     boxed3    boxed4     boxed5    boxed6    5   4   3   2 \n10   8   6   4   boxed2   boxed4     boxed6    boxed8     boxed10   boxed12  10   8   6   4 \n15  12   9   6   boxed3   boxed6     boxed9    boxed12    boxed15   boxed18  15  12   9   6 \n20  16  12   8   boxed4   boxed8     boxed12   boxed16    boxed20   boxed24  20  16  12   8 \n25  20  15  10   boxed5   boxed10    boxed15   boxed20    boxed25   boxed30  25  20  15  10 \n30  24  18  12   boxed6   boxed12    boxed18   boxed24    boxed30   boxed36  30  24  18  12 \n25  20  15  10          5           10            15           20            25           30   25  20  15  10 \n20  16  12   8          4           8             12           16            20           24   20  16  12   8 \n15  12   9   6          3           6              9           12            15           18   15  12   9   6 \n10   8   6   4          2           4              6            8            10           12   10   8   6   4\nendarray\n\n\nExamples with Fill\n\nThe command padarray(A, Fill(0,(4,4),(4,4))) yields\n\nboxed\nbeginarrayccccccccccccc\n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0   boxed1   boxed2    boxed3    boxed4    boxed5     boxed6   0  0  0  0 \n0  0  0  0   boxed2   boxed4    boxed6    boxed8    boxed10    boxed12  0  0  0  0 \n0  0  0  0   boxed3   boxed6    boxed9    boxed12   boxed15    boxed18  0  0  0  0 \n0  0  0  0   boxed4   boxed8    boxed12   boxed16   boxed20    boxed24  0  0  0  0 \n0  0  0  0   boxed5   boxed10   boxed15   boxed20   boxed25    boxed30  0  0  0  0 \n0  0  0  0   boxed6   boxed12   boxed18   boxed24   boxed30    boxed36  0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0 \n0  0  0  0          0           0            0            0            0             0    0  0  0  0\nendarray\n\n\n3D Examples\n\nEach example is based on a multi-dimensional array mathsfA inmathbbR^2 times 2 times 2 given by\n\nmathsfA(1) =\nboxed\nbeginarraycc\n1  2 \n3  4\nendarray\nquad\ntextand\nquad\nmathsfA(2) =\nboxed\nbeginarraycc\n5  6 \n7  8\nendarray\n\nNote that each example will yield a new multi-dimensional array mathsfA in mathbbR^4 times 4 times 4 of type OffsetArray, where prepended dimensions may be negative or start from zero.\n\nExamples with Pad\n\nThe command padarray(A,Pad(:replicate,1,1,1)) yields\n\nbeginaligned\nmathsfA(0)  =\nboxed\nbeginarraycccc\n1  1  2  2 \n1  1  2  2 \n3  3  4  4 \n3  3  4  4\nendarray\n\nmathsfA(1)  =\nboxed\nbeginarraycccc\n1          1           2   2 \n1   boxed1   boxed2  2 \n3   boxed3   boxed4  4 \n3          3           4   4\nendarray \nmathsfA(2)  =\nboxed\nbeginarraycccc\n5          5           6   6 \n5   boxed5   boxed6  6 \n7   boxed7   boxed8  8 \n7          7           8   8\nendarray\n\nmathsfA(3)  =\nboxed\nbeginarraycccc\n5  5  6  6 \n5  5  6  6 \n7  7  8  8 \n7  7  8  8\nendarray\nendaligned\n\n\nThe command padarray(A,Pad(:circular,1,1,1)) yields\n\nbeginaligned\nmathsfA(0)  =\nboxed\nbeginarraycccc\n8  7  8  7 \n6  5  6  5 \n8  7  8  7 \n6  5  6  5\nendarray\n\nmathsfA(1)  =\nboxed\nbeginarraycccc\n4          3           4   3 \n2   boxed1   boxed2  1 \n4   boxed3   boxed4  3 \n2          1           2   1\nendarray \nmathsfA(2)  =\nboxed\nbeginarraycccc\n8          7           8   7 \n6   boxed5   boxed6  5 \n8   boxed7   boxed8  7 \n6          5           6   5\nendarray\n\nmathsfA(3)  =\nboxed\nbeginarraycccc\n4  3  4  3 \n2  1  2  1 \n4  3  4  3 \n2  1  2  1\nendarray\nendaligned\n\n\nThe command padarray(A,Pad(:symmetric,1,1,1)) yields\n\nbeginaligned\nmathsfA(0)  =\nboxed\nbeginarraycccc\n1  1  2  2 \n1  1  2  2 \n3  3  4  4 \n3  3  4  4\nendarray\n\nmathsfA(1)  =\nboxed\nbeginarraycccc\n1          1           2   2 \n1   boxed1   boxed2  2 \n2   boxed3   boxed4  4 \n2          3           4   4\nendarray \nmathsfA(2)  =\nboxed\nbeginarraycccc\n5          5           6   6 \n5   boxed5   boxed6  6 \n7   boxed7   boxed8  8 \n7          7           8   8\nendarray\n\nmathsfA(3)  =\nboxed\nbeginarraycccc\n5  5  6  6 \n5  5  6  6 \n7  7  8  8 \n7  7  8  8\nendarray\nendaligned\n\n\nThe command padarray(A,Pad(:reflect,1,1,1)) yields\n\nbeginaligned\nmathsfA(0)  =\nboxed\nbeginarraycccc\n8  7  8  7 \n6  5  6  5 \n8  7  8  7 \n6  5  6  5\nendarray\n\nmathsfA(1)  =\nboxed\nbeginarraycccc\n4          3           4   3 \n2   boxed1   boxed2  1 \n4   boxed3   boxed4  3 \n2          1           2   1\nendarray \nmathsfA(2)  =\nboxed\nbeginarraycccc\n8          7           8   7 \n6   boxed5   boxed6  5 \n8   boxed7   boxed8  7 \n6          5           6   5\nendarray\n\nmathsfA(3)  =\nboxed\nbeginarraycccc\n4  3  4  3 \n2  1  2  1 \n4  3  4  3 \n2  1  2  1\nendarray\nendaligned\n\n\nExamples with Fill\n\nThe command padarray(A,Fill(0,(1,1,1))) yields\n\nbeginaligned\nmathsfA(0)  =\nboxed\nbeginarraycccc\n0  0  0  0 \n0  0  0  0 \n0  0  0  0 \n0  0  0  0\nendarray\n\nmathsfA(1)  =\nboxed\nbeginarraycccc\n0          0           0   0 \n0   boxed1   boxed2  0 \n0   boxed3   boxed4  0 \n0          0           0   0\nendarray \nmathsfA(2)  =\nboxed\nbeginarraycccc\n0          0           0   0 \n0   boxed5   boxed6  0 \n0   boxed7   boxed8  0 \n0          0           0   0\nendarray\n\nmathsfA(3)  =\nboxed\nbeginarraycccc\n0  0  0  0 \n0  0  0  0 \n0  0  0  0 \n0  0  0  0\nendarray\nendaligned\n\n\n\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFiltering.Pad","page":"References","title":"ImageFiltering.Pad","text":"    struct Pad{N} <: AbstractBorder\n        style::Symbol\n        lo::Dims{N}    # number to extend by on the lower edge for each dimension\n        hi::Dims{N}    # number to extend by on the upper edge for each dimension\n    end\n\nPad is a type that designates the form of padding which should be used to extrapolate pixels beyond the boundary of an image. Instances must set style, a Symbol specifying the boundary conditions of the image.\n\nOutput\n\nThe type Pad specifying how the boundary of an image should be padded.\n\nDetails\n\nWhen representing a spatial two-dimensional image filtering operation as a discrete convolution between the image and a D times D filter, the results are undefined for pixels closer than D pixels from the border of the image. To define the operation near and at the border, one needs a scheme for extrapolating pixels beyond the edge. The Pad type allows one to specify the necessary extrapolation scheme.\n\nThe type facilitates the padding of one, two or multi-dimensional images.\n\nYou can specify a different amount of padding at the lower and upper borders of each dimension of the image (top, left, bottom and right in two dimensions).\n\nOptions\n\nSome valid style options are described below. As an indicative example of each option the results of the padding are illustrated on an image consisting of a row of six pixels which are specified alphabetically: boxeda  b  c d  e  f. We show the effects of padding only on the left and right border, but analogous consequences hold for the top and bottom border.\n\n:replicate (Default)\n\nThe border pixels extend beyond the image boundaries.\n\nboxed\nbeginarraylcr\n  a a a a    a  b  c  d  e  f  f  f  f  f\nendarray\n\n\nSee also: Fill, padarray, Inner and NoPad\n\n:circular\n\nThe border pixels wrap around. For instance, indexing beyond the left border returns values starting from the right border.\n\nboxed\nbeginarraylcr\n  c d e f    a  b  c  d  e  f  a  b  c  d\nendarray\n\n\nSee also: Fill, padarray, Inner and NoPad\n\n:symmetric\n\nThe border pixels reflect relative to a position between pixels. That is, the border pixel is omitted when mirroring.\n\nboxed\nbeginarraylcr\n  e d c b    a  b  c  d  e  f  e  d  c  b\nendarray\n\n\nSee also: Fill,padarray, Inner and NoPad\n\n:reflect\n\nThe border pixels reflect relative to the edge itself.\n\nboxed\nbeginarraylcr\n  d c b a    a  b  c  d  e  f  f  e  d  c\nendarray\n\n\nSee also: Fill,padarray, Inner and NoPad\n\n\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Fill","page":"References","title":"ImageFiltering.Fill","text":"    struct Fill{T,N} <: AbstractBorder\n        value::T\n        lo::Dims{N}\n        hi::Dims{N}\n    end\n\nFill is a type that designates a particular value which will be used to extrapolate pixels beyond the boundary of an image.\n\nOutput\n\nThe type Fill specifying the value with which the boundary of the image should be padded.\n\nDetails\n\nWhen representing a two-dimensional spatial image filtering operation as a discrete convolution between an image and a D times D filter, the results are undefined for pixels closer than D pixels from the border of the image. To define the operation near and at the border, one needs a scheme for extrapolating pixels beyond the edge. The Fill type allows one to specify a particular value which will be used in the extrapolation. For more elaborate extrapolation schemes refer to the documentation of  Pad.\n\nThe type facilitates the padding of one, two or multi-dimensional images.\n\nYou can specify a different amount of padding at the lower and upper borders of each dimension of the image (top, left, bottom and right in two dimensions).\n\nExample\n\nAs an indicative illustration consider an image consisting of a row of six pixels which are specified alphabetically: boxeda  b  c  d  e  f. We show the effects of padding with a constant value m only on the left and right border, but analogous consequences hold for the top and bottom border.\n\nboxed\nbeginarraylcr\n  m m m m    a  b  c  d  e  f  m  m  m  m\nendarray\n\n\nSee also: Pad, padarray, Inner and NoPad\n\n\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Inner","page":"References","title":"ImageFiltering.Inner","text":"Inner()\nInner(lo, hi)\n\nIndicate that edges are to be discarded in filtering, only the interior of the result is to be returned.\n\nExample:\n\nimfilter(img, kernel, Inner())\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.NA","page":"References","title":"ImageFiltering.NA","text":"NA(na=isnan)\n\nChoose filtering using \"NA\" (Not Available) boundary conditions. This is most appropriate for filters that have only positive weights, such as blurring filters. Effectively, the output value is normalized in the following way:\n\n          filtered array with Fill(0) boundary conditions\noutput =  -----------------------------------------------\n          filtered 1     with Fill(0) boundary conditions\n\nArray elements for which na returns true are also considered outside array boundaries.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.NoPad","page":"References","title":"ImageFiltering.NoPad","text":"NoPad()\nNoPad(border)\n\nIndicates that no padding should be applied to the input array, or that you have already pre-padded the input image. Passing a border object allows you to preserve \"memory\" of a border choice; it can be retrieved by indexing with [].\n\nExample\n\nThe commands\n\nnp = NoPad(Pad(:replicate))\nimfilter!(out, img, kernel, np)\n\nrun filtering directly, skipping any padding steps.  Every entry of out must be computable using in-bounds operations on img and kernel.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Algorithms","page":"References","title":"Algorithms","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Algorithm.FIR\nAlgorithm.FFT\nAlgorithm.IIR\nAlgorithm.Mixed","category":"page"},{"location":"function_reference/#ImageFiltering.Algorithm.FIR","page":"References","title":"ImageFiltering.Algorithm.FIR","text":"Filter using a direct algorithm\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Algorithm.FFT","page":"References","title":"ImageFiltering.Algorithm.FFT","text":"Filter using the Fast Fourier Transform\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Algorithm.IIR","page":"References","title":"ImageFiltering.Algorithm.IIR","text":"Filter with an Infinite Impulse Response filter\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFiltering.Algorithm.Mixed","page":"References","title":"ImageFiltering.Algorithm.Mixed","text":"Filter with a cascade of mixed types (IIR, FIR)\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Internal-machinery","page":"References","title":"Internal machinery","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"KernelFactors.ReshapedOneD","category":"page"},{"location":"function_reference/#ImageFiltering.KernelFactors.ReshapedOneD","page":"References","title":"ImageFiltering.KernelFactors.ReshapedOneD","text":"ReshapedOneD{N,Npre}(data)\n\nReturn an object of dimensionality N, where data must have dimensionality 1. The axes are 0:0 for the first Npre dimensions, have the axes of data for dimension Npre+1, and are 0:0 for the remaining dimensions.\n\ndata must support eltype and ndims, but does not have to be an AbstractArray.\n\nReshapedOneDs allow one to specify a \"filtering dimension\" for a 1-dimensional filter.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Nonlinear-filtering-and-transformation","page":"References","title":"Nonlinear filtering and transformation","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"mapwindow\nimROF","category":"page"},{"location":"function_reference/#ImageFiltering.MapWindow.mapwindow","page":"References","title":"ImageFiltering.MapWindow.mapwindow","text":"mapwindow(f, img, window; [border=\"replicate\"], [indices=axes(img)]) -> imgf\n\nApply f to sliding windows of img, with window size or axes specified by window. For example, mapwindow(median!, img, window) returns an Array of values similar to img (median-filtered, of course), whereas mapwindow(extrema, img, window) returns an Array of (min,max) tuples over a window of size window centered on each point of img.\n\nThe function f receives a buffer buf for the window of data surrounding the current point. If window is specified as a Dims-tuple (tuple-of-integers), then all the integers must be odd and the window is centered around the current image point. For example, if window=(3,3), then f will receive an Array buf corresponding to offsets (-1:1, -1:1) from the imgf[i,j] for which this is currently being computed. Alternatively, window can be a tuple of AbstractUnitRanges, in which case the specified ranges are used for buf; this allows you to use asymmetric windows if needed.\n\nborder specifies how the edges of img should be handled; see imfilter for details.\n\nFinally indices allows to omit unnecessary computations, if you want to do things like mapwindow on a subimage, or a strided variant of mapwindow. It works as follows:\n\nmapwindow(f, img, window, indices=(2:5, 1:2:7)) == mapwindow(f,img,window)[2:5, 1:2:7]\n\nExcept more efficiently because it omits computation of the unused values.\n\nBecause the data in the buffer buf that is received by f is copied from img, and the buffer's memory is reused, f should not return references to buf. This\n\nf = buf->copy(buf) # as opposed to f = buf->buf\nmapwindow(f, img, window, indices=(2:5, 1:2:7))\n\nwould work as expected.\n\nFor functions that can only take AbstractVector inputs, you might have to first specialize default_shape:\n\nf = v->quantile(v, 0.75)\nImageFiltering.MapWindow.default_shape(::typeof(f)) = vec\n\nand then mapwindow(f, img, (m,n)) should filter at the 75th quantile.\n\nSee also: imfilter.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.imROF","page":"References","title":"Images.imROF","text":"imgr = imROF(img, λ, iterations)\n\nPerform Rudin-Osher-Fatemi (ROF) filtering, more commonly known as Total Variation (TV) denoising or TV regularization. λ is the regularization coefficient for the derivative, and iterations is the number of relaxation iterations taken. 2d only.\n\nSee https://en.wikipedia.org/wiki/Totalvariationdenoising and Chambolle, A. (2004). \"An algorithm for total variation minimization and applications\".     Journal of Mathematical Imaging and Vision. 20: 89–97\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Edge-detection","page":"References","title":"Edge detection","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"magnitude\nphase\norientation\nmagnitude_phase\nimedge\nthin_edges\ncanny\nPercentile","category":"page"},{"location":"function_reference/#Images.magnitude","page":"References","title":"Images.magnitude","text":"m = magnitude(grad_x, grad_y)\n\nCalculates the magnitude of the gradient images given by grad_x and grad_y. Equivalent to sqrt(grad_x.^2 + grad_y.^2).\n\nReturns a magnitude image the same size as grad_x and grad_y.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.phase","page":"References","title":"Images.phase","text":"phase(grad_x, grad_y) -> p\n\nCalculate the rotation angle of the gradient given by grad_x and grad_y. Equivalent to atan(-grad_y, grad_x), except that when both grad_x and grad_y are effectively zero, the corresponding angle is set to zero.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.orientation","page":"References","title":"Images.orientation","text":"orientation(grad_x, grad_y) -> orient\n\nCalculate the orientation angle of the strongest edge from gradient images given by grad_x and grad_y.  Equivalent to atan(grad_x, grad_y).  When both grad_x and grad_y are effectively zero, the corresponding angle is set to zero.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.magnitude_phase","page":"References","title":"Images.magnitude_phase","text":"magnitude_phase(grad_x, grad_y) -> m, p\n\nConvenience function for calculating the magnitude and phase of the gradient images given in grad_x and grad_y.  Returns a tuple containing the magnitude and phase images.  See magnitude and phase for details.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.imedge","page":"References","title":"Images.imedge","text":"grad_y, grad_x, mag, orient = imedge(img, kernelfun=KernelFactors.ando3, border=\"replicate\")\n\nEdge-detection filtering. kernelfun is a valid kernel function for imgradients, defaulting to KernelFactors.ando3. border is any of the boundary conditions specified in padarray.\n\nReturns a tuple (grad_y, grad_x, mag, orient), which are the horizontal gradient, vertical gradient, and the magnitude and orientation of the strongest edge, respectively.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.thin_edges","page":"References","title":"Images.thin_edges","text":"thinned = thin_edges(img, gradientangle, [border])\nthinned, subpix = thin_edges_subpix(img, gradientangle, [border])\nthinned, subpix = thin_edges_nonmaxsup(img, gradientangle, [border]; [radius::Float64=1.35], [theta=pi/180])\nthinned, subpix = thin_edges_nonmaxsup_subpix(img, gradientangle, [border]; [radius::Float64=1.35], [theta=pi/180])\n\nEdge thinning for 2D edge images.  Currently the only algorithm available is non-maximal suppression, which takes an edge image and its gradient angle, and checks each edge point for local maximality in the direction of the gradient. The returned image is non-zero only at maximal edge locations.\n\nborder is any of the boundary conditions specified in padarray.\n\nIn addition to the maximal edge image, the _subpix versions of these functions also return an estimate of the subpixel location of each local maxima, as a 2D array or image of Graphics.Point objects.  Additionally, each local maxima is adjusted to the estimated value at the subpixel location.\n\nCurrently, the _nonmaxsup functions are identical to the first two function calls, except that they also accept additional keyword arguments.  radius indicates the step size to use when searching in the direction of the gradient; values between 1.2 and 1.5 are suggested (default 1.35).  theta indicates the step size to use when discretizing angles in the gradientangle image, in radians (default: 1 degree in radians = pi/180).\n\nExample:\n\ng = rgb2gray(rgb_image)\ngx, gy = imgradients(g)\nmag, grad_angle = magnitude_phase(gx,gy)\nmag[mag .< 0.5] = 0.0  # Threshold magnitude image\nthinned, subpix =  thin_edges_subpix(mag, grad_angle)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.canny","page":"References","title":"Images.canny","text":"canny_edges = canny(img, (upper, lower), sigma=1.4)\n\nPerforms Canny Edge Detection on the input image.\n\nParameters :\n\n(upper, lower) :  Bounds for hysteresis thresholding   sigma :           Specifies the standard deviation of the gaussian filter\n\nExample\n\nimgedg = canny(img, (Percentile(80), Percentile(20)))\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.Percentile","page":"References","title":"Images.Percentile","text":"Percentile(x)\n\nIndicate that x should be interpreted as a percentile rather than an absolute value. For example,\n\ncanny(img, 1.4, (80, 20)) uses absolute thresholds on the edge magnitude image\ncanny(img, 1.4, (Percentile(80), Percentile(20))) uses percentiles of the edge magnitude image as threshold\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Corner-Detection","page":"References","title":"Corner Detection","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"imcorner\nharris\nshi_tomasi\nkitchen_rosenfeld\nfastcorners","category":"page"},{"location":"function_reference/#Images.imcorner","page":"References","title":"Images.imcorner","text":"corners = imcorner(img; [method])\ncorners = imcorner(img, threshold, percentile; [method])\n\nPerforms corner detection using one of the following methods -\n\n1. harris\n2. shi_tomasi\n3. kitchen_rosenfeld\n\nThe parameters of the individual methods are described in their documentation. The maxima values of the resultant responses are taken as corners. If a threshold is specified, the values of the responses are thresholded to give the corner pixels. The threshold is assumed to be a percentile value unless percentile is set to false.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.harris","page":"References","title":"Images.harris","text":"harris_response = harris(img; [k], [border], [weights])\n\nPerforms Harris corner detection. The covariances can be taken using either a mean weighted filter or a gamma kernel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.shi_tomasi","page":"References","title":"Images.shi_tomasi","text":"shi_tomasi_response = shi_tomasi(img; [border], [weights])\n\nPerforms Shi Tomasi corner detection. The covariances can be taken using either a mean weighted filter or a gamma kernel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.kitchen_rosenfeld","page":"References","title":"Images.kitchen_rosenfeld","text":"kitchen_rosenfeld_response = kitchen_rosenfeld(img; [border])\n\nPerforms Kitchen Rosenfeld corner detection. The covariances can be taken using either a mean weighted filter or a gamma kernel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.fastcorners","page":"References","title":"Images.fastcorners","text":"fastcorners(img, n, threshold) -> corners\n\nPerforms FAST Corner Detection. n is the number of contiguous pixels which need to be greater (lesser) than intensity + threshold (intensity - threshold) for a pixel to be marked as a corner. The default value for n is 12.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Feature-Extraction","page":"References","title":"Feature Extraction","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"See the ImageFeatures package for a much more comprehensive set of tools.","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"blob_LoG\nBlobLoG\nfindlocalmaxima\nfindlocalminima","category":"page"},{"location":"function_reference/#Images.blob_LoG","page":"References","title":"Images.blob_LoG","text":"blob_LoG(img, σscales, [edges], [σshape]) -> Vector{BlobLoG}\n\nFind \"blobs\" in an N-D image using the negative Lapacian of Gaussians with the specifed vector or tuple of σ values. The algorithm searches for places where the filtered image (for a particular σ) is at a peak compared to all spatially- and σ-adjacent voxels, where σ is σscales[i] * σshape for some i. By default, σshape is an ntuple of 1s.\n\nThe optional edges argument controls whether peaks on the edges are included. edges can be true or false, or a N+1-tuple in which the first entry controls whether edge-σ values are eligible to serve as peaks, and the remaining N entries control each of the N dimensions of img.\n\nCitation:\n\nLindeberg T (1998), \"Feature Detection with Automatic Scale Selection\", International Journal of Computer Vision, 30(2), 79–116.\n\nSee also: BlobLoG.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.BlobLoG","page":"References","title":"Images.BlobLoG","text":"BlobLoG stores information about the location of peaks as discovered by blob_LoG. It has fields:\n\nlocation: the location of a peak in the filtered image (a CartesianIndex)\nσ: the value of σ which lead to the largest -LoG-filtered amplitude at this location\namplitude: the value of the -LoG(σ)-filtered image at the peak\n\nNote that the radius is equal to σ√2.\n\nSee also: blob_LoG.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Images.findlocalmaxima","page":"References","title":"Images.findlocalmaxima","text":"findlocalmaxima(img, [region, edges]) -> Vector{CartesianIndex}\n\nReturns the coordinates of elements whose value is larger than all of their immediate neighbors.  region is a list of dimensions to consider.  edges is a boolean specifying whether to include the first and last elements of each dimension, or a tuple-of-Bool specifying edge behavior for each dimension separately.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.findlocalminima","page":"References","title":"Images.findlocalminima","text":"Like findlocalmaxima, but returns the coordinates of the smallest elements.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Exposure","page":"References","title":"Exposure","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"build_histogram\nadjust_histogram\nadjust_histogram!\nAdaptiveEqualization\nEqualization\nGammaCorrection\nLinearStretching\nMatching\nMidwayEqualization\ncliphist\nimstretch\nimadjustintensity\ncomplement","category":"page"},{"location":"function_reference/#ImageContrastAdjustment.build_histogram","page":"References","title":"ImageContrastAdjustment.build_histogram","text":"edges, count = build_histogram(img)            # For 8-bit images only\nedges, count = build_histogram(img, nbins)\nedges, count = build_histogram(img, nbins; minval, maxval)\nedges, count = build_histogram(img, edges)\n\nGenerates a histogram for the image over nbins spread between [minval, maxval]. Color images are automatically converted to grayscale.\n\nOutput\n\nReturns edges which is a AbstractRange type that specifies how the  interval [minval, maxval] is divided into bins, and an array count which records the concomitant bin frequencies. In particular, count has the following properties:\n\ncount[0] is the number satisfying x < edges[1]\ncount[i] is the number of values x that satisfy edges[i] <= x < edges[i+1]\ncount[end] is the number satisfying x >= edges[end].\nlength(count) == length(edges)+1.\n\nDetails\n\nOne can consider a histogram as a piecewise-constant model of a probability density function f [1]. Suppose that f has support on some interval I = ab.  Let m be an integer and a = a_1  a_2  ldots  a_m  a_m+1 = b a sequence of real numbers. Construct a sequence of intervals\n\nI_1 = a_1a_2 I_2 = (a_2 a_3 ldots I_m = (a_ma_m+1\n\nwhich partition I into subsets I_j (j = 1 ldots m) on which f is constant. These subsets satisfy I_i cap I_j = emptyset forall i neq j, and are commonly referred to as bins. Together they encompass the entire range of data values such that sum_j I_j  =  I . Each bin has width w_j = I_j = a_j+1 - a_j and height h_j which is the constant probability density over the region of the bin. Integrating the constant probability density over the width of the bin w_j yields a probability mass of pi_j = h_j w_j for the bin.\n\nFor a sample x_1 x_2 ldots x_N, let\n\nn_j = sum_n = 1^Nmathbf1_(I_j)(x_n)\nquad textwhere quad\nmathbf1_(I_j)(x) =\nbegincases\n 1  textif  x in I_j\n 0  textotherwise\nendcases\n\nrepresent the number of samples falling into the interval I_j. An estimate for the probability mass of the jth bin is given by the relative frequency hatpi = fracn_jN, and the histogram estimator of the probability density function is defined as\n\nbeginaligned\nhatf_n(x)   = sum_j = 1^mfracn_jNw_j mathbf1_(I_j)(x) \n = sum_j = 1^mfrachatpi_jw_j mathbf1_(I_j)(x) \n = sum_j = 1^mhath_j mathbf1_(I_j)(x)\nendaligned\n\nThe function hatf_n(x) is a genuine density estimator because hatf_n(x)  ge 0 and\n\nbeginaligned\nint_-infty^inftyhatf_n(x) operatornamedx  = sum_j=1^m fracn_jNw_j w_j \n = 1\nendaligned\n\nOptions\n\nVarious options for the parameters of this function are described in more detail below.\n\nChoices for nbins\n\nYou can specify the number of discrete bins for the histogram. When specifying the number of bins consider the maximum number of graylevels that your image type supports. For example, with an image of type N0f8 there is a maximum of 256 possible graylevels. Hence, if you request more than 256 bins for that type of image you should expect to obtain zero counts for numerous bins.\n\nChoices for minval\n\nYou have the option to specify the lower bound of the interval over which the histogram will be computed.  If minval is not specified then the minimum value present in the image is taken as the lower bound.\n\nChoices for maxval\n\nYou have the option to specify the upper bound of the interval over which the histogram will be computed.  If maxval is not specified then the maximum value present in the image is taken as the upper bound.\n\nChoices for edges\n\nIf you do not designate the number of bins, nor the lower or upper bound of the interval, then you have the option to directly stipulate how the intervals will be divided by specifying a AbstractRange type.\n\nExample\n\nCompute the histogram of a grayscale image.\n\n\nusing TestImages, FileIO, ImageView\n\nimg =  testimage(\"mandril_gray\");\nedges, counts  = build_histogram(img, 256, minval = 0, maxval = 1)\n\nGiven a color image, compute the histogram of the red channel.\n\nimg = testimage(\"mandrill\")\nr = red.(img)\nedges, counts  = build_histogram(r, 256, minval = 0, maxval = 1)\n\nReferences\n\n[1] E. Herrholz, \"Parsimonious Histograms,\" Ph.D. dissertation, Inst. of Math. and Comp. Sci., University of Greifswald, Greifswald, Germany, 2011.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageContrastAdjustment.HistogramAdjustmentAPI.adjust_histogram","page":"References","title":"ImageContrastAdjustment.HistogramAdjustmentAPI.adjust_histogram","text":"adjust_histogram([T::Type,] img, f::AbstractHistogramAdjustmentAlgorithm, args...; kwargs...)\n\nAdjust histogram of img using algorithm f.\n\nOutput\n\nThe return image img_adjusted is an Array{T}.\n\nIf T is not specified, then it's inferred.\n\nExamples\n\nJust simply pass the input image and algorithm to adjust_histogram\n\nimg_adjusted = adjust_histogram(img, f)\n\nThis reads as \"adjust_histogram of image img using algorithm f\".\n\nYou can also explicitly specify the return type:\n\nimg_adjusted_float32 = adjust_histogram(Gray{Float32}, img, f)\n\nSee also adjust_histogram! for in-place histogram adjustment.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageContrastAdjustment.HistogramAdjustmentAPI.adjust_histogram!","page":"References","title":"ImageContrastAdjustment.HistogramAdjustmentAPI.adjust_histogram!","text":"adjust_histogram!([out,] img, f::AbstractHistogramAdjustmentAlgorithm, args...; kwargs...)\n\nAdjust histogram of img using algorithm f.\n\nOutput\n\nIf out is specified, it will be changed in place. Otherwise img will be changed in place.\n\nExamples\n\nJust simply pass an algorithm to adjust_histogram!:\n\nimg_adjusted = similar(img)\nadjust_histogram!(img_adjusted, img, f)\n\nFor cases you just want to change img in place, you don't necessarily need to manually allocate img_adjusted; just use the convenient method:\n\nadjust_histogram!(img, f)\n\nSee also: adjust_histogram\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageContrastAdjustment.AdaptiveEqualization","page":"References","title":"ImageContrastAdjustment.AdaptiveEqualization","text":"    AdaptiveEqualization <: AbstractHistogramAdjustmentAlgorithm\n    AdaptiveEqualization(; nbins = 256, minval = 0, maxval = 1, rblocks = 8, cblocks = 8, clip = 0.1)\n\n    adjust_histogram([T,] img, f::AdaptiveEqualization)\n    adjust_histogram!([out,] img, f::AdaptiveEqualization)\n\nPerforms Contrast Limited Adaptive Histogram Equalisation (CLAHE) on the input image. It differs from ordinary histogram equalization in the respect that the adaptive method computes several histograms, each corresponding to a distinct section of the image, and uses them to redistribute the lightness values of the image. It is therefore suitable for improving the local contrast and enhancing the definitions of edges in each region of an image.\n\nDetails\n\nHistogram equalisation was initially conceived to  improve the contrast in a single-channel grayscale image. The method transforms the distribution of the intensities in an image so that they are as uniform as possible [1]. The natural justification for uniformity is that the image has better contrast  if the intensity levels of an image span a wide range on the intensity scale. As it turns out, the necessary transformation is a mapping based on the cumulative histogram–-see Equalization for more details.\n\nA natural extension of histogram equalisation is to apply the contrast enhancement locally rather than globally [2]. Conceptually, one can imagine that the process involves partitioning the image into a grid of rectangular regions and applying histogram equalisation based on the local CDF of each contextual region. However, to smooth the transition of the pixels from one contextual region to another,  the mapping of a pixel is not necessarily done soley based on the local CDF of its contextual region. Rather, the mapping of a pixel may be interpolated based on the CDF of its contextual region, and the CDFs of the immediate neighbouring regions.\n\nIn adaptive histogram equalisation the image mathbfG is partitioned into P times Q equisized submatrices,\n\nmathbfG =  beginbmatrix\nmathbfG_11  mathbfG_12  ldots  mathbfG_1C \nmathbfG_21  mathbfG_22  ldots  mathbfG_2C \nvdots  vdots  ldots  vdots \nmathbfG_R1  mathbfG_R2  ldots  mathbfG_RC \nendbmatrix\n\nFor each submatrix mathbfG_rc, one computes a concomitant CDF, which we shall denote by T_rc(G_ij). In the most general case, we will require four CDFs\n\nbeginaligned\nT_1(v)   triangleq  T_rc(G_ij) \nT_2(v)   triangleq  T_(r+1)c(G_ij) \nT_3(v)   triangleq  T_(r+1)(c+1)(G_ij) \nT_4(v)   triangleq  T_r(c+1)(G_ij)\nendaligned\n\nIn order to determine which particular CDFs will be used in the interpolation step, it is useful to (i) introduce the function\n\nPhi(mathbfG_rc) = left(  phi_rc  phi_rcright) triangleq left(rP - fracP2 cQ - fracQ2 right)\n\n(ii) form the sequences  left(phi_11 phi_21 ldots phi_R1 right) and left(phi_11 phi_12 ldots phi_1C right), and (iii) define\n\nbeginaligned\nt   triangleq  fraci - phi_r1phi_(r+1)1 - phi_r1  \nu   triangleq  fracj - phi_1cphi_1(c+1) - phi_1c \nendaligned\n\nCase I (Interior)\n\nFor a  pixel G_ij in the range\n\nP - fracP2 le i le RP - fracP2  quad textand  quad  Q - fracQ2 le j le CQ - fracQ2\n\nvalues of r and c are implicitly defined by the solution to the inequalities\n\nphi_r1 le i  phi_(r+1)1  quad textand  quad  phi_1c le j  phi_1(c+1)\n\nThe bilinearly interpolated transformation that maps an intensity v at location (ij) in the image to an intensity v is given by [3]\n\nv triangleq barT(v)  = (1-t) (1-u)T_1(G_ij) + t(1-u)T_2(G_ij) + tuT_3(G_ij) +(1-t)uT_4(G_ij)\n\nCase II (Vertical Border)\n\nFor a  pixel G_ij in the range\n\nP - fracP2 le i le RP - fracP2  quad textand  quad  1 le j  Q - fracQ2   cup    CQ - fracQ2  j le CQ\n\nr is implicitly defined by the solution to the inequality phi_r1 le i  phi_(r+1)1, while\n\nc = begincases\n   1  textif   quad  1 le j  Q - fracQ2  \n   C  textif  quad   CQ - fracQ2  j le CQ\nendcases\n\nThe linearly interpolated transformation that maps an intensity v at location (ij) in the image to an intensity v is given by\n\nv triangleq barT(v)  = (1-t)T_1(G_ij) + tT_2(G_ij)\n\nCase III (Horizontal Border)\n\nFor a  pixel G_ij in the range\n\n1 le i  P - fracP2  cup    RP - fracP2  i le RP    quad textand  quad  Q - fracQ2 le j le CQ - fracQ2\n\nc is implicitly defined by the solution to the inequality phi_1c le j  phi_1(c+1), while\n\nr = begincases\n   1  textif   quad  1 le i  P - fracP2  \n   R  textif  quad   RP - fracP2  i le RP \nendcases\n\nThe linearly interpolated transformation that maps an intensity v at location (ij) in the image to an intensity v is given by\n\nv triangleq barT(v)  = (1-u)T_1(G_ij) + uT_4(G_ij)\n\nCase IV (Corners)\n\nFor a  pixel G_ij in the range\n\n1 le i  fracP2  cup  RP - fracP2  i le RP   quad textand  quad  1 le j  CQ -  fracQ2  cup    CQ - fracQ2  j le CQ \n\nwe have\n\nr = begincases\n   1  textif   quad  1 le i  P - fracP2  \n   R  textif  quad   RP - fracP2  i le RP\nendcases\n quad textand  quad\nc = begincases\n   1  textif   quad  1 le j  Q - fracQ2  \n   C  textif  quad   CQ - fracQ2  j le CQ\nendcases\n\nThe transformation that maps an intensity v at location (ij) in the image to an intensity v is given by\n\nv triangleq barT(v)  = T_1(G_ij)\n\nLimiting Contrast\n\nAn unfortunate side-effect of contrast enhancement is that it has a tendency to amplify the level of noise in an image, especially when the magnitude of the contrast enhancement is very high. The magnitude of contrast enhancement is associated with the gradient of T(cdot), because the  gradient determines the extent to which consecutive input intensities are stretched across the grey-level spectrum. One can diminish the level of noise amplification by limiting the magnitude of the contrast enhancement, that is, by limiting the magnitude of the gradient.\n\nSince the derivative of T(cdot) is the empirical density hatf_G, the slope of the mapping function at any input intensity is proportional to the height of the histogram  hatf_G at that intensity.  Therefore, limiting the slope of the local mapping function is equivalent to clipping the height of the histogram. A detailed description of the  implementation  details of the clipping process can be found in [2].\n\nOptions\n\nVarious options for the parameters of this function are described in more detail below.\n\nChoices for img\n\nThe function can handle a variety of input types. The returned image depends on the input type.\n\nFor coloured images, the input is converted to YIQ type and the Y channel is equalised. This is the combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoices for nbins in AdaptiveEqualization\n\nYou can specify the total number of bins in the histogram of each local region.\n\nChoices for rblocks and cblocks in AdaptiveEqualization\n\nThe rblocks and cblocks specify the number of blocks to divide the input image into in each direction. By default both values are set to eight.\n\nChoices for clip in AdaptiveEqualization\n\nThe clip parameter must be a value between 0 and 1. It defines an implicit threshold at which a histogram is clipped. Counts that exceed the threshold are redistributed as equally as possible so that no bin exceeds the threshold limit. A value of zero means no clipping, whereas a value of one sets the threshold at the smallest feasible bin limit. A bin limit is feasible if all bin counts can be redistributed such that no bin count exceeds the limit. In practice, a clip value of zero corresponds to maximal contrast enhancement, whereas a clip value of one corredponds to minimal contrast enhancement. The default value is 0.1.\n\nChoices for minval and maxval in AdaptiveEqualization\n\nIf minval and maxval are specified then intensities are equalized to the range [minval, maxval]. The default values are 0 and 1.\n\nExample\n\n\nusing TestImages, FileIO, ImageView\n\nimg =  testimage(\"mandril_gray\")\nimgeq = adjust_histogram(img, AdativeEqualization(nbins = 256, rblocks = 4, cblocks = 4, clip = 0.2))\n\nimshow(img)\nimshow(imgeq)\n\nReferences\n\nR. C. Gonzalez and R. E. Woods. Digital Image Processing (3rd Edition).  Upper Saddle River, NJ, USA: Prentice-Hall,  2006.\nS. M. Pizer, E. P. Amburn, J. D. Austin, R. Cromartie, A. Geselowitz, T. Greer, B. ter Haar Romeny, J. B. Zimmerman and K. Zuiderveld “Adaptive histogram equalization and its variations,” Computer Vision, Graphics, and Image Processing, vol. 38, no. 1, p. 99, Apr. 1987. 10.1016/S0734-189X(87)80186-X\nW. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery.  Numerical Recipes: The Art of Scientific Computing (3rd Edition). New York, NY, USA: Cambridge University Press, 2007.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageContrastAdjustment.Equalization","page":"References","title":"ImageContrastAdjustment.Equalization","text":"    Equalization <: AbstractHistogramAdjustmentAlgorithm\n    Equalization(; nbins = 256, minval = 0, maxval = 1)\n\n    adjust_histogram([T,] img, f::Equalization)\n    adjust_histogram!([out,] img, f::Equalization)\n\nReturns a histogram equalized image with a granularity of nbins number of bins.\n\nDetails\n\nHistogram equalization was initially conceived to  improve the contrast in a single-channel grayscale image. The method transforms the distribution of the intensities in an image so that they are as uniform as possible [1]. The natural justification for uniformity is that the image has better contrast  if the intensity levels of an image span a wide range on the intensity scale. As it turns out, the necessary transformation is a mapping based on the cumulative histogram.\n\nOne can consider an L-bit single-channel I times J image with gray values in the set 01ldotsL-1 , as a collection of independent and identically distributed random variables. Specifically, let the sample space Omega be the set of all IJ-tuples omega =(omega_11omega_12ldotsomega_1Jomega_21omega_22ldotsomega_2Jomega_I1omega_I2ldotsomega_IJ), where each omega_ij in 01ldots L-1 . Furthermore, impose a probability measure on Omega such that the functions Omega ni omega to omega_ij in 01ldotsL-1 are independent and identically distributed.\n\nOne can then regard an image as a matrix of random variables mathbfG = G_ij(omega), where each function G_ij Omega to mathbbR is defined by\n\nG_ij(omega) = fracomega_ijL-1\n\nand each G_ij is distributed according to some unknown density f_G. While f_G is unknown, one can approximate it with a normalized histogram of gray levels,\n\nhatf_G(v)= fracn_vIJ\n\nwhere\n\nn_v = left  left(ij)   G_ij(omega)  = v right  right \n\nrepresents the number of times a gray level with intensity v occurs in mathbfG. To transform the distribution of the intensities so that they are as uniform as possible one needs to find a mapping T(cdot) such that T(G_ij) thicksim U. The required mapping turns out to be the cumulative distribution function (CDF) of the empirical density hatf_G,\n\n T(G_ij) = int_0^G_ijhatf_G(w)mathrmd w\n\nOptions\n\nVarious options for the parameters of the adjust_histogram function and Equalization type are described in more detail below.\n\nChoices for img\n\nThe adjust_histogram function can handle a variety of input types.  By default type of the returned image matches the input type.\n\nFor colored images, the input is converted to YIQ type and the Y channel is equalized. This is the combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoices for nbins in Equalization\n\nYou can specify the total number of bins in the histogram.\n\nChoices for minval and maxval in Equalization\n\nIf minval and maxval are specified then intensities are equalized to the range [minval, maxval]. The default values are 0 and 1.\n\nExample\n\n\nusing TestImages, FileIO, ImageView\n\nimg =  testimage(\"mandril_gray\")\nimgeq = adjust_histogram(img, Equalization(nbins = 256, minval = 0, maxval = 1))\n\nimshow(img)\nimshow(imgeq)\n\nReferences\n\nR. C. Gonzalez and R. E. Woods. Digital Image Processing (3rd Edition).  Upper Saddle River, NJ, USA: Prentice-Hall,  2006.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageContrastAdjustment.GammaCorrection","page":"References","title":"ImageContrastAdjustment.GammaCorrection","text":"    GammaCorrection <: AbstractHistogramAdjustmentAlgorithm\n    GammaCorrection(; gamma = 1)\n\n    adjust_histogram([T,] img, f::GammaCorrection)\n    adjust_histogram!([out,] img, f::GammaCorrection)\n\nReturns a gamma corrected image.\n\nDetails\n\nGamma correction is a non-linear  transformation given by the relation\n\nf(x) = x^gamma quad textfor  x in mathbbR gamma  0\n\nIt is called a power law transformation because one quantity varies as a power of another quantity.\n\nGamma correction has historically been used to preprocess an image to compensate for the fact that the intensity of light generated by a physical device is not usually a linear function of the applied signal but instead follows a power law [1]. For example, for many Cathode Ray Tubes (CRTs) the emitted light intensity on the display is approximately equal to the voltage raised to the power of γ, where γ ∈ [1.8, 2.8]. Hence preprocessing a raw image with an exponent of 1/γ  would have ensured a linear response to brightness.\n\nResearch in psychophysics has also established an empirical  power law   between light intensity and perceptual brightness. Hence, gamma correction often serves as a useful image enhancement tool.\n\nOptions\n\nVarious options for the parameters of the adjust_histogram function and the Gamma type are described in more detail below.\n\nChoices for img\n\nThe function can handle a variety of input types. The returned image depends on the input type.\n\nFor colored images, the input is converted to YIQ type and the Y channel is gamma corrected. This is the combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoice for gamma\n\nThe gamma value must be a non-zero positive number. A gamma value less than one will yield a brighter image whereas a value greater than one will produce a darker image. If left unspecified a default value of one is assumed.\n\nExample\n\nusing ImageContrastAdjustment, ImageView\n\n# Create an example image consisting of a linear ramp of intensities.\nn = 32\nintensities = 0.0:(1.0/n):1.0\nimg = repeat(intensities, inner=(20,20))'\n\n# Brighten the dark tones.\nimgadj = adjust_histogram( img, GammaCorrection(gamma = 1/2))\n\n# Display the original and adjusted image.\nimshow(img)\nimshow(imgadj)\n\nReferences\n\nW. Burger and M. J. Burge. Digital Image Processing. Texts in Computer Science, 2016. doi:10.1007/978-1-4471-6684-9\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageContrastAdjustment.LinearStretching","page":"References","title":"ImageContrastAdjustment.LinearStretching","text":"    LinearStretching <: AbstractHistogramAdjustmentAlgorithm\n    LinearStretching(; [src_minval], [src_maxval],\n                       dst_minval=0, dst_maxval=1,\n                       no_clamp=false)\n\n    LinearStretching((src_minval, src_maxval) => (dst_minval, dst_maxval))\n    LinearStretching((src_minval, src_maxval) => nothing)\n    LinearStretching(nothing => (dst_minval, dst_maxval))\n\n    adjust_histogram([T,] img, f::LinearStretching)\n    adjust_histogram!([out,] img, f::LinearStretching)\n\nReturns an image where the range of the intensities spans the interval [dst_minval, dst_maxval].\n\nDetails\n\nLinear stretching (also called normalization) is a contrast enhancing transformation that is used to modify the dynamic range of the image. In particular, suppose that the input image has gray values in the range [A,B] and one wishes to change the dynamic range to [a,b] using a linear mapping, then the necessary transformation is given by the relation\n\nf(x) = (x-A) fracb-aB-A + a\n\nOptions\n\nVarious options for the parameters of the adjust_histogram and LinearStretching type  are described in more detail below.\n\nChoices for img\n\nThe function can handle a variety of input types. The returned image depends on the input type.\n\nFor colored images, the input is converted to the YIQ  type and the intensities of the Y channel are stretched to the specified range. The modified Y channel is then combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoices for dst_minval and dst_maxval\n\nIf destination value range dst_minval and dst_maxval are specified then intensities are mapped to the range [dst_minval, dst_maxval]. The default values are 0 and 1.\n\nChoices for src_minval and src_maxval\n\nThe source value range src_minval and src_maxval specifies the intensity range of input image. By default, the values are extrema(img) (finite). If custom values are provided, the output intensity value will be clamped to range [dst_minval, dst_maxval] if it exceeds that.\n\nno_clamp\n\nSetting no_clamp=true to disable the automatic clamp even if the output intensity value exceeds the range [dst_minval, dst_maxval]. Note that a clamp is still applied for types that has limited value range, for example, if the input eltype is N0f8, then the output will be clamped to [0.0N0f8, 1.0N0f8] even if no_clamp==true.\n\nExample\n\nusing ImageContrastAdjustment, TestImages\n\nimg = testimage(\"mandril_gray\")\n# Stretches the contrast in `img` so that it spans the unit interval.\nimgo = adjust_histogram(img, LinearStretching(dst_minval = 0, dst_maxval = 1))\n\nFor convenience, Constructing a LinearStretching object using Pair is also supported\n\n# these two constructors are equivalent\nLinearStretching(src_minval=0.1, src_maxval=0.9, dst_minval=0.05, dst_maxval=0.95)\nLinearStretching((0.1, 0.9) => (0.05, 0.95))\n\n# replace the part with `nothing` to use default values, e.g.,\n# specify only destination value range\nLinearStretching(nothing => (0.05, 0.95))\n# specify only source value range and use default destination value range, i.e., (0, 1)\nLinearStretching((0.1, 0.9) => nothing)\n\nReferences\n\nW. Burger and M. J. Burge. Digital Image Processing. Texts in Computer Science, 2016. doi:10.1007/978-1-4471-6684-9\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageContrastAdjustment.Matching","page":"References","title":"ImageContrastAdjustment.Matching","text":"    Matching <: AbstractHistogramAdjustmentAlgorithm\n    Matching(targetimg; nbins = 256, edges = nothing)\n\n    adjust_histogram([T,] img, f::Matching)\n    adjust_histogram!([out,] img, f::Matching)\n\nReturns a histogram matched image with a granularity of nbins number of bins. The first argument img is the image to be matched, whereas the argument targetimg in Matching() is the image having the desired histogram to be matched to.\n\nDetails\n\nThe purpose of histogram matching is to transform the intensities in a source image so that the intensities distribute according to the histogram of a specified target image. If one interprets histograms as piecewise-constant models of probability density functions (see build_histogram), then the histogram matching task can be modelled as the problem of transforming one probability distribution into another [1]. It turns out that the solution to this transformation problem involves the cumulative and inverse cumulative distribution functions of the source and target probability density functions.\n\nIn particular, let the random variables x thicksim p_x and z thicksim p_z  represent an intensity in the source and target image respectively, and let\n\n S(x) = int_0^xp_x(w)mathrmd w quad textand quad\n T(z) = int_0^zp_z(w)mathrmd w\n\nrepresent their concomitant cumulative distribution functions. Then the sought-after mapping Q(cdot) such that Q(x) thicksim p_z is given by\n\nQ(x) =  T^-1left( S(x) right)\n\nwhere T^-1(y) = operatornamemin  x in mathbbR  y leq T(x)  is the inverse cumulative distribution function of T(x).\n\nThe mapping suggests that one can conceptualize histogram matching as performing histogram equalization on the source and target image and relating the two equalized histograms. Refer to adjust_histogram for more details on histogram equalization.\n\nOptions\n\nVarious options for the parameters of the adjust_histogram function and Matching type are described in more detail below.\n\nChoices for img and targetimg\n\nThe adjust_histogram(img, Matching()) function can handle a variety of input types. The type of the returned image matches the input type.\n\nFor colored images, the inputs are converted to YIQ  type and the distributions of the Y channels are matched. The modified Y channel is then combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoices for nbins\n\nYou can specify the total number of bins in the histogram. If you do not specify the number of bins then a default value of 256 bins is utilized.\n\nChoices for edges\n\nIf you do not designate the number of bins, then you have the option to directly stipulate how the intervals will be divided by specifying a AbstractRange type.\n\nExample\n\nusing Images, TestImages, ImageView\n\nimg_source = testimage(\"mandril_gray\")\nimg_target = adjust_gamma(img_source, 1/2)\nimg_transformed = adjust_histogram(img_source, Matching(targetimg = img_target))\n#=\n    A visual inspection confirms that img_transformed resembles img_target\n    much more closely than img_source.\n=#\nimshow(img_source)\nimshow(img_target)\nimshow(img_transformed)\n\nReferences\n\nW. Burger and M. J. Burge. Digital Image Processing. Texts in Computer Science, 2016. doi:10.1007/978-1-4471-6684-9\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageContrastAdjustment.MidwayEqualization","page":"References","title":"ImageContrastAdjustment.MidwayEqualization","text":"    MidwayEqualization <: AbstractHistogramAdjustmentAlgorithm\n    MidwayEqualization(; nbins = 256, minval = 0, maxval = 1)\n\n    adjust_histogram([T,] img_sequence, f::MidwayEqualization(nbins = 256, edges = nothing))\n    adjust_histogram!([out_sequence,] img_sequence, f::MidwayEqualization(nbins = 256, edges = nothing))\n\nGives a pair of images the same histogram whilst maintaining as much as possible their previous grey level dynamics.\n\nDetails\n\nThe purpose of midway histogram equalization is to transform the intensities in a pair of images so that the intensities distribute according to a common \"midway\" distribution. The histogram representing the common distribution is chosen so that the original  gray level dynamics of the images are preserved as much as possible. If one interprets histograms as piecewise-constant models of probability density functions (see build_histogram), then the midway histogram equalization task can be modeled as the problem of transforming one probability distribution into another (see adjust_histogram). It turns out that the solution to this transformation problem involves the cumulative and inverse cumulative distribution functions of the source and \"midway\" probability density functions. In particular, let the random variables X_i thicksim p_x_i  (i = 12), and Z thicksim p_z  represent an intensity in the first, second and \"midway\" image respectively, and let\n\n S_X_i(x) = int_0^xp_x_i(w)mathrmd w  quad textand quad\n T_Z(x) = frac2frac1S_X_1(x) + frac1S_X_2(x)\n\nrepresent the cumulative distribution functions of the two input images, and their harmonic mean, respectively. Then the sought-after mapping Q_X_i(cdot) (i = 12) such that Q_X_i(x) thicksim p_z is given by\n\nQ_X_i(x) =  T_Z^-1left( S_X_i(x) right)\n\nwhere T_Z^-1(y) = operatornamemin  x in mathbbR  y leq T_Z(x)  is the inverse cumulative distribution function of T_Z(x).\n\nOptions\n\nVarious options for the parameters of the adjust_histogram function and MidwayEqualization types are described in more detail below.\n\nChoices for img_sequence\n\nThe function adjust_histogram expects a length-2 Vector of images (the pair of images) and returns a length-2 Vector of modified images.  The  function can handle a variety of input types. The type of the returned image matches the input type.\n\nFor colored images, the inputs are converted to YIQ  type and the distributions of the Y channels are transformed according to a \"midway\" distribution. The modified Y channel is then combined with the I and Q channels and the resulting image converted to the same type as the input.\n\nChoices for nbins\n\nYou can specify the total number of bins in the histogram. If you do not specify the number of bins then a default value of 256 bins is utilized.\n\nChoices for edges\n\nIf you do not designate the number of bins, then you have the option to directly stipulate how the intervals will be divided by specifying a AbstractRange type.\n\nExample\n\nusing Images, TestImages, ImageView, ImageContrastAdjustment\n\nimg = testimage(\"mandril_gray\")\n\n# The same image but with different intensitiy distributions\nimg1 = adjust_histogram(img, GammaCorrection(gamma = 2))\nimg2 = adjust_histogram(img, GammaCorrection(gamma = 1.2))\n\n# Midway histogram equalization will transform these two images so that their\n# intensity distributions are almost identical.\nimg_sequence = adjust_histogram([img1, img2], MidwayEqualization(nbins = 256))\nimg1o = first(img_sequence)\nimg2o = last(img_sequence)\n\nReferences\n\nT. Guillemot and J. Delon, “Implementation of the Midway Image Equalization,” Image Processing On Line, vol. 5, pp. 114–129, Jun. 2016. doi:10.5201/ipol.2016.140\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Images.cliphist","page":"References","title":"Images.cliphist","text":"clipped_hist = cliphist(hist, clip)\n\nClips the histogram above a certain value clip. The excess left in the bins exceeding clip is redistributed among the remaining bins.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.imstretch","page":"References","title":"Images.imstretch","text":"imgs = imstretch(img, m, slope) enhances or reduces (for slope > 1 or < 1, respectively) the contrast near saturation (0 and 1). This is essentially a symmetric gamma-correction. For a pixel of brightness p, the new intensity is 1/(1+(m/(p+eps))^slope).\n\nThis assumes the input img has intensities between 0 and 1.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.imadjustintensity","page":"References","title":"Images.imadjustintensity","text":"imadjustintensity(img [, (minval,maxval)]) -> Image\n\nMap intensities over the interval (minval,maxval) to the interval    [0,1]. This is equivalent to map(ScaleMinMax(eltype(img), minval,    maxval), img).  (minval,maxval) defaults to extrema(img).\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.complement","page":"References","title":"Images.complement","text":"y = complement(x)\n\nTake the complement 1-x of x.  If x is a color with an alpha channel, the alpha channel is left untouched. Don't forget to add a dot when x is an array: complement.(x)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Spatial-transformations-and-resizing","page":"References","title":"Spatial transformations and resizing","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"imresize\nimrotate\nrestrict\nwarp\nwarpedview\ninvwarpedview\nWarpedView\nInvWarpedView","category":"page"},{"location":"function_reference/#ImageTransformations.imresize","page":"References","title":"ImageTransformations.imresize","text":"imresize(img, sz) -> imgr\nimresize(img, inds) -> imgr\nimresize(img; ratio) -> imgr\n\nChange img to be of size sz (or to have indices inds). If ratio is used, then sz = ceil(Int, size(img).*ratio). This interpolates the values at sub-pixel locations. If you are shrinking the image, you risk aliasing unless you low-pass filter img first.\n\nExamples\n\njulia> img = testimage(\"lena_gray_256\") # 256*256\njulia> imresize(img, 128, 128) # 128*128\njulia> imresize(img, 1:128, 1:128) # 128*128\njulia> imresize(img, (128, 128)) # 128*128\njulia> imresize(img, (1:128, 1:128)) # 128*128\njulia> imresize(img, (1:128, )) # 128*256\njulia> imresize(img, 128) # 128*256\njulia> imresize(img, ratio = 0.5) # \njulia> imresize(img, ratio = (2, 1)) # 256*128\n\nσ = map((o,n)->0.75*o/n, size(img), sz)\nkern = KernelFactors.gaussian(σ)   # from ImageFiltering\nimgr = imresize(imfilter(img, kern, NA()), sz)\n\nSee also restrict.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.imrotate","page":"References","title":"ImageTransformations.imrotate","text":"imrotate(img, θ, [indices], [degree = Linear()], [fill = NaN]) -> imgr\n\nRotate image img by θ∈[0,2π) in a clockwise direction around its center point. To rotate the image counterclockwise, specify a negative value for angle.\n\nBy default, rotated image imgr will not be cropped. Bilinear interpolation will be used and values outside the image are filled with NaN if possible, otherwise with 0.\n\nExamples\n\njulia> img = testimage(\"cameraman\")\n\n# rotate with bilinear interpolation but without cropping \njulia> imrotate(img, π/4)\n\n# rotate with bilinear interpolation and with cropping\njulia> imrotate(img, π/4, axes(img))\n\n# rotate with nearest interpolation but without cropping\njulia> imrotate(img, π/4, Constant())\n\nSee also warp.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.restrict","page":"References","title":"ImageTransformations.restrict","text":"restrict(img[, region]) -> imgr\n\nReduce the size of img by approximately two-fold along the dimensions listed in region, or all spatial coordinates if region is not specified. The term restrict is taken from the coarsening operation of algebraic multigrid methods; it is the adjoint of \"prolongation\" (which is essentially interpolation). restrict anti-aliases the image as it goes, so is better than a naive summation over 2x2 blocks. The implementation of restrict has been tuned for performance, and should be a fast method for constructing pyramids.\n\nIf l is the size of img along a particular dimension, restrict produces an array of size (l+1)÷2 for odd l, and l÷2 + 1 for even l. See the example below for an explanation.\n\nSee also imresize.\n\nExample\n\na_course = [0, 1, 0.3]\n\nIf we were to interpolate this at the halfway points, we'd get\n\na_fine = [0, 0.5, 1, 0.65, 0.3]\n\nNote that a_fine is obtained from a_course via the prolongation operator P as P*a_course, where\n\nP = [1   0   0;      # this line \"copies over\" the first point\n     0.5 0.5 0;      # this line takes the mean of the first and second point\n     0   1   0;      # copy the second point\n     0   0.5 0.5;    # take the mean of the second and third\n     0   0   1]      # copy the third\n\nrestrict is the adjoint of prolongation. Consequently,\n\njulia> restrict(a_fine)\n3-element Array{Float64,1}:\n 0.125\n 0.7875\n 0.3125\n\njulia> (P'*a_fine)/2\n3-element Array{Float64,1}:\n 0.125\n 0.7875\n 0.3125\n\nwhere the division by 2 approximately preserves the mean intensity of the input.\n\nAs we see here, for odd-length a_fine, restriction is the adjoint of interpolation at half-grid points. When length(a_fine) is even, restriction is the adjoint of interpolation at 1/4 and 3/4-grid points. This turns out to be the origin of the l->l÷2 + 1 behavior.\n\nOne consequence of this definition is that the edges move towards zero:\n\njulia> restrict(ones(11))\n6-element Array{Float64,1}:\n 0.75\n 1.0\n 1.0\n 1.0\n 1.0\n 0.75\n\nIn some applications (e.g., image registration), you may find it useful to trim the edges.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.warp","page":"References","title":"ImageTransformations.warp","text":"warp(img, tform, [indices], [degree = Linear()], [fill = NaN]) -> imgw\n\nTransform the coordinates of img, returning a new imgw satisfying imgw[I] = img[tform(I)]. This approach is known as backward mode warping. The transformation tform must accept a SVector as input. A useful package to create a wide variety of such transformations is CoordinateTransformations.jl.\n\nReconstruction scheme\n\nDuring warping, values for img must be reconstructed at arbitrary locations tform(I) which do not lie on to the lattice of pixels. How this reconstruction is done depends on the type of img and the optional parameter degree.\n\nWhen img is a plain array, then on-grid b-spline interpolation will be used. It is possible to configure what degree of b-spline to use with the parameter degree. For example one can use degree = Linear() for linear interpolation, degree = Constant() for nearest neighbor interpolation, or degree = Quadratic(Flat()) for quadratic interpolation.\n\nIn the case tform(I) maps to indices outside the original img, those locations are set to a value fill (which defaults to NaN if the element type supports it, and 0 otherwise). The parameter fill also accepts extrapolation schemes, such as Flat(), Periodic() or Reflect().\n\nFor more control over the reconstruction scheme –- and how beyond-the-edge points are handled –- pass img as an AbstractInterpolation or AbstractExtrapolation from Interpolations.jl.\n\nThe meaning of the coordinates\n\nThe output array imgw has indices that would result from applying inv(tform) to the indices of img. This can be very handy for keeping track of how pixels in imgw line up with pixels in img.\n\nIf you just want a plain array, you can \"strip\" the custom indices with parent(imgw).\n\nExamples: a 2d rotation (see JuliaImages documentation for pictures)\n\njulia> using Images, CoordinateTransformations, Rotations, TestImages, OffsetArrays\n\njulia> img = testimage(\"lighthouse\");\n\njulia> axes(img)\n(Base.OneTo(512),Base.OneTo(768))\n\n# Rotate around the center of `img`\njulia> tfm = recenter(RotMatrix(-pi/4), center(img))\nAffineMap([0.707107 0.707107; -0.707107 0.707107], [-196.755,293.99])\n\njulia> imgw = warp(img, tfm);\n\njulia> axes(imgw)\n(-196:709,-68:837)\n\n# Alternatively, specify the origin in the image itself\njulia> img0 = OffsetArray(img, -30:481, -384:383);  # origin near top of image\n\njulia> rot = LinearMap(RotMatrix(-pi/4))\nLinearMap([0.707107 -0.707107; 0.707107 0.707107])\n\njulia> imgw = warp(img0, rot);\n\njulia> axes(imgw)\n(-293:612,-293:611)\n\njulia> imgr = parent(imgw);\n\njulia> axes(imgr)\n(Base.OneTo(906),Base.OneTo(905))\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.warpedview","page":"References","title":"ImageTransformations.warpedview","text":"warpedview(img, tform, [indices], [degree = Linear()], [fill = NaN]) -> wv\n\nCreate a view of img that lazily transforms any given index I passed to wv[I] to correspond to img[tform(I)]. This approach is known as backward mode warping. The given transformation tform must accept a SVector as input. A useful package to create a wide variety of such transformations is CoordinateTransformations.jl.\n\nWhen invoking wv[I], values for img must be reconstructed at arbitrary locations tform(I) which do not lie on to the lattice of pixels. How this reconstruction is done depends on the type of img and the optional parameter degree. When img is a plain array, then on-grid b-spline interpolation will be used, where the pixel of img will serve as the coeficients. It is possible to configure what degree of b-spline to use with the parameter degree. The two possible values are degree = Linear() for linear interpolation, or degree = Constant() for nearest neighbor interpolation.\n\nIn the case tform(I) maps to indices outside the domain of img, those locations are set to a value fill (which defaults to NaN if the element type supports it, and 0 otherwise). Additionally, the parameter fill also accepts extrapolation schemes, such as Flat(), Periodic() or Reflect().\n\nThe optional parameter indices can be used to specify the domain of the resulting WarpedView. By default the indices are computed in such a way that the resulting WarpedView contains all the original pixels in img. To do this inv(tform) has to be computed. If the given transformation tform does not support inv, then the parameter indices has to be specified manually.\n\nwarpedview is essentially a non-coping, lazy version of warp. As such, the two functions share the same interface, with one important difference. warpedview will insist that the resulting WarpedView will be a view of img (i.e. parent(warpedview(img, ...)) === img). Consequently, warpedview restricts the parameter degree to be either Linear() or Constant().\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.invwarpedview","page":"References","title":"ImageTransformations.invwarpedview","text":"invwarpedview(img, tinv, [indices], [degree = Linear()], [fill = NaN]) -> wv\n\nCreate a view of img that lazily transforms any given index I passed to wv[I] to correspond to img[inv(tinv)(I)]. While technically this approach is known as backward mode warping, note that InvWarpedView is created by supplying the forward transformation. The given transformation tinv must accept a SVector as input and support inv(tinv). A useful package to create a wide variety of such transformations is CoordinateTransformations.jl.\n\nWhen invoking wv[I], values for img must be reconstructed at arbitrary locations inv(tinv)(I). InvWarpedView serves as a wrapper around WarpedView which takes care of interpolation and extrapolation. The parameters degree and fill can be used to specify the b-spline degree and the extrapolation scheme respectively.\n\nThe optional parameter indices can be used to specify the domain of the resulting wv. By default the indices are computed in such a way that wv contains all the original pixels in img.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageTransformations.WarpedView","page":"References","title":"ImageTransformations.WarpedView","text":"WarpedView(img, tform, [indices]) -> wv\n\nCreate a view of img that lazily transforms any given index I passed to wv[I] to correspond to img[tform(I)]. This approach is known as backward mode warping.\n\nThe optional parameter indices can be used to specify the domain of the resulting wv. By default the indices are computed in such a way that wv contains all the original pixels in img. To do this inv(tform) has to be computed. If the given transformation tform does not support inv, then the parameter indices has to be specified manually.\n\nsee warpedview for more information.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageTransformations.InvWarpedView","page":"References","title":"ImageTransformations.InvWarpedView","text":"InvWarpedView(img, tinv, [indices]) -> wv\n\nCreate a view of img that lazily transforms any given index I passed to wv[I] to correspond to img[inv(tinv)(I)]. While technically this approach is known as backward mode warping, note that InvWarpedView is created by supplying the forward transformation\n\nThe conceptual difference to WarpedView is that InvWarpedView is intended to be used when reasoning about the image is more convenient that reasoning about the indices. Furthermore, InvWarpedView allows simple nesting of transformations, in which case the transformations will be composed into a single one.\n\nThe optional parameter indices can be used to specify the domain of the resulting wv. By default the indices are computed in such a way that wv contains all the original pixels in img.\n\nsee invwarpedview for more information.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Image-statistics","page":"References","title":"Image statistics","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Functions for image statistics are spreaded out in Images.jl, ImageDistances.jl and ImageQualityIndexes.jl","category":"page"},{"location":"function_reference/","page":"References","title":"References","text":"minfinite\nmaxfinite\nmaxabsfinite\nmeanfinite\nentropy","category":"page"},{"location":"function_reference/#Images.minfinite","page":"References","title":"Images.minfinite","text":"m = minfinite(A) calculates the minimum value in A, ignoring any values that are not finite (Inf or NaN).\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.maxfinite","page":"References","title":"Images.maxfinite","text":"m = maxfinite(A) calculates the maximum value in A, ignoring any values that are not finite (Inf or NaN).\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.maxabsfinite","page":"References","title":"Images.maxabsfinite","text":"m = maxabsfinite(A) calculates the maximum absolute value in A, ignoring any values that are not finite (Inf or NaN).\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.meanfinite","page":"References","title":"Images.meanfinite","text":"M = meanfinite(img, region) calculates the mean value along the dimensions listed in region, ignoring any non-finite values.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.entropy","page":"References","title":"Images.entropy","text":"entropy(logᵦ, img)\nentropy(img; [kind=:shannon])\n\nCompute the entropy of a grayscale image defined as -sum(p.*logᵦ(p)). The base β of the logarithm (a.k.a. entropy unit) is one of the following:\n\n:shannon (log base 2, default), or use logᵦ = log2\n:nat (log base e), or use logᵦ = log\n:hartley (log base 10), or use logᵦ = log10\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#General-Distances","page":"References","title":"General Distances","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"type name convenient syntax math definition\nEuclidean euclidean(x, y) sqrt(sum((x - y) .^ 2))\nSqEuclidean sqeuclidean(x, y) sum((x - y).^2)\nCityblock cityblock(x, y) sum(abs(x - y))\nTotalVariation totalvariation(x, y) sum(abs(x - y)) / 2\nMinkowski minkowski(x, y, p) sum(abs(x - y).^p) ^ (1/p)\nHamming hamming(x, y) sum(x .!= y)\nSumAbsoluteDifference sad(x, y) sum(abs(x - y))\nSumSquaredDifference ssd(x, y) sum((x - y).^2)\nMeanAbsoluteError mae(x, y), sadn(x, y) sum(abs(x - y))/len(x)\nMeanSquaredError mse(x, y), ssdn(x, y) sum((x - y).^2)/len(x)\nRootMeanSquaredError rmse(x, y) sqrt(sum((x - y) .^ 2))\nNCC ncc(x, y) dot(x,y)/(norm(x)*norm(y))","category":"page"},{"location":"function_reference/#Image-specific-Distances","page":"References","title":"Image-specific Distances","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Distance type Convenient syntax References\nHausdorff and ModifiedHausdorff hausdorff(imgA,imgB) and modified_hausdorff(imgA,imgB) Dubuisson, M-P et al. 1994. A Modified Hausdorff Distance for Object-Matching.\nCIEDE2000 ciede2000(imgA,imgB) and ciede2000(imgA,imgB; metric=DE_2000()) Sharma, G., Wu, W., and Dalal, E. N., 2005. The CIEDE2000 color‐difference formula.","category":"page"},{"location":"function_reference/#Image-metrics","page":"References","title":"Image metrics","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"PSNR\nSSIM\ncolorfulness\nHASLER_AND_SUSSTRUNK_M3","category":"page"},{"location":"function_reference/#ImageQualityIndexes.PSNR","page":"References","title":"ImageQualityIndexes.PSNR","text":"PSNR <: FullReferenceIQI\nassess(PSNR(), x, ref, [, peakval])\nassess_psnr(x, ref [, peakval])\n\nPeak signal-to-noise ratio (PSNR) is used to measure the quality of image in present of noise and corruption.\n\nFor gray image x, PSNR (in dB) is calculated by 10log10(peakval^2/mse(x, ref)), where peakval is the maximum possible pixel value of image ref. x will be converted to type of ref when necessary.\n\nGenerally, for non-gray image x, PSNR is reported against each channel of ref and outputs a Vector, peakval needs to be a vector as well.\n\ninfo: Info\nConventionally, m×n rgb image is treated as m×n×3 gray image. To calculated channelwise PSNR of rgb image, one could pass peakval as vector, e.g., psnr(x, ref, [1.0, 1.0, 1.0])\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageQualityIndexes.SSIM","page":"References","title":"ImageQualityIndexes.SSIM","text":"SSIM([kernel], [(α, β, γ)]) <: FullReferenceIQI\nassess(iqi::SSIM, img, ref)\nassess_ssim(img, ref)\n\nStructural similarity (SSIM) index is an image quality assessment method based on degradation of structural information.\n\nThe SSIM index is composed of three components: luminance, contrast, and structure; ssim = 𝐿ᵅ * 𝐶ᵝ * 𝑆ᵞ, where W := (α, β, γ) controls relative importance of each components. By default W = (1.0, 1.0, 1.0).\n\nIn practice, a mean version SSIM is used. At each pixel, SSIM is calculated locally with neighborhoods weighted by kernel, returning a ssim map; ssim is actaully mean(ssim_map). By default kernel = KernelFactors.gaussian(1.5, 11).\n\ninfo: Info\nSSIM is defined only for gray images. RGB images are treated as 3d Gray images. General Color3 images are converted to RGB images first, in which case, you could manually expand them using channelview if you don't want them converted to RGB first.\n\nExample\n\nssim(img, ref) should be sufficient to get a benchmark for algorithms. One could also instantiate a customed SSIM, then pass it to assess or use it as a function. For example:\n\niqi = SSIM(KernelFactors.gaussian(2.5, 17), (1.0, 1.0, 2.0))\nassess(iqi, img, ref)\niqi(img, ref)\n\nReference\n\n[1] Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4), 600-612.\n\n[2] Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2003). The SSIM Index for Image Quality Assessment. Retrived May 30, 2019, from http://www.cns.nyu.edu/~lcv/ssim/\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageQualityIndexes.colorfulness","page":"References","title":"ImageQualityIndexes.colorfulness","text":" M =  colorfulness(img)\n M =  colorfulness(HASLER_AND_SUSSTRUNK_M3(), img)\n\nMeasures the colorfulness of a natural image. Uses the HASLER_AND_SUSSTRUNK_M3 method by default.\n\nSee also: HASLER_AND_SUSSTRUNK_M3.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageQualityIndexes.HASLER_AND_SUSSTRUNK_M3","page":"References","title":"ImageQualityIndexes.HASLER_AND_SUSSTRUNK_M3","text":"HASLER_AND_SUSSTRUNK_M3 <: NoReferenceIQI\nM =  hasler_and_susstrunk_m3(img)\n\nCalculates the colorfulness of a natural image using method M3 from [1]. As a guide to interpretation of results, the authors suggest:\n\nAttribute M3\nNot colorful 0\nslightly colorful 15\nmoderately colorful 33\naveragely colorful 45\nquite colorful 59\nhighly colorful 82\nextremely colorful 109\n\n[1] Hasler, D. and Süsstrunk, S.E., 2003, June. Measuring colorfulness in natural images. In Human vision and electronic imaging VIII (Vol. 5007, pp. 87-96). International Society for Optics and Photonics.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Morphological-operations","page":"References","title":"Morphological operations","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"dilate\nerode\nopening\nclosing\ntophat\nbothat\nmorphogradient\nmorpholaplace\nlabel_components\ncomponent_boxes\ncomponent_lengths\ncomponent_indices\ncomponent_subscripts\ncomponent_centroids\nfeature_transform\ndistance_transform\nconvexhull\nGuoAlgo\nthinning","category":"page"},{"location":"function_reference/#ImageMorphology.dilate","page":"References","title":"ImageMorphology.dilate","text":"imgd = dilate(img, [region])\n\nperform a max-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g., region = [1,2] would exclude the third dimension from filtering.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.erode","page":"References","title":"ImageMorphology.erode","text":"imge = erode(img, [region])\n\nperform a min-filter over nearest-neighbors. The default is 8-connectivity in 2d, 27-connectivity in 3d, etc. You can specify the list of dimensions that you want to include in the connectivity, e.g., region = [1,2] would exclude the third dimension from filtering.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.opening","page":"References","title":"ImageMorphology.opening","text":"imgo = opening(img, [region]) performs the opening morphology operation, equivalent to dilate(erode(img)). region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.closing","page":"References","title":"ImageMorphology.closing","text":"imgc = closing(img, [region]) performs the closing morphology operation, equivalent to erode(dilate(img)). region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.tophat","page":"References","title":"ImageMorphology.tophat","text":"imgth = tophat(img, [region]) performs top hat of an image, which is defined as the image minus its morphological opening. region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.bothat","page":"References","title":"ImageMorphology.bothat","text":"imgbh = bothat(img, [region]) performs bottom hat of an image, which is defined as its morphological closing minus the original image. region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.morphogradient","page":"References","title":"ImageMorphology.morphogradient","text":"imgmg = morphogradient(img, [region]) returns morphological gradient of the image, which is the difference between the dilation and the erosion of a given image. region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.morpholaplace","page":"References","title":"ImageMorphology.morpholaplace","text":"imgml = morpholaplace(img, [region]) performs Morphological Laplacian of an image, which is defined as the arithmetic difference between the internal and the external gradient. region allows you to control the dimensions over which this operation is performed.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.label_components","page":"References","title":"ImageMorphology.label_components","text":"label = label_components(tf, [connectivity])\nlabel = label_components(tf, [region])\n\nFind the connected components in a binary array tf. There are two forms that connectivity can take:\n\nIt can be a boolean array of the same dimensionality as tf, of size 1 or 3\n\nalong each dimension. Each entry in the array determines whether a given neighbor is used for connectivity analyses. For example, connectivity = trues(3,3) would use 8-connectivity and test all pixels that touch the current one, even the corners.\n\nYou can provide a list indicating which dimensions are used to\n\ndetermine connectivity. For example, region = [1,3] would not test neighbors along dimension 2 for connectivity. This corresponds to just the nearest neighbors, i.e., 4-connectivity in 2d and 6-connectivity in 3d.\n\nThe default is region = 1:ndims(A).\n\nThe output label is an integer array, where 0 is used for background pixels, and each connected region gets a different integer index.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.component_boxes","page":"References","title":"ImageMorphology.component_boxes","text":"component_boxes(labeled_array) -> an array of bounding boxes for each label, including the background label 0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.component_lengths","page":"References","title":"ImageMorphology.component_lengths","text":"component_lengths(labeled_array) -> an array of areas (2D), volumes (3D), etc. for each label, including the background label 0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.component_indices","page":"References","title":"ImageMorphology.component_indices","text":"component_indices(labeled_array) -> an array of pixels for each label, including the background label 0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.component_subscripts","page":"References","title":"ImageMorphology.component_subscripts","text":"component_subscripts(labeled_array) -> an array of pixels for each label, including the background label 0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.component_centroids","page":"References","title":"ImageMorphology.component_centroids","text":"component_centroids(labeled_array) -> an array of centroids for each label, including the background label 0\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.FeatureTransform.feature_transform","page":"References","title":"ImageMorphology.FeatureTransform.feature_transform","text":"feature_transform(I::AbstractArray{Bool, N}, [w=nothing]) -> F\n\nCompute the feature transform of a binary image I, finding the closest \"feature\" (positions where I is true) for each location in I.  Specifically, F[i] is a CartesianIndex encoding the position closest to i for which I[F[i]] is true.  In cases where two or more features in I have the same distance from i, an arbitrary feature is chosen. If I has no true values, then all locations are mapped to an index where each coordinate is typemin(Int).\n\nOptionally specify the weight w assigned to each coordinate.  For example, if I corresponds to an image where voxels are anisotropic, w could be the voxel spacing along each coordinate axis. The default value of nothing is equivalent to w=(1,1,...).\n\nSee also: distance_transform.\n\nCitation\n\n'A Linear Time Algorithm for Computing Exact Euclidean Distance Transforms of Binary Images in Arbitrary Dimensions' Maurer et al., 2003\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.FeatureTransform.distance_transform","page":"References","title":"ImageMorphology.FeatureTransform.distance_transform","text":"distance_transform(F::AbstractArray{CartesianIndex}, [w=nothing]) -> D\n\nCompute the distance transform of F, where each element F[i] represents a \"target\" or \"feature\" location assigned to i. Specifically, D[i] is the distance between i and F[i]. Optionally specify the weight w assigned to each coordinate; the default value of nothing is equivalent to w=(1,1,...).\n\nSee also: feature_transform.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.convexhull","page":"References","title":"ImageMorphology.convexhull","text":"chull = convexhull(img)\n\nComputes the convex hull of a binary image and returns the vertices of convex hull as a CartesianIndex array.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMorphology.GuoAlgo","page":"References","title":"ImageMorphology.GuoAlgo","text":"struct GuoAlgo <: ThinAlgo end\n\nThe Guo algorithm evaluates three conditions in order to determine which pixels of the image should be removed.\n\nThe three conditions are explained in the page 361 of Guo, Z., & Hall, R. W. (1989). Parallel thinning with two-subiteration algorithms. Communications of the ACM, 32(3), 359-373.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageMorphology.thinning","page":"References","title":"ImageMorphology.thinning","text":"thinning(img::AbstractArray{Bool}; algo::ThinAlgo=GuoAlgo())\n\nApplies a binary blob thinning operation to achieve a skeletization of the input image.\n\nSee also: GuoAlgo\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Interpolation","page":"References","title":"Interpolation","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"bilinear_interpolation","category":"page"},{"location":"function_reference/#Images.bilinear_interpolation","page":"References","title":"Images.bilinear_interpolation","text":"P = bilinear_interpolation(img, r, c)\n\nBilinear Interpolation is used to interpolate functions of two variables on a rectilinear 2D grid.\n\nThe interpolation is done in one direction first and then the values obtained are used to do the interpolation in the second direction.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Integral-Images","page":"References","title":"Integral Images","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"integral_image\nboxdiff","category":"page"},{"location":"function_reference/#Images.integral_image","page":"References","title":"Images.integral_image","text":"integral_img = integral_image(img)\n\nReturns the integral image of an image. The integral image is calculated by assigning to each pixel the sum of all pixels above it and to its left, i.e. the rectangle from (1, 1) to the pixel. An integral image is a data structure which helps in efficient calculation of sum of pixels in a rectangular subset of an image. See boxdiff for more information.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Images.boxdiff","page":"References","title":"Images.boxdiff","text":"sum = boxdiff(integral_image, ytop:ybot, xtop:xbot)\nsum = boxdiff(integral_image, CartesianIndex(tl_y, tl_x), CartesianIndex(br_y, br_x))\nsum = boxdiff(integral_image, tl_y, tl_x, br_y, br_x)\n\nAn integral image is a data structure which helps in efficient calculation of sum of pixels in a rectangular subset of an image. It stores at each pixel the sum of all pixels above it and to its left. The sum of a window in an image can be directly calculated using four array references of the integral image, irrespective of the size of the window, given the yrange and xrange of the window. Given an integral image -\n\n    A - - - - - - B -\n    - * * * * * * * -\n    - * * * * * * * -\n    - * * * * * * * -\n    - * * * * * * * -\n    - * * * * * * * -\n    C * * * * * * D -\n    - - - - - - - - -\n\nThe sum of pixels in the area denoted by * is given by S = D + A - B - C.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Pyramids","page":"References","title":"Pyramids","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"gaussian_pyramid","category":"page"},{"location":"function_reference/#Images.gaussian_pyramid","page":"References","title":"Images.gaussian_pyramid","text":"pyramid = gaussian_pyramid(img, n_scales, downsample, sigma)\n\nReturns a  gaussian pyramid of scales n_scales, each downsampled by a factor downsample > 1 and sigma for the gaussian kernel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Image-metadata-utilities","page":"References","title":"Image metadata utilities","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"ImageMeta\narraydata\nproperties\ncopyproperties\nshareproperties\nspatialproperties","category":"page"},{"location":"function_reference/#ImageMetadata.ImageMeta","page":"References","title":"ImageMetadata.ImageMeta","text":"ImageMeta is an AbstractArray that can have metadata, stored in a dictionary.\n\nConstruct an image with ImageMeta(A, props) (for a properties dictionary props), or with ImageMeta(A, prop1=val1, prop2=val2, ...).\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageAxes.arraydata","page":"References","title":"ImageAxes.arraydata","text":"arraydata(img::ImageMeta) -> array\n\nExtract the data from img, omitting the properties dictionary. array shares storage with img, so changes to one affect the other.\n\nSee also: properties.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMetadata.properties","page":"References","title":"ImageMetadata.properties","text":"properties(imgmeta) -> props\n\nExtract the properties dictionary props for imgmeta. props shares storage with img, so changes to one affect the other.\n\nSee also: arraydata.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMetadata.copyproperties","page":"References","title":"ImageMetadata.copyproperties","text":"copyproperties(img::ImageMeta, data) -> imgnew\n\nCreate a new \"image,\" copying the properties dictionary of img but using the data of the AbstractArray data. Note that changing the properties of imgnew does not affect the properties of img.\n\nSee also: shareproperties.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMetadata.shareproperties","page":"References","title":"ImageMetadata.shareproperties","text":"shareproperties(img::ImageMeta, data) -> imgnew\n\nCreate a new \"image,\" reusing the properties dictionary of img but using the data of the AbstractArray data. The two images have synchronized properties; modifying one also affects the other.\n\nSee also: copyproperties.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageMetadata.spatialproperties","page":"References","title":"ImageMetadata.spatialproperties","text":"spatialproperties(img)\n\nReturn a vector of strings, containing the names of properties that have been declared \"spatial\" and hence should be permuted when calling permutedims.  Declare such properties like this:\n\nimg[:spatialproperties] = [:spacedirections]\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Image-segmentation","page":"References","title":"Image segmentation","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"SegmentedImage\nImageEdge\notsu_threshold\nlabels_map\nsegment_labels\nsegment_pixel_count\nsegment_mean\nseeded_region_growing\nunseeded_region_growing\nfelzenszwalb\nfast_scanning\nwatershed\nhmin_transform\nregion_adjacency_graph\nrem_segment\nrem_segment!\nprune_segments\nregion_tree\nregion_splitting","category":"page"},{"location":"function_reference/#ImageSegmentation.SegmentedImage","page":"References","title":"ImageSegmentation.SegmentedImage","text":"SegmentedImage type contains the index-label mapping, assigned labels, segment mean intensity and pixel count of each segment.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageSegmentation.ImageEdge","page":"References","title":"ImageSegmentation.ImageEdge","text":"edge = ImageEdge(index1, index2, weight)\n\nConstruct an edge in a Region Adjacency Graph. index1 and index2 are the integers corresponding to individual pixels/voxels (in the sense of linear indexing via sub2ind), and weight is the edge weight (measures the dissimilarity between pixels/voxels).\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Images.otsu_threshold","page":"References","title":"Images.otsu_threshold","text":"thres = otsu_threshold(img)\nthres = otsu_threshold(img, bins)\n\nComputes threshold for grayscale image using Otsu's method.\n\nParameters:\n\nimg         = Grayscale input image\nbins        = Number of bins used to compute the histogram. Needed for floating-point images.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.labels_map","page":"References","title":"ImageSegmentation.labels_map","text":"img_labeled = labels_map(seg)\n\nReturn an array containing the label assigned to each pixel.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.segment_labels","page":"References","title":"ImageSegmentation.segment_labels","text":"labels = segment_labels(seg)\n\nReturns the list of assigned labels\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.segment_pixel_count","page":"References","title":"ImageSegmentation.segment_pixel_count","text":"c = segment_pixel_count(seg, l)\n\nReturns the count of pixels that are assigned label l. If no label is supplied, it returns a Dict(label=>pixel_count) of all the labels.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.segment_mean","page":"References","title":"ImageSegmentation.segment_mean","text":"m = segment_mean(seg, l)\n\nReturns the mean intensity of label l. If no label is supplied, it returns a Dict(label=>mean) of all the labels.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.seeded_region_growing","page":"References","title":"ImageSegmentation.seeded_region_growing","text":"seg_img = seeded_region_growing(img, seeds, [kernel_dim], [diff_fn])\nseg_img = seeded_region_growing(img, seeds, [neighbourhood], [diff_fn])\n\nSegments the N-D image img using the seeded region growing algorithm and returns a SegmentedImage containing information about the segments.\n\nArguments:\n\nimg             :  N-D image to be segmented (arbitrary axes are allowed)\nseeds           :  Vector containing seeds. Each seed is a Tuple of a                      CartesianIndex{N} and a label. See below note for more                      information on labels.\nkernel_dim      :  (Optional) Vector{Int} having length N or a NTuple{N,Int}                      whose ith element is an odd positive integer representing                      the length of the ith edge of the N-orthotopic neighbourhood\nneighbourhood   :  (Optional) Function taking CartesianIndex{N} as input and                      returning the neighbourhood of that point.\ndiff_fn         :  (Optional) Function that returns a difference measure(δ)                      between the mean color of a region and color of a point\n\nnote: Note\nThe labels attached to points must be positive integers, although multiple points can be assigned the same label. The output includes a labelled array that has same indexing as that of input image. Every index is assigned to either one of labels or a special label '0' indicating that the algorithm was unable to assign that index to a unique label.\n\nExamples\n\njulia> img = zeros(Gray{N0f8},4,4);\n\njulia> img[2:4,2:4] .= 1;\n\njulia> seeds = [(CartesianIndex(3,1),1),(CartesianIndex(2,2),2)];\n\njulia> seg = seeded_region_growing(img, seeds);\n\njulia> labels_map(seg)\n4×4 Array{Int64,2}:\n 1  1  1  1\n 1  2  2  2\n 1  2  2  2\n 1  2  2  2\n\nCitation:\n\nAlbert Mehnert, Paul Jackaway (1997), \"An improved seeded region growing algorithm\", Pattern Recognition Letters 18 (1997), 1065-1071\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.unseeded_region_growing","page":"References","title":"ImageSegmentation.unseeded_region_growing","text":"seg_img = unseeded_region_growing(img, threshold, [kernel_dim], [diff_fn])\nseg_img = unseeded_region_growing(img, threshold, [neighbourhood], [diff_fn])\n\nSegments the N-D image using automatic (unseeded) region growing algorithm and returns a SegmentedImage containing information about the segments.\n\nArguments:\n\nimg             :  N-D image to be segmented (arbitrary axes are allowed)\nthreshold       :  Upper bound of the difference measure (δ) for considering                      pixel into same segment\nkernel_dim      :  (Optional) Vector{Int} having length N or a NTuple{N,Int}                      whose ith element is an odd positive integer representing                      the length of the ith edge of the N-orthotopic neighbourhood\nneighbourhood   :  (Optional) Function taking CartesianIndex{N} as input and                      returning the neighbourhood of that point.\ndiff_fn         :  (Optional) Function that returns a difference measure (δ)                      between the mean color of a region and color of a point\n\nExamples\n\njulia> img = zeros(Gray{N0f8},4,4);\n\njulia> img[2:4,2:4] .= 1;\n\njulia> seg = unseeded_region_growing(img, 0.2);\n\njulia> labels_map(seg)\n4×4 Array{Int64,2}:\n 1  1  1  1\n 1  2  2  2\n 1  2  2  2\n 1  2  2  2\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.felzenszwalb","page":"References","title":"ImageSegmentation.felzenszwalb","text":"segments                = felzenszwalb(img, k, [min_size])\nindex_map, num_segments = felzenszwalb(edges, num_vertices, k, [min_size])\n\nSegments an image using Felzenszwalb's graph-based algorithm. The function can be used in either of two ways -\n\nsegments = felzenszwalb(img, k, [min_size])\n\nSegments an image using Felzenszwalb's segmentation algorithm and returns the result as SegmentedImage. The algorithm uses euclidean distance in color space as edge weights for the region adjacency graph.\n\nParameters:\n\nimg            = input image\nk              = Threshold for region merging step. Larger threshold will result in bigger segments.\nmin_size       = Minimum segment size\n\nindex_map, num_segments = felzenszwalb(edges, num_vertices, k, [min_size])\n\nSegments an image represented as Region Adjacency Graph(RAG) using Felzenszwalb's segmentation algorithm. Each pixel/region  corresponds to a node in the graph and weights on each edge measure the dissimilarity between pixels. The function returns the number of segments and index mapping from nodes of the RAG to segments.\n\nParameters:\n\nedges          = Array of edges in RAG. Each edge is represented as ImageEdge.\nnum_vertices   = Number of vertices in RAG\nk              = Threshold for region merging step. Larger threshold will result in bigger segments.\nmin_size       = Minimum segment size\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.fast_scanning","page":"References","title":"ImageSegmentation.fast_scanning","text":"seg_img = fast_scanning(img, threshold, [diff_fn])\n\nSegments the N-D image using a fast scanning algorithm and returns a SegmentedImage containing information about the segments.\n\nArguments:\n\nimg         : N-D image to be segmented (arbitrary axes are allowed)\nthreshold   : Upper bound of the difference measure (δ) for considering                 pixel into same segment; an AbstractArray can be passed                 having same number of dimensions as that of img for adaptive                 thresholding\ndiff_fn     : (Optional) Function that returns a difference measure (δ)                 between the mean color of a region and color of a point\n\nExamples:\n\njulia> img = zeros(Float64, (3,3));\n\njulia> img[2,:] .= 0.5;\n\njulia> img[:,2] .= 0.6;\n\njulia> seg = fast_scanning(img, 0.2);\n\njulia> labels_map(seg)\n3×3 Array{Int64,2}:\n 1  4  5\n 4  4  4\n 3  4  6\n\nCitation:\n\nJian-Jiun Ding, Cheng-Jin Kuo, Wen-Chih Hong, \"An efficient image segmentation technique by fast scanning and adaptive merging\"\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.watershed","page":"References","title":"ImageSegmentation.watershed","text":"segments                = watershed(img, markers; compactness, mask)\n\nSegments the image using watershed transform. Each basin formed by watershed transform corresponds to a segment. If you are using image local minimas as markers, consider using hmin_transform to avoid oversegmentation.\n\nParameters:\n\nimg            = input grayscale image\nmarkers        = An array (same size as img) with each region's marker assigned a index starting from 1. Zero means not a marker.                     If two markers have the same index, their regions will be merged into a single region.                     If you have markers as a boolean array, use label_components.\ncompactness    = Use the compact watershed algorithm with the given compactness parameter. Larger values lead to more regularly                     shaped watershed basins.[1]\nmask           = Only segment pixels where the value of mask is true, used to restrict segmentation to only areas of interest\n\n[1]: https://www.tu-chemnitz.de/etit/proaut/publications/cwspSLICICPR.pdf\n\nExample\n\njulia> seeds = falses(100, 100); seeds[50, 25] = true; seeds[50, 75] = true;\n\njulia> dists = distance_transform(feature_transform(seeds)); # calculate distances from seeds\n\njulia> markers = label_components(seeds); # give each seed a unique integer id\n\njulia> results = watershed(dists, markers);\n\njulia> labels_map(results); # labels of segmented image\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.hmin_transform","page":"References","title":"ImageSegmentation.hmin_transform","text":"out = hmin_transform(img, h)\n\nSuppresses all minima in grayscale image whose depth is less than h.\n\nH-minima transform is defined as the reconstruction by erosion of (img + h) by img. See Morphological image analysis by Soille pg 170-172.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.region_adjacency_graph","page":"References","title":"ImageSegmentation.region_adjacency_graph","text":"G, vert_map = region_adjacency_graph(seg, weight_fn)\n\nConstructs a region adjacency graph (RAG) from the SegmentedImage. It returns the RAG along with a Dict(label=>vertex) map. weight_fn is used to assign weights to the edges.\n\nweight_fn(label1, label2)\n\nReturns a real number corresponding to the weight of the edge between label1 and label2.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.prune_segments","page":"References","title":"ImageSegmentation.prune_segments","text":"new_seg = prune_segments(seg, rem_labels, diff_fn)\n\nRemoves all segments that have labels in rem_labels replacing them with their neighbouring segment having least diff_fn. rem_labels is a Vector of labels.\n\nnew_seg = prune_segments(seg, is_rem, diff_fn)\n\nRemoves all segments for which is_rem returns true replacing them with their neighbouring segment having least diff_fn.\n\nis_rem(label) -> Bool\n\nReturns true if label label is to be removed otherwise false.\n\nd = diff_fn(rem_label, neigh_label)\n\nA difference measure between label to be removed and its neighbors. isless must be defined for objects of the type of d.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.region_tree","page":"References","title":"ImageSegmentation.region_tree","text":"t = region_tree(img, homogeneous)\n\nCreates a region tree from img by splitting it recursively until all the regions are homogeneous.\n\nb = homogeneous(img)\n\nReturns true if img is homogeneous.\n\nExamples\n\njulia> img = 0.1*rand(6, 6);\n\njulia> img[4:end, 4:end] .+= 10;\n\njulia> function homogeneous(img)\n           min, max = extrema(img)\n           max - min < 0.2\n       end\nhomogeneous (generic function with 1 method)\n\njulia> t = region_tree(img, homogeneous);\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageSegmentation.region_splitting","page":"References","title":"ImageSegmentation.region_splitting","text":"seg = region_splitting(img, homogeneous)\n\nSegments img by recursively splitting it until all the segments are homogeneous.\n\nb = homogeneous(img)\n\nReturns true if img is homogeneous.\n\nExamples\n\njulia> img = 0.1*rand(6, 6);\n\njulia> img[4:end, 4:end] .+= 10;\n\njulia> function homogeneous(img)\n           min, max = extrema(img)\n           max - min < 0.2\n       end\nhomogeneous (generic function with 1 method)\n\njulia> seg = region_splitting(img, homogeneous);\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures","page":"References","title":"ImageFeatures","text":"","category":"section"},{"location":"function_reference/#Geometric-features","page":"References","title":"Geometric features","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"hough_transform_standard\nhough_circle_gradient","category":"page"},{"location":"function_reference/#ImageFeatures.hough_transform_standard","page":"References","title":"ImageFeatures.hough_transform_standard","text":"lines = hough_transform_standard(\n    img_edges::AbstractMatrix;\n    stepsize=1,\n    angles=range(0,stop=pi,length=minimum(size(img))),\n    vote_threshold=minimum(size(img)) / stepsize -1,\n    max_linecount=typemax(Int))\n\nReturns a vector of tuples corresponding to the tuples of (r,t) where r and t are parameters for normal form of line:     x * cos(t) + y * sin(t) = r\n\nr = length of perpendicular from (1,1) to the line\nt = angle between perpendicular from (1,1) to the line and x-axis\n\nThe lines are generated by applying hough transform on the image.\n\nParameters:\n\nimg_edges      = Image to be transformed (eltype should be Bool)\nstepsize       = Discrete step size for perpendicular length of line\nangles         = List of angles for which the transform is computed\nvote_threshold = Accumulator threshold for line detection\nmax_linecount  = Maximum no of lines to return\n\nExample\n\njulia> using ImageFeatures\n\njulia> img = fill(false,5,5); img[3,:] .= true; img\n5×5 Array{Bool,2}:\n false  false  false  false  false\n false  false  false  false  false\n  true   true   true   true   true\n false  false  false  false  false\n false  false  false  false  false\n\njulia> hough_transform_standard(img)\n1-element Array{Tuple{Float64,Float64},1}:\n (3.0, 1.5707963267948966)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.hough_circle_gradient","page":"References","title":"ImageFeatures.hough_circle_gradient","text":"circle_centers, circle_radius = hough_circle_gradient(img_edges, img_phase, radii; scale=1, min_dist=minimum(radii), vote_threshold)\n\nReturns two vectors, corresponding to circle centers and radius.\n\nThe circles are generated using a hough transform variant in which a non-zero point only votes for circle centers perpendicular to the local gradient. In case of concentric circles, only the largest circle is detected.\n\nParameters:\n\nimg_edges    = edges of the image\nimg_phase    = phase of the gradient image\nradii        = circle radius range\nscale        = relative accumulator resolution factor\nmin_dist     = minimum distance between detected circle centers\nvote_threshold   = accumulator threshold for circle detection\n\ncanny and phase can be used for obtaining imgedges and imgphase respectively.\n\nExample\n\njulia> using Images, ImageFeatures, FileIO, ImageView\n\njulia> img = load(download(\"http://docs.opencv.org/3.1.0/water_coins.jpg\"));\n\njulia> img = Gray.(img);\n\njulia> img_edges = canny(img, (Percentile(99), Percentile(80)));\n\njulia> dx, dy=imgradients(img, KernelFactors.ando5);\n\njulia> img_phase = phase(dx, dy);\n\njulia> centers, radii = hough_circle_gradient(img_edges, img_phase, 20:30);\n\njulia> img_demo = Float64.(img_edges); for c in centers img_demo[c] = 2; end\n\njulia> imshow(img_demo)\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Types","page":"References","title":"Types","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"Feature\nFeatures\nKeypoint\nKeypoints\nBRIEF\nORB\nFREAK\nBRISK\nHOG","category":"page"},{"location":"function_reference/#ImageFeatures.Feature","page":"References","title":"ImageFeatures.Feature","text":"feature = Feature(keypoint, orientation = 0.0, scale = 0.0)\n\nThe Feature type has the keypoint, its orientation and its scale.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.Features","page":"References","title":"ImageFeatures.Features","text":"features = Features(boolean_img)\nfeatures = Features(keypoints)\n\nReturns a Vector{Feature} of features generated from the true values in a boolean image or from a list of keypoints.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.Keypoint","page":"References","title":"ImageFeatures.Keypoint","text":"keypoint = Keypoint(y, x)\nkeypoint = Keypoint(feature)\n\nA Keypoint may be created by passing the coordinates of the point or from a feature.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.Keypoints","page":"References","title":"ImageFeatures.Keypoints","text":"keypoints = Keypoints(boolean_img)\nkeypoints = Keypoints(features)\n\nCreates a Vector{Keypoint} of the true values in a boolean image or from a list of features.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.BRIEF","page":"References","title":"ImageFeatures.BRIEF","text":"brief_params = BRIEF([size = 128], [window = 9], [sigma = 2 ^ 0.5], [sampling_type = gaussian], [seed = 123])\n\nArgument Type Description\nsize Int Size of the descriptor\nwindow Int Size of sampling window\nsigma Float64 Value of sigma used for inital gaussian smoothing of image\nsampling_type Function Type of sampling used for building the descriptor (See BRIEF Sampling Patterns)\nseed Int Random seed used for generating the sampling pairs. For matching two descriptors, the seed used to build both should be same.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.ORB","page":"References","title":"ImageFeatures.ORB","text":"orb_params = ORB([num_keypoints = 500], [n_fast = 12], [threshold = 0.25], [harris_factor = 0.04], [downsample = 1.3], [levels = 8], [sigma = 1.2])\n\nArgument Type Description\nnum_keypoints Int Number of keypoints to extract and size of the descriptor calculated\nn_fast Int Number of consecutive pixels used for finding corners with FAST. See [fastcorners]\nthreshold Float64 Threshold used to find corners in FAST. See [fastcorners]\nharris_factor Float64 Harris factor k used to rank keypoints by harris responses and extract the best ones\ndownsample Float64 Downsampling parameter used while building the gaussian pyramid. See [gaussian_pyramid] in Images.jl\nlevels Int Number of levels in the gaussian pyramid.  See [gaussian_pyramid] in Images.jl\nsigma Float64 Used for gaussian smoothing in each level of the gaussian pyramid.  See [gaussian_pyramid] in Images.jl\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.FREAK","page":"References","title":"ImageFeatures.FREAK","text":"freak_params = FREAK([pattern_scale = 22.0])\n\nArgument Type Description\npattern_scale Float64 Scaling factor for the sampling window\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.BRISK","page":"References","title":"ImageFeatures.BRISK","text":"brisk_params = BRISK([pattern_scale = 1.0])\n\nArgument Type Description\npattern_scale Float64 Scaling factor for the sampling window\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#ImageFeatures.HOG","page":"References","title":"ImageFeatures.HOG","text":"hog_params = HOG([orientations = 9], [cell_size = 8], [block_size = 2], [block_stride = 1], [norm_method = \"L2-norm\"])\n\nHistogram of Oriented Gradient (HOG) is a dense feature desciptor usually used for object detection. See \"Histograms of Oriented Gradients for Human Detection\" by Dalal and Triggs.\n\nParameters:\n\norientations   = number of orientation bins\ncellsize      = size of a cell is cellsize x cell_size (in pixels)\nblocksize     = size of a block is blocksize x block_size (in terms of cells)\nblock_stride   = stride of blocks. Controls how much adjacent blocks overlap.\nnorm_method    = block normalization method. Options: L2-norm, L2-hys, L1-norm, L2-sqrt.\n\n\n\n\n\n","category":"type"},{"location":"function_reference/#Corners","page":"References","title":"Corners","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"corner_orientations","category":"page"},{"location":"function_reference/#ImageFeatures.corner_orientations","page":"References","title":"ImageFeatures.corner_orientations","text":"orientations = corner_orientations(img)\norientations = corner_orientations(img, corners)\norientations = corner_orientations(img, corners, kernel)\n\nReturns the orientations of corner patches in an image. The orientation of a corner patch is denoted by the orientation of the vector between intensity centroid and the corner. The intensity centroid can be calculated as C = (m01/m00, m10/m00) where mpq is defined as -\n\n`mpq = (x^p)(y^q)I(y, x) for each p, q in the corner patch`\n\nThe kernel used for the patch can be given through the kernel argument. The default kernel used is a gaussian kernel of size 5x5.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#BRIEF-Sampling-Patterns","page":"References","title":"BRIEF Sampling Patterns","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"random_uniform\nrandom_coarse\ngaussian\ngaussian_local\ncenter_sample","category":"page"},{"location":"function_reference/#ImageFeatures.random_uniform","page":"References","title":"ImageFeatures.random_uniform","text":"sample_one, sample_two = random_uniform(size, window, seed)\n\nBuilds sampling pairs using random uniform sampling.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.random_coarse","page":"References","title":"ImageFeatures.random_coarse","text":"sample_one, sample_two = random_coarse(size, window, seed)\n\nBuilds sampling pairs using random sampling over a coarse grid.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.gaussian","page":"References","title":"ImageFeatures.gaussian","text":"sample_one, sample_two = gaussian(size, window, seed)\n\nBuilds sampling pairs using gaussian sampling.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.gaussian_local","page":"References","title":"ImageFeatures.gaussian_local","text":"sample_one, sample_two = gaussian_local(size, window, seed)\n\nPairs (Xi, Yi) are randomly sampled using a Gaussian distribution where first X is sampled with a standard deviation of 0.04*S^2 and then the Yi’s are sampled using a Gaussian distribution – Each Yi is sampled with mean Xi and standard deviation of 0.01 * S^2\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.center_sample","page":"References","title":"ImageFeatures.center_sample","text":"sample_one, sample_two = center_sample(size, window, seed)\n\nBuilds sampling pairs (Xi, Yi) where Xi is (0, 0) and Yi is sampled uniformly from the window.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Feature-Description","page":"References","title":"Feature Description","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"create_descriptor","category":"page"},{"location":"function_reference/#ImageFeatures.create_descriptor","page":"References","title":"ImageFeatures.create_descriptor","text":"desc, keypoints = create_descriptor(img, keypoints, params)\ndesc, keypoints = create_descriptor(img, params)\n\nCreate a descriptor for each entry in keypoints from the image img. params specifies the parameters for any of several descriptors:\n\nBRIEF\nORB\nBRISK\nFREAK\nHOG\n\nSome descriptors support discovery of the keypoints from fastcorners.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Feature-Matching","page":"References","title":"Feature Matching","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"hamming_distance\nmatch_keypoints","category":"page"},{"location":"function_reference/#ImageFeatures.hamming_distance","page":"References","title":"ImageFeatures.hamming_distance","text":"distance = hamming_distance(desc_1, desc_2)\n\nCalculates the hamming distance between two descriptors.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#ImageFeatures.match_keypoints","page":"References","title":"ImageFeatures.match_keypoints","text":"matches = match_keypoints(keypoints_1, keypoints_2, desc_1, desc_2, threshold = 0.1)\n\nFinds matched keypoints using the hamming_distance function having distance value less than threshold.\n\n\n\n\n\n","category":"function"},{"location":"function_reference/#Texture-Matching","page":"References","title":"Texture Matching","text":"","category":"section"},{"location":"function_reference/#Gray-Level-Co-occurence-Matrix","page":"References","title":"Gray Level Co-occurence Matrix","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"glcm\nglcm_symmetric\nglcm_norm\nglcm_prop\nmax_prob\ncontrast\nASM\nIDM\nglcm_entropy\nenergy\ndissimilarity\ncorrelation\nglcm_mean_ref\nglcm_mean_neighbour\nglcm_var_ref\nglcm_var_neighbour","category":"page"},{"location":"function_reference/#Local-Binary-Patterns","page":"References","title":"Local Binary Patterns","text":"","category":"section"},{"location":"function_reference/","page":"References","title":"References","text":"lbp\nmodified_lbp\ndirection_coded_lbp\nlbp_original\nlbp_uniform\nlbp_rotation_invariant\nmulti_block_lbp","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/#Image-Difference-View","page":"Image Difference View","title":"Image Difference View","text":"","category":"section"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"(Image: Source code) (Image: notebook) (Image: Author) (Image: Update time)","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"This demonstration shows some common tricks in image comparision – difference view","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"People with MATLAB experiences would miss the function imshowpair, but in JuliaImages it is not that indispensable.","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"using Images\nusing TestImages\n\nimg = float.(testimage(\"cameraman\"))\n# rotate img by 4 degrees and keep axes unchanged\nimg_r = imrotate(img, -pi/45, axes(img))\nnothing #hide","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"mosaicview is a convenience tool to show multiple images, especially useful when they have different sizes and colors.","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"mosaicview(img, img_r; nrow=1, npad=20, fillvalue=colorant\"white\")","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"In some cases, when the differences of two images are relative insignificant, a plain substraction can help amplify the difference.","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"plain_diffview = @. img - img_r\nnothing #hide","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"For gray images, a fancy trick is to fill each image into different RGB channels and make a RGB view","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"RGB_diffview = colorview(RGB, channelview(img), channelview(img_r), fill(0., size(img)))\nnothing #hide","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"or convert the RGB view back to Gray image after that","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"Gray_diffview = Gray.(RGB_diffview)\n\nmosaicview(plain_diffview, RGB_diffview, Gray_diffview;\n           nrow=1, npad=20, fillvalue=colorant\"white\")","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"","category":"page"},{"location":"democards/examples/spatial_transformation/image_diffview/","page":"Image Difference View","title":"Image Difference View","text":"This page was generated using DemoCards.jl and Literate.jl.","category":"page"},{"location":"pkgs/axes/#ImageAxes.jl","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"","category":"section"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"While images can often be represented as plain Arrays, sometimes additional information about the \"meaning\" of each axis of the array is needed.  For example, in a 3-dimensional MRI scan, the voxels may not have the same spacing along the z-axis that they do along the x- and y-axes, and this fact should be accounted for during the display and/or analysis of such images.  Likewise, a movie has two spatial axes and one temporal axis; this fact may be relevant for how one performs image processing.","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"The ImageAxes package (which is incorporated into Images) combines features from AxisArrays and SimpleTraits to provide a convenient representation and programming paradigm for dealing with such images.","category":"page"},{"location":"pkgs/axes/#Installation","page":"ImageAxes.jl","title":"Installation","text":"","category":"section"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"If you want to directly use ImageAxes, add it via the package manager.","category":"page"},{"location":"pkgs/axes/#Usage","page":"ImageAxes.jl","title":"Usage","text":"","category":"section"},{"location":"pkgs/axes/#Names-and-locations","page":"ImageAxes.jl","title":"Names and locations","text":"","category":"section"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"The simplest thing you can do is to provide names to your image axes:","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes\nimg = AxisArray(reshape(1:192, (8,8,3)), :x, :y, :z)","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"As described in more detail in the AxisArrays documentation, you can now take slices like this:","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"sl = img[Axis{:z}(2)]","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"You can also give units to the axes:","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes, Unitful\nconst mm = u\"mm\"\nimg = AxisArray(reshape(1:192, (8,8,3)),\n                Axis{:x}(1mm:1mm:8mm),\n                Axis{:y}(1mm:1mm:8mm),\n                Axis{:z}(2mm:3mm:8mm))","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"which specifies that x and y have spacing of 1mm and z has a spacing of 3mm, as well as the location of the center of each voxel.","category":"page"},{"location":"pkgs/axes/#Temporal-axes","page":"ImageAxes.jl","title":"Temporal axes","text":"","category":"section"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"Any array possessing an axis Axis{:time} will be recognized as having a temporal dimension.  Given an array A,","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes, Unitful\nconst s = u\"s\"\nimg = AxisArray(reshape(1:9*300, (3,3,300)),\n                Axis{:x}(1:3),\n                Axis{:y}(1:3),\n                Axis{:time}(1s/30:1s/30:10s))","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"you can retrieve its temporal axis with","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"ax = timeaxis(img)","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"and index it like","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"img[ax(4)]  # returns the 4th \"timeslice\"","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"You can also specialize methods like this:","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes, SimpleTraits\n@traitfn nimages(img::AA) where {AA<:AxisArray;  HasTimeAxis{AA}} = length(timeaxis(img))\n@traitfn nimages(img::AA) where {AA<:AxisArray; !HasTimeAxis{AA}} = 1","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"where the pre-defined HasTimeAxis trait will restrict that method to arrays that have a timeaxis. A more complex example is","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes, SimpleTraits, Statistics\n\n@traitfn meanintensity(img::AA) where {AA<:AxisArray; !HasTimeAxis{AA}} = mean(img)\n@traitfn function meanintensity(img::AA) where {AA<:AxisArray; HasTimeAxis{AA}}\n    ax = timeaxis(img)\n    n = length(ax)\n    intensity = zeros(eltype(img), n)\n    for ti in 1:n\n        sl = view(img, ax(ti))\n        intensity[ti] = mean(sl)\n    end\n    intensity\nend","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"and, when appropriate, it will return the mean intensity at each timeslice.","category":"page"},{"location":"pkgs/axes/#Custom-temporal-axes","page":"ImageAxes.jl","title":"Custom temporal axes","text":"","category":"section"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"Using SimpleTraits's @traitimpl, you can add Axis{:t} or Axis{:scantime} or any other name to the list of axes that have a temporal dimension:","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"using ImageAxes, SimpleTraits\n@traitimpl TimeAxis{Axis{:t}}","category":"page"},{"location":"pkgs/axes/","page":"ImageAxes.jl","title":"ImageAxes.jl","text":"Note this declaration affects all arrays throughout your entire session.  Moreover, it should be made before calling any functions on array-types that possess such axes; a convenient place to do this is right after you say using ImageAxes in your top-level script.","category":"page"},{"location":"tutorials/indexing/#page_indexing","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"","category":"section"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"In addition to the handling of numbers and colors, one of the main ways that JuliaImages leverages Julia is through a number of more sophisticated indexing operations. These are perhaps best illustrated with examples.","category":"page"},{"location":"tutorials/indexing/#Keeping-track-of-location-with-unconventional-indices","page":"Arrays: more advanced indexing","title":"Keeping track of location with unconventional indices","text":"","category":"section"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"Consider the following pair of images:","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"imgref img\n(Image: cameraman) (Image: cameraman)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"You might guess that the one on the right is a rotated version of the one on the left. But, what is the angle? Is there also a translation?","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"A \"low tech\" way to test this is to rotate and shift the image on the right until it seems aligned with the one on the left. We could overlay the two images (Using colorview to make color overlays) to see how well we're doing.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"julia> using Images, CoordinateTransformations, Rotations\n\njulia> tfm = recenter(RotMatrix(pi/8), center(img))\nAffineMap([0.9238795325112867 -0.3826834323650898; 0.3826834323650898 0.9238795325112867], [88.7785546217109, -59.31993370357884])\n\njulia> imgrot = warp(img, tfm);\n\njulia> summary(img)\n\"386×386 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}\"\n\njulia> summary(imgrot)\n\"506×506 OffsetArray(::Array{Gray{N0f8},2}, -59:446, -59:446) with eltype Gray{Normed{UInt8,8}} with indices -59:446×-59:446\"","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"While img has axes that start with the conventional 1, the summary of imgrot reports that it has axes (-59:446, -59:446). This means that the first element of imgrot is indexed with imgrot[-59,-59] and the last element with imgrot[446,446].","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"What is the meaning of these indices that extend beyond those of the original array in both directions? Displaying the rotated image–-especially when overlaid on the original–-reveals why:","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"julia> imgov = colorview(RGB, paddedviews(0, img, imgrot, zeroarray)...)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"(Image: rot_overlay)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"The padding on all sides of the array leaves space for the fact that the rotated image (green) contains some pixels out of the region covered by the original image (red).  The fact that Julia allows these indices to be negative means that we have no trouble adding appropriate \"padding\" to the original image: we just copy the original over to the padded array, using its original indices.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"We can test whether imgrot aligns well with the original unrotated image imgref at the top of this page:","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"julia> imgov_ref = colorview(RGB, paddedviews(0, imgref, imgrot, zeroarray)...)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"(Image: ref_overlay)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"The fact that the overlapping portion looks yellow–-the combination of red and green–-indicates that we have perfect alignment.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"You can learn more about Julia's support for arbitrary indices in this blog post.","category":"page"},{"location":"tutorials/indexing/#Keeping-track-of-orientation-with-named-axes","page":"Arrays: more advanced indexing","title":"Keeping track of orientation with named axes","text":"","category":"section"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"Suppose you are presented with a 3-dimensional grayscale image. Is this a movie (2d over time), or a 3d image (x, y, and z)? In such situations, one of the best ways to keep yourself oriented is by naming the axes. The TestImages package contains an example of a file that illustrates this:","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"julia> using Images, TestImages\n\njulia> img = testimage(\"mri\");\n\njulia> println(summary(img))\n3-dimensional AxisArray{Gray{N0f8},3,...} with axes:\n    :P, 0:1:225\n    :R, 0:1:185\n    :S, 0:5:130\nAnd data, a 226×186×27 Array{Gray{N0f8},3} with eltype Gray{Normed{UInt8,8}}","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"TestImages uses the AxisArrays package to name the axes of this particular image in terms of the RAS coordinate system (Right, Anterior, Superior) as commonly used in magnetic resonance imaging. See the documentation for that package to learn more about how you can create your own AxisArray objects.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"We can use this coordinate system to help with visualization. Let's look at a \"horizontal slice,\" one perpendicular to the superior-inferior axis (i.e., a slice with constant S value):","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"(Image: Sslice)","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"From the summary you can see that the slice has just the :A and :R axes remaining.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"We could slice along the R and A axes too, although for this image (which is sampled very anisotropically) they are not as informative.","category":"page"},{"location":"tutorials/indexing/","page":"Arrays: more advanced indexing","title":"Arrays: more advanced indexing","text":"The ImageAxes and ImageMetadata packages add additional functionality to AxisArrays that may be useful when you need to encode more information about your image.","category":"page"},{"location":"#JuliaImages:-image-processing-and-machine-vision-for-Julia","page":"Home","title":"JuliaImages: image processing and machine vision for Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JuliaImages (source code) hosts the major Julia packages for image processing. Julia is well-suited to image processing because it is a modern and elegant high-level language that is a pleasure to use, while also allowing you to write \"inner loops\" that compile to efficient machine code (i.e., it is as fast as C).  Julia supports multithreading and, through add-on packages, GPU processing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"JuliaImages is a collection of packages specifically focused on image processing.  It is not yet as complete as some toolkits for other programming languages, but it has many useful algorithms.  It is focused on clean architecture and is designed to unify \"machine vision\" and \"biomedical 3d image processing\" communities.","category":"page"},{"location":"","page":"Home","title":"Home","text":"These pages are designed to help you get started with image analysis in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nPlease help improve this documentation–if something confuses you, chances are you're not alone. It's easy to do as you read along: just click on the \"Edit on GitHub\" link above, and then edit the files directly in your browser. Your changes will be vetted by developers before becoming permanent, so don't worry about whether you might say something wrong.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This documentation is a collection of several parts:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The \"Tutorials\" part contains a list of tutorials that help you gain better understanding of the JuliaImages ecosystem.\nThe \"Packages\" part contains information about specific components (themselves Julia packages) that together comprise JuliaImages and address specific subfields of image processing.\nThe \"Demos\" part gives you demonstrations of how to carry out specific tasks with JuliaImages.\nThe \"References\" part is a collection of function references provided by JuliaImages. The recommended way to use the references is by the searching function of your browser Ctrl-F/Cmd-F.\nThe \"Comparison with other image processing frameworks\" would be helpful if you've used other frameworks previously.","category":"page"},{"location":"pkgs/#page_packages_index","page":"Introduction","title":"Packages under JuliaImages","text":"","category":"section"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"tip: Tip\nMost users should probably start with the tutorials before diving into the documentation for individual packages. Much of JuliaImages' functionality comes from composing very basic operations that are not always available in other environments, and the tutorials will make this workflow clearer.","category":"page"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"🚧This section documents the individual components that form the JuliaImages ecosystem. The ones marked with a * are available via using Images, but you can also use packages individually. Below, they're grouped into broad categories, then ordered alphabetically so you can start with whatever you are interested in most.","category":"page"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"Low-level core packages\nColorTypes.jl*, Colors.jl* and ColorVectorSpaces.jl* provides pixel-level definitions and functions.\nFixedPointNumbers.jl* provides several data types (e.g., N0f8) for image storage usage.\nImageCore.jl* provides various basic and convenient views, traits and functions to support image processing algorithms.\nTraits and utilities\nImageAxes.jl* supports AxisArrays.jl* to endow the axes with \"meaning\".\nImageMetadata.jl* is a simple package providing utilities for working with images that have metadata attached.\n🚧 ImageDraw.jl let you draw shapes on an image.\nImageDistances.jl* is a Distances.jl wrapper for images.\nOffsetArrays.jl supports arrays with arbitrary indices offsets.\nMappedViews.jl provides lazy in-place transformations of arrays.\nPaddedViews.jl* add virtual padding to the edges of an array. It also allows you to composite multiple images together.\nTestImages.jl provides several \"standard\" test images.\nImage visulization are supported by various packages\nImage saving and loading are supported by packages under JuliaIO\nhigh-level algorithms\nImageBinarization.jl provides various image binarization algorithms.\nImageContrastAdjustment.jl* supports image contrast enhancement and manipulation.\nImageMorphology.jl* provides several morphological operations for image processing.\nImageFiltering.jl* supports basic filtering operations.\nImageFeatures.jl is a package for identifying and characterizing \"keypoints\" (salient features) in images.\nImageQualityIndexes.jl* provides several image quality assessment indexes, e.g., PSNR and SSIM.\nImageTransformations.jl* provides functions related to geometric transformations.\nImageSegmentation.jl provides several image segmentation algorithms.\nImageInpainting.jl provides image inpainting algorithms in Julia","category":"page"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"tip: Tip\nFor package developers, Images.jl is usually a large dependency to be included in the deps section of your Project.toml. Hence it is reccomended to add only ImageCore together with the exact sub-packages you need. You can use @which to find out the exact package and file a method/function belongs to.","category":"page"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"JuliaImages is not a closed ecosystem; it works nicely with many other packages outside of JuliaImages. The following is an incomplete list of third-party packages that are widely used together with Images.jl:","category":"page"},{"location":"pkgs/","page":"Introduction","title":"Introduction","text":"Augmentor.jl provides several basic image augmentation operations for image-related machine learning tasks.\nFlux.jl is a deep learning toolbox in Julia.","category":"page"}]
}
